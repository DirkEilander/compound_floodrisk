{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from os.path import isfile, join\n",
    "import numpy as np\n",
    "import hydromt\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdir = r\"../../3_models\"\n",
    "rdir = r\"../../4_results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings\n",
    "hmin = 0.15\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flood impact functions\n",
    "\n",
    "def flood_damage(da_flddph, da_exposure, df_susceptibility, **kwargs):\n",
    "    nodata = da_exposure.attrs['_FillValue']\n",
    "    da0 = df_susceptibility.to_xarray()['factor']#.chunk({'depth':-1})\n",
    "    factor = np.minimum(1, da0.interp(depth=np.minimum(da0.max(),da_flddph), **kwargs))\n",
    "    damage = (factor * da_exposure).fillna(nodata).astype(np.float32)\n",
    "    damage.name = da_exposure.name\n",
    "    damage.attrs.update(**da_exposure.attrs)\n",
    "    return damage\n",
    "\n",
    "def flood_exposed(da_flddph, da_exposure, min_flddph=hmin):\n",
    "    exposed = xr.where(da_flddph>min_flddph,da_exposure,0.0).astype(np.float32)\n",
    "    exposed.attrs.update(**da_exposure.attrs)\n",
    "    exposed.name = da_exposure.name\n",
    "    return exposed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read basemodel\n",
    "from hydromt_sfincs import SfincsModel\n",
    "mod0 = SfincsModel(join(mdir, 'sfincs', '00_base_riv'), mode='r')\n",
    "rivmsk = mod0.staticmaps['rivmsk']==1\n",
    "mask0 = mod0.staticmaps['dep']==mod0.staticmaps['dep'].raster.nodata\n",
    "mask = np.logical_or(rivmsk, mask0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read vulnerability curves and \n",
    "df = pd.read_csv(join(mdir, 'fiat', 'susceptibility', 'AF000.csv'), index_col=0)\n",
    "df.columns = ['factor']\n",
    "df.index.name = 'depth'\n",
    "# correct min flood depth (hmin)\n",
    "df[df.index<=hmin] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read exposure\n",
    "ds_exp = hydromt.open_mfraster(join(mdir, 'fiat', 'exposure', '*.tif')).load()\n",
    "ds_exp = ds_exp[['buildings_value', 'population_count']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read scenario where we expect no flooding for bias correction\n",
    "postfix = '_dt0'\n",
    "da_bias = xr.open_rasterio(join(mdir, 'sfincs', f'qb000_qp000_h000_p000{postfix}', 'gis', 'hmax.tif')).squeeze(drop=True)\n",
    "da_bias = np.maximum(0, da_bias).where(~mask)\n",
    "da_bias.raster.set_nodata(np.nan)\n",
    "# da_bias.raster.to_raster(join(rdir, 'hmax', f'bias{postfix}.tif'), compress='deflate')\n",
    "# da_bias = da_bias.fillna(0)\n",
    "# da_bias.where(~mask).plot(vmax=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare output\n",
    "set_name = 'sim_SCEN_all_rps7'\n",
    "df_scens = pd.read_csv(join(rdir, f'{set_name}.csv'), index_col=0)\n",
    "if postfix:\n",
    "    df_scens['scen'] = [f'{name}{postfix}' for name in df_scens['scen']]\n",
    "df_scens['finished'] = False\n",
    "df_scens.to_csv(join(rdir, f'impact_bias0{postfix}.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save univariate and full dep hazards\n",
    "\n",
    "for i, row in df_scens.iterrows():\n",
    "    scen = row['scen']\n",
    "    hazard_fn = join(mdir, 'sfincs', scen, 'gis', 'hmax.tif')\n",
    "    rps = row[['qb_rp', 'qp_rp', 'p_rp', 'h_tsw_rp']]\n",
    "    # save rasters\n",
    "    if np.all(np.diff(rps) == 0) or np.sum(rps > 0) == 1:\n",
    "        da_hmax = xr.open_rasterio(hazard_fn).squeeze(drop=True)\n",
    "        # correct for perm water and bias\n",
    "        da_hmax = da_hmax.where(mask0, np.maximum(0, da_hmax - da_bias))\n",
    "        # save\n",
    "        da_hmax.raster.set_nodata(np.nan)\n",
    "        da_hmax.raster.to_raster(join(rdir, 'hmax', f'{scen}.tif'), compress='deflate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_hmax = hydromt.open_mfraster(join(rdir, 'hmax', f'*{postfix}.tif')).load()\n",
    "ds_hmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zoning -> preprocess exposure based on fulldep rp2/5/10 zones\n",
    "zoning = {}\n",
    "for rp0 in [2,5,10]:\n",
    "    flood_zone = xr.concat([\n",
    "        ds_hmax[f'qb{rp0:03d}_qp000_h000_p000'],\n",
    "        ds_hmax[f'qb000_qp{rp0:03d}_h000_p000'],\n",
    "        ds_hmax[f'qb000_qp000_h{rp0:03d}_p000'],\n",
    "        ds_hmax[f'qb000_qp000_h000_p{rp0:03d}'],\n",
    "    ], dim='dvar').max('dvar') > hmin\n",
    "    zoning[rp0] = ds_exp.copy().where(~flood_zone,0).compute()\n",
    "    for dvar in ds_exp.data_vars.keys():\n",
    "        print(rp0, dvar, (ds_exp[dvar].sum() - zoning[rp0][dvar].sum()).item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dry proofing -> range of hmin [0.3-0.7] in vulnerability curves\n",
    "dryproof = {}\n",
    "for hmin0 in [0.5,0.75,0.999]:\n",
    "    df0 = df.copy(deep=True)\n",
    "    df0[df0.index<=hmin0] = 0\n",
    "    dryproof[int(np.round(hmin0*100))] = df0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process per hazard simulatio\n",
    "df_scens = pd.read_csv(join(rdir, f'impact_bias0{postfix}.csv'), index_col=0)#.head()\n",
    "\n",
    "for i, row in df_scens.iterrows():\n",
    "    scen = row['scen']\n",
    "    hazard_fn = join(mdir, 'sfincs', scen, 'gis', 'hmax.tif')\n",
    "    rps = row[['qb_rp', 'qp_rp', 'p_rp', 'h_tsw_rp']]\n",
    "    qb_rp, qp_rp, p_rp, h_rp = rps\n",
    "    if row['finished'] or not isfile(hazard_fn):\n",
    "        continue \n",
    "    # read hazard\n",
    "    da_hmax = xr.open_rasterio(hazard_fn).squeeze(drop=True)\n",
    "    # correct for perm water and bias\n",
    "    da_hmax = da_hmax.where(mask, da_hmax - da_bias)\n",
    "    da_hmax = np.maximum(0, da_hmax).where(~mask)\n",
    "    # impact assessment - base\n",
    "    df_scens.loc[i, 'dam'] = flood_damage(da_hmax, ds_exp['buildings_value'], df).sum()\n",
    "    df_scens.loc[i, 'ppl'] = flood_exposed(da_hmax, ds_exp['population_count'], hmin).sum()\n",
    "\n",
    "    # impact assessment - adaptation scenarios\n",
    "    # zoning\n",
    "    for rp0 in zoning:\n",
    "        df_scens.loc[i, f'dam_zoning{rp0:03d}'] = flood_damage(da_hmax, zoning[rp0]['buildings_value'], df).sum()\n",
    "        df_scens.loc[i, f'ppl_zoning{rp0:03d}'] = flood_exposed(da_hmax, zoning[rp0]['population_count'], hmin).sum()\n",
    "    # dry proofing\n",
    "    for hmin0 in dryproof:\n",
    "        df_scens.loc[i, f'dam_dryproof{hmin0}'] = flood_damage(da_hmax, ds_exp['buildings_value'], dryproof[hmin0]).sum()\n",
    "        df_scens.loc[i, f'ppl_dryproof{hmin0}'] = flood_exposed(da_hmax, ds_exp['population_count'], hmin0/100).sum()\n",
    "    # dikes -> remove q and h hazard; keep p hazard below or equal to rp5/10/50\n",
    "    for rp0 in [5, 10, 50]:\n",
    "        da_hmax1 = da_hmax.copy()\n",
    "        if qb_rp <= rp0:\n",
    "            da_hmax1 = da_hmax1.where(mask, np.maximum(0, da_hmax1 - ds_hmax[f'qb{qb_rp:03d}_qp000_h000_p000']))\n",
    "        if qp_rp <= rp0:\n",
    "            da_hmax1 = da_hmax1.where(mask, np.maximum(0, da_hmax1 - ds_hmax[f'qb000_qp{qp_rp:03d}_h000_p000']))\n",
    "        if h_rp <= rp0:\n",
    "            da_hmax1 = da_hmax1.where(mask, np.maximum(0, da_hmax1 - ds_hmax[f'qb000_qp000_h{h_rp:03d}_p000']))\n",
    "        if p_rp > 0:\n",
    "            da_hmax1 = da_hmax1.where(mask, np.maximum(da_hmax1, ds_hmax[f'qb000_qp000_h000_p{p_rp:03d}']))\n",
    "        df_scens.loc[i, f'dam_dikes{rp0:03d}'] = flood_damage(da_hmax1, ds_exp['buildings_value'], df).sum()\n",
    "        df_scens.loc[i, f'ppl_dikes{rp0:03d}'] = flood_exposed(da_hmax1, ds_exp['population_count'], hmin).sum()\n",
    "\n",
    "    df_scens.loc[i, 'finished'] = True\n",
    "\n",
    "# save to csv\n",
    "df_scens = df_scens.round(1)\n",
    "df_scens.to_csv(join(rdir, f'impact_bias0{postfix}.csv'))\n",
    "df_scens[df_scens['finished']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3808d5b5b54949c7a0a707a38b0a689040fa9c90ab139a050e41373880719ab1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 ('hydromt-dev')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
