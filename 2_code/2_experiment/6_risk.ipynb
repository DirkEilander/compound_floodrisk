{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045de6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from os.path import join\n",
    "import glob\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48674fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# log-linear integration from FIAT\n",
    "def get_logcov(rp_lst):\n",
    "    f_lst = [1 / i for i in rp_lst]\n",
    "    lf = [np.log(1 / i) for i in rp_lst]\n",
    "    c = [(1 / (lf[i] - lf[i+1])) for i in range(len(rp_lst[:-1]))]\n",
    "    G = [(f_lst[i] * lf[i] - f_lst[i]) for i in range(len(rp_lst))]\n",
    "    a = [((1 + c[i] * lf[i+1]) * (f_lst[i] - f_lst[i+1]) + c[i] * (G[i+1] - G[i])) for i in range(len(rp_lst[:-1]))]\n",
    "    b = [(c[i] * (G[i] - G[i+1] + lf[i+1] * (f_lst[i+1] - f_lst[i]))) for i in range(len(rp_lst[:-1]))]\n",
    "    if len(rp_lst) == 1:\n",
    "        cov_lst = f_lst\n",
    "    else:\n",
    "        cov_lst = [b[0] if i == 0 else f_lst[i] + a[i-1] if i == len(rp_lst) - 1 else a[i-1] + b[i] for i in range(len(rp_lst))]\n",
    "\n",
    "    return cov_lst\n",
    "\n",
    "def loglin_trapz(exp, rp_lst=None, nyears=None, max_rp=None):\n",
    "    if rp_lst is None:\n",
    "        if nyears is None:\n",
    "            nyears = exp.size\n",
    "        rp_lst = (1/(1-np.arange(exp.size)/exp.size)*(nyears/exp.size))\n",
    "        # import pdb; pdb.set_trace()\n",
    "    exp_lst = np.sort(exp)\n",
    "    cov_lst = np.asarray(get_logcov(rp_lst.tolist()))\n",
    "    if max_rp:\n",
    "        exp_lst = exp_lst[rp_lst<=max_rp]\n",
    "        cov_lst = cov_lst[rp_lst<=max_rp]\n",
    "    risk = (cov_lst * exp_lst).sum()\n",
    "    return risk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94124cce",
   "metadata": {},
   "source": [
    "## read return values and simulated impact data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5c9431",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdir = r\"../../4_results\"\n",
    "rm = {'h_tsw':'h'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bfd9ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abaf55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "RPS = [0,2,5,10,50,100,500]\n",
    "dvars = ['qb', 'qp', 'h', 'p']\n",
    "df_rps = pd.read_csv(join(rdir, f'marginal_rps.csv'), index_col=0).rename(columns=rm)\n",
    "df_rps.loc[0,:] = df_rps.loc[1,:]\n",
    "df_rps = df_rps.loc[RPS, dvars]\n",
    "df_rps.index.name = 'rps'\n",
    "df_rps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c882a480",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_cols = ['h_rp','p_rp','qb_rp','qp_rp']\n",
    "drop_cols = ['scen','finished']#, 'h','p','qb','qp']\n",
    "exp_cols = {'buildings_value': 'dam', 'population_count': 'pop'}\n",
    "\n",
    "# read impacts and convert to 4D xarray\n",
    "impacts = []\n",
    "psfs = ['', '_dt0']\n",
    "for i, postfix in enumerate(psfs):\n",
    "    fn_impact = join(rdir, f'impact_bias0{postfix}.csv')\n",
    "    df_impact = pd.read_csv(fn_impact, index_col=0).rename(columns={'h_tsw_rp':'h_rp'})\n",
    "    # df_flood[index_cols] = np.maximum(1, df_flood[index_cols])\n",
    "    # df_flood = df_impact.reset_index(drop=True)\n",
    "    ds_impact0 = df_impact.reset_index(drop=True).drop(columns=index_cols+drop_cols).set_index(\n",
    "        pd.MultiIndex.from_frame(df_impact[index_cols])\n",
    "    ).to_xarray()#.fillna(0)  # fillna required to avoid nans as xarray looks at neighbors even for exact simulated locations\n",
    "    # set return values on axis\n",
    "    for dvar in dvars:\n",
    "        ds_impact0[dvar] = xr.Variable(f'{dvar}_rp', df_rps[dvar].values)\n",
    "    ds_impact = ds_impact0.swap_dims({f'{dvar}_rp': dvar for dvar in df_rps.columns})\n",
    "    impacts.append(ds_impact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f79a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = dict(\n",
    "    # indep0 = pd.read_csv(join(rdir, r'sim_EVENTS_indep_rp.csv'), index_col=0).rename(columns=rm),\n",
    "    # indep = pd.read_csv(join(rdir, r'sim_EVENTS_indep1.csv'), index_col=0).rename(columns=rm),\n",
    "    # fulldep = pd.read_csv(join(rdir, r'sim_EVENTS_fulldep1_rp.csv'), index_col=0).rename(columns=rm),\n",
    "    # obs0 = pd.read_csv(join(rdir, r'sim_EVENTS.csv'), index_col=0).rename(columns=rm),\n",
    "    indep = pd.read_csv(join(rdir, r'sim_EVENTS_indep1_30k.csv'), index_col=0).rename(columns=rm),\n",
    "    fulldep = pd.read_csv(join(rdir, r'sim_EVENTS_fulldep1_30k.csv'), index_col=0).rename(columns=rm),\n",
    "    obs = pd.read_csv(join(rdir, r'sim_EVENTS_30k.csv'), index_col=0).rename(columns=rm),\n",
    "    # obs_lag2d = pd.read_csv(join(rdir, r'sim_EVENTS_30k_lag2dx.csv'), index_col=0).rename(columns=rm),\n",
    "    # obs_lag5d = pd.read_csv(join(rdir, r'sim_EVENTS_30k_lag5dx.csv'), index_col=0).rename(columns=rm),\n",
    ")\n",
    "for postfix in psfs[1:]:\n",
    "    for key in ['indep', 'fulldep', 'obs']:\n",
    "        samples[f'{key}{postfix}'] = samples[key].copy(deep=True)\n",
    "for key in samples:\n",
    "    df0  = samples[key].copy()\n",
    "    for dvar in dvars:\n",
    "        df0[dvar] = np.minimum(np.maximum(df0[dvar], df_rps.loc[0,dvar]), df_rps.loc[500,dvar])\n",
    "    ds0 = df0[dvars + ['year']].to_xarray().set_coords('year')\n",
    "    postfix = '_' + key.split('_')[-1]\n",
    "    if postfix in psfs[1:]:\n",
    "        ds_impact = impacts[psfs.index(postfix)]\n",
    "    else:\n",
    "        ds_impact = impacts[0]\n",
    "    ds0_impact = ds_impact.interp(ds0, method='linear')\n",
    "    for col in ds_impact.data_vars.keys():\n",
    "        assert not np.isnan(ds0_impact[col]).any()\n",
    "        samples[key][col] = ds0_impact[col].to_series()#.fillna(0)\n",
    "\n",
    "samples.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf49a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dvar = 'h'\n",
    "_, ax = plt.subplots(1,1)\n",
    "\n",
    "for key in samples:\n",
    "    df0 =samples[key][dvar].copy(deep=True).sort_values().to_frame()#.plot()\n",
    "    n, nyears = df0.index.size, samples[key]['year'].max()\n",
    "    df0['rp'] = (1/(1-np.arange(n)/n)*(nyears/n))\n",
    "    ax.plot(df0['rp'], df0[dvar], label=key)\n",
    "\n",
    "\n",
    "ax.set_xlim([0,5000])\n",
    "ax.set_ylim([df0.loc[df0['rp']>1, dvar].values[0], 10])\n",
    "ax.grid()\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9fdf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df0  = samples['indep'].copy()\n",
    "# for dvar in dvars:\n",
    "#     df0[dvar] = np.minimum(np.maximum(df0[dvar], df_rps.loc[0,dvar]), df_rps.loc[500,dvar])\n",
    "# ds0 = df0[dvars + ['year']].to_xarray().set_coords('year')\n",
    "# for dvar in dvars:\n",
    "#     samples[dvar] = df0.copy(deep=True)\n",
    "#     ds0_impact = ds_impact.isel(**{d: 0 for d in dvars if d != dvar}\n",
    "#         ).interp({dvar: ds0[dvar].groupby('year').max()}, method='linear')\n",
    "#     for col in ds_impact.data_vars.keys():\n",
    "#         assert not np.isnan(ds0_impact[col]).any()\n",
    "#         samples[dvar][col] = ds0_impact[col].to_series()#.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3721a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "samples_uni = {\n",
    "    'h': ds_impact0.sel(p_rp=0, qb_rp=0, qp_rp=0).reset_coords(drop=True).to_dataframe(),\n",
    "    'p': ds_impact0.sel(h_rp=0, qb_rp=0, qp_rp=0).reset_coords(drop=True).to_dataframe(),\n",
    "    'qp': ds_impact0.sel(h_rp=0, qb_rp=0, p_rp=0).reset_coords(drop=True).to_dataframe(),\n",
    "    'qb': ds_impact0.sel(h_rp=0, qp_rp=0, p_rp=0).reset_coords(drop=True).to_dataframe()\n",
    "}\n",
    "\n",
    "ds_fulldep = xr.merge([xr.DataArray(dims='rps', name=dvar, data=ds_impact0[dvar].values) for dvar in ds_impact0.coords])\n",
    "# samples_uni['fulldep0'] =  ds_impact0.sel(ds_fulldep).to_dataframe().set_index('h_rp')#[['dam', 'ppl']]\n",
    "\n",
    "for key in samples_uni:\n",
    "    samples_uni[key].index.name = 'rp'\n",
    "    samples_uni[key].index = np.maximum(1,samples_uni[key].index.values)\n",
    "\n",
    "samples_uni['h']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269c7daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "risk = {str(col): {} for col in next(iter(samples.values())).columns.values if col[:3] in ['dam', 'ppl']}\n",
    "dfs = deepcopy(risk)\n",
    "\n",
    "# univariate and full dep\n",
    "for key in samples_uni:\n",
    "    for exp in ['dam', 'ppl']: #risk.keys():\n",
    "        df0 = samples_uni[key][[exp]]\n",
    "        risk[exp][key] = loglin_trapz(df0[exp].values, df0.index.values)\n",
    "    \n",
    "# obs dep\n",
    "nyears=1000\n",
    "m = 30e3//(nyears)\n",
    "for key in list(samples.keys()):\n",
    "    exps = list(risk.keys()) if key == 'obs' else ['dam', 'ppl']\n",
    "    for exp in exps:\n",
    "        df0 = samples[key][['year', exp]].groupby('year').sum().reset_index()\n",
    "        # df0['mil'] =  (df0.index.values//nyears + 1).astype(int)\n",
    "        df0['mil'] = ((df0['year']-1)//nyears+1).astype(int)\n",
    "        dfs[exp][key] = pd.DataFrame({i: np.sort(grp[exp].values) for i,grp in df0[df0['mil']<=m][['mil', exp]].groupby('mil')})\n",
    "        risk[exp][f'{key}'] = loglin_trapz(df0.loc[df0['year']<=10000, exp].values, nyears=10000)\n",
    "        out = []\n",
    "        for i, df0_mil in df0.groupby('mil'):\n",
    "            if np.unique(df0_mil['year']).size != nyears: \n",
    "                break\n",
    "            out.append(loglin_trapz(df0_mil[exp].values, nyears=nyears))\n",
    "        out = np.asarray(out)\n",
    "        risk[exp][f'{key}_p05'], risk[exp][f'{key}_med'], risk[exp][f'{key}_p95'] = np.percentile(out, [5,50,95])\n",
    "        risk[exp][f'{key}_min'], risk[exp][f'{key}_max'], risk[exp][f'{key}_mean'] = out.min(), out.max(), out.mean()\n",
    "    #     break\n",
    "    # break\n",
    "df_risk = pd.DataFrame(risk)\n",
    "df_risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5cf661",
   "metadata": {},
   "outputs": [],
   "source": [
    "exps = ['dam', 'ppl']\n",
    "df_risk.loc[['indep_p05', 'indep_med', 'indep_p95', 'obs_p05', 'obs_med', 'obs_p95', 'fulldep_p05', 'fulldep_med', 'fulldep_p95'], exps] #- df_risk.loc['indep', exps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844aa1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_risk.loc['fulldep_med', exps]-df_risk.loc['indep_med', exps]) / df_risk.loc['obs_med', exps] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_risk.loc['obs_dt0_med', exps] / df_risk.loc['obs_med', exps] * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a1bd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "labs = {\n",
    "    'indep': 'independence',\n",
    "    'fulldep': 'full dependence',\n",
    "    # 'fulldep0': 'full dependence',\n",
    "    'obs': 'obs. dependence',\n",
    "    # 'magInd_timeInd': 'full indep.',\n",
    "    # 'magInd_timeObs': 'independence',\n",
    "    # 'magObs_timeObs_sum': 'obs. dependence',\n",
    "    # 'magInd_timeInd_sum': 'full indep.',\n",
    "    # 'magInd_timeObs_sum': 'independence',\n",
    "    'p': 'pluvial', \n",
    "    'qp': 'fluvial Pungwe', \n",
    "    'qb': 'fluvial Buzi', \n",
    "    'h': 'coastal', \n",
    "}\n",
    "rps= [1,2,5,10,25,50,100,250,500]\n",
    "ylabs = {\n",
    "    'dam': 'annual building damage [mil. USD]',\n",
    "    'ppl': 'annual people exposed [x 1000]'\n",
    "}\n",
    "legend_titles = {\n",
    "    'dam': '(EAD [mil. USD])',\n",
    "    'ppl': '(EAAP [x 1000])'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12fbf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import colors, patheffects\n",
    "ann_kwargs = dict(\n",
    "    xytext=(0, 0),\n",
    "    textcoords=\"offset points\",\n",
    "    zorder=4,\n",
    "    path_effects=[\n",
    "        patheffects.Stroke(linewidth=3, foreground=\"w\"),\n",
    "        patheffects.Normal(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "levels = ['low', 'middle', 'high']\n",
    "exp0, factor = 'ppl', 1e3\n",
    "# exp0, factor = 'dam', 1e6\n",
    "exps = [d for d in df_risk.columns if d[:3] == exp0]\n",
    "drr_dict = {}\n",
    "for stat in ['med', 'p05', 'p95']:\n",
    "    df_drr = df_risk.loc[f'obs_{stat}', exps].to_frame() / factor\n",
    "    base, df_drr = df_drr.iloc[0], df_drr.iloc[1:]\n",
    "    drr = [n.split('_')[1] for n in df_drr.index ]\n",
    "    df_drr['lvl'], df_drr['drr'] = zip(*[(int(''.join([s for s in drr if s.isdigit()])), ''.join([s for s in drr if not s.isdigit()])) for drr in drr])\n",
    "    df_drr['level'] = np.tile(levels, (3))\n",
    "    df_drr1 = df_drr.set_index(['drr', 'level'], drop=True)[f'obs_{stat}'].unstack().loc[['zoning', 'dryproof', 'dikes'], levels]\n",
    "    drr_dict[stat] = df_drr1\n",
    "yerr = np.stack([(drr_dict['p95']-drr_dict['med']).values, (drr_dict['med']-drr_dict['p05']).values]).swapaxes(0,1)\n",
    "ax = drr_dict['med'].plot.bar(rot=0, width=0.8, yerr=yerr, error_kw=dict(lw=2, capsize=3, capthick=1))\n",
    "ax1 = ax.twinx()\n",
    "ymin,ymax = ax.get_ylim()\n",
    "ax1.set_ylim([0, ymax/base.item()*100])\n",
    "ax.set_ylabel('expected '+ylabs[exp0])\n",
    "ax.set_xlabel('')\n",
    "ax1.set_ylabel('percentage of base risk [%]')\n",
    "ax.legend(title='protection level')\n",
    "\n",
    "for i, p in enumerate(ax.patches):\n",
    "    j,k = i // 3, i % 3\n",
    "    lvl, drr = df_drr.iloc[j+(k*3)]['lvl'], df_drr.iloc[j+(k*3)]['drr']\n",
    "    txt = f'rp {lvl}' if drr != 'dryproof' else f'{lvl} cm'\n",
    "    ax.annotate(txt, (p.get_x() + p.get_width()*0.25, base.item()*0.01), rotation=90, **ann_kwargs)\n",
    "\n",
    "plt.savefig(join(rdir, f'drr_{exp0}{postfix}.png'), dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cc7678",
   "metadata": {},
   "outputs": [],
   "source": [
    "(1 - df_drr1 / base.item()).round(3)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efe38b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "(base.item() - df_drr1).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b381af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(1,1, figsize=(6,4))\n",
    "# # exp0, factor = 'ppl', 1e3\n",
    "# exp0, factor = 'dam', 1e6\n",
    "# key = 'obs'\n",
    "# for drr in [f'{exp0}_zoning005', f'{exp0}_dryproof100', f'{exp0}_dikes010']:\n",
    "#     risk0 = df_risk.loc[f'{key}_med', drr]/factor\n",
    "#     lab = f'{drr} ({risk0:.2f})'\n",
    "#     ls = '-' if 'obs' in key else '--'\n",
    "#     lw = 1.5 if key == 'obs' else 1.5\n",
    "#     df0 = dfs[drr][key].quantile([0.05,0.5,0.95], axis=1).T/factor\n",
    "#     n = df0.index.size\n",
    "#     df0['rp'] = (1/(1-np.arange(n)/n)*(nyears/n))\n",
    "#     df0 = df0.set_index('rp')\n",
    "#     ax.fill_between(df0.index.values, df0[0.05], df0[0.95], alpha=0.5, zorder=0)\n",
    "#     df0[0.5].plot(ls=ls, lw=lw, ax=ax, label=lab)\n",
    "# ax.set_xlim([1, 500])\n",
    "# if 'dam' in exp0:\n",
    "#     ax.set_ylim([0, 350])\n",
    "# else:\n",
    "#     ax.set_ylim([0, 150])\n",
    "# ax.set_xscale('log')\n",
    "# ax.set_xticks(rps)\n",
    "# ax.set_xticklabels(rps)\n",
    "# ax.set_ylabel(f'{ylabs[exp0]}')\n",
    "# ax.set_xlabel('return period [year]')\n",
    "# legend = ax.legend(title=f'compound scenario\\n {legend_titles[exp0]}', loc='upper left')#, fontsize='large')\n",
    "# ax.set_title('Compound flood risk - adaptation scenario')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be1c555",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(6,4))\n",
    "exp0, factor = 'ppl', 1e3\n",
    "exp0, factor = 'dam', 1e6\n",
    "ls = '-'\n",
    "lw = 2.5\n",
    "for key in ['fulldep', 'indep', 'obs']:\n",
    "    risk0 = df_risk.loc[f'{key}_med', exp0]/factor\n",
    "    lab = f'{labs[key]} ({risk0:.2f})'\n",
    "    df0 = dfs[exp0][key].quantile([0.05,0.5,0.95], axis=1).T/factor\n",
    "    n = df0.index.size\n",
    "    df0['rp'] = (1/(1-np.arange(n)/n)*(nyears/n))\n",
    "    df0 = df0.set_index('rp')\n",
    "    ax.fill_between(df0.index.values, df0[0.05], df0[0.95], alpha=0.3, zorder=0)#, label='0.05-095 quantile')\n",
    "    df0[0.5].plot(ls=ls, lw=lw, ax=ax, label=lab, zorder=1)\n",
    "ax.set_xlim([1, 500])\n",
    "if 'dam' in exp0:\n",
    "    ax.set_ylim([0, 550])\n",
    "else:\n",
    "    ax.set_ylim([0, 250])\n",
    "ax.set_xscale('log')\n",
    "ax.set_xticks(rps)\n",
    "ax.set_xticklabels(rps)\n",
    "ax.set_ylabel(f'{ylabs[exp0]}')\n",
    "ax.set_xlabel('return period [year]')\n",
    "legend = ax.legend(title=f'compound scenario\\n {legend_titles[exp0]}', loc='upper left')#, fontsize='large')\n",
    "ax.set_title('Compound flood risk - base scenario')\n",
    "# ax.set_yticks(np.arange(0,500,10))\n",
    "# ax.grid()\n",
    "plt.savefig(join(rdir, f'risk_curve_{exp0}_compound{postfix}.png'), dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdc9ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(6,4))\n",
    "# exp0, factor = 'ppl', 1e3\n",
    "# exp0, factor = 'dam', 1e6\n",
    "for key in ['qp', 'qb', 'h', 'p']:\n",
    "    risk0 = df_risk.loc[key, exp0]/factor\n",
    "    lab = f'{labs[key]} ({risk0:.2f})'\n",
    "    df0 = samples_uni[key].copy()\n",
    "    (df0[exp0]/factor).plot(ls=ls, lw=lw, ax=ax, label=lab)\n",
    "ax.set_xlim([1, 500])\n",
    "if 'dam' in exp0:\n",
    "    ax.set_ylim([0, 550])\n",
    "else:\n",
    "    ax.set_ylim([0, 250])\n",
    "ax.set_xscale('log')\n",
    "ax.set_xticks(rps)\n",
    "ax.set_xticklabels(rps)\n",
    "ax.set_ylabel(f'{ylabs[exp0]}')\n",
    "ax.set_xlabel('return period [year]')\n",
    "legend = ax.legend(title=f'single driver scenario\\n {legend_titles[exp0]}', loc='upper left')#, fontsize='large')\n",
    "ax.set_title('Univariate flood risk - base scenario')\n",
    "plt.savefig(join(rdir, f'risk_curve_{exp0}_single{postfix}.png'), dpi=300, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29a1973",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "38648ff479392915c1a5d77722aa6edad827edf2098e3798b9c4282ba45e9fb7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 ('compound_risk')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
