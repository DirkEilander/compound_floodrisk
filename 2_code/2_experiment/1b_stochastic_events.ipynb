{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b097498",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "from os.path import join\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f136bfa0",
   "metadata": {},
   "source": [
    "## read peaks data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88119fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data directory\n",
    "ddir = r'../../1_data/2_forcing'\n",
    "rdir = r\"../../4_results\"\n",
    "\n",
    "# data labels\n",
    "labels = {\n",
    "    'qb': 'Discharge Buzi\\n[m3/s]',\n",
    "    'qp': 'Discharge Pungwe\\n[m3/s]',\n",
    "    'p': 'Rainfall\\n[mm/hr]',\n",
    "    't': 'Tide\\n[m+MSL]',\n",
    "    's': 'Surge\\n[m]',\n",
    "    'w': 'Sign. wave height\\n[m]',\n",
    "    'h_ts': 'Total waterlevel\\n[m+MSL]',\n",
    "    'h_tsw': 'Total waterlevel (incl. wave setup)\\n[m+MSL]',\n",
    "    'ss': 'Skew surge\\n[m]',\n",
    "    'ssw': 'Skew surge (incl. wave setup)\\n[m]',\n",
    "    'sw': 'Non-tidal residual\\n[m]'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a51edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read timeseries and peaks data\n",
    "period='AS-AUG'\n",
    "\n",
    "ds = xr.open_dataset(join(ddir, 'beira_drivers_daily.nc'))\n",
    "df_peaks0 = pd.read_csv(join(rdir, 'drivers_am_peaks.csv'), index_col=0, parse_dates=['time'])\n",
    "df_bm = df_peaks0.resample(period).max().dropna()\n",
    "\n",
    "drivers = ['p', 'qb', 'qp', 's', 'w']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332dadb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read distributions\n",
    "from eva import get_frozen_dist, rps_dist, emperical_dist, _RPS\n",
    "\n",
    "dist_params = pd.read_csv(join(rdir, 'marginal_params.csv'), index_col=0).rename({'h_tsw': 'h_tsw0'})\n",
    "dists = {}\n",
    "for dvar, row in dist_params.iterrows():\n",
    "    params = row[-2:] if row[0] == 'gumb' else row[-3:]\n",
    "    dists[dvar] = get_frozen_dist(params, row[0])\n",
    "\n",
    "# surge\n",
    "df_surge_emp_dist = pd.read_csv(join(rdir, 'marginal_surge.csv'), index_col=0)\n",
    "dists['s'] = rps_dist(df_surge_emp_dist['rp[year]'].values, df_surge_emp_dist['surge[m]'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f0311f",
   "metadata": {},
   "source": [
    "## analyse co-occurence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae742fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE that `max_time_lag` is the timelag between two consecutive extremes\n",
    "# the total time lag between all four events could theoretically be the sum of all lagtime per driver\n",
    "## settings\n",
    "max_time_lag = {\n",
    "    # 'qp': pd.Timedelta('5D'),\n",
    "    # 'qb': pd.Timedelta('5D'),\n",
    "}\n",
    "dt0 = pd.Timedelta('30D')  # default time lag\n",
    "slag = ''#'_lag2d'\n",
    "\n",
    "# reduce daily events to selected drivers\n",
    "df_peaks = df_peaks0[drivers].dropna(how='all')\n",
    "\n",
    "# combine daily events to event sets based on maximum time_lag\n",
    "df_peaks['dt'] = np.hstack([[0], np.diff(df_peaks.index)])\n",
    "df_peaks['max_dt'] = (df_peaks[drivers].notna()*np.array([max_time_lag.get(d,dt0) for d in drivers])).max(axis=1)\n",
    "# df_peaks['max_dt'] = [pd.Timedelta(days=d) for d in df_peaks['max_dt'].dt.days.rolling(2, min_periods=1).max().values]\n",
    "df_peaks['event'] = np.cumsum(~(df_peaks['dt']<df_peaks['max_dt']))\n",
    "df_events = df_peaks.groupby('event').apply(lambda x: x.notna().sum()).drop(columns=['dt', 'event'])\n",
    "df_events['n_extreme'] = df_events[drivers].sum(axis=1)\n",
    "df_events['time'] = df_peaks.reset_index().groupby('event').first()['time']\n",
    "offset = pd.Timedelta(days=df_bm.index.dayofyear.min()-1)\n",
    "df_events['year'] = (df_events['time'] - offset).dt.year\n",
    "df_events['time_lag'] = [evnt.iloc[1:,:]['dt'].sum() for _, evnt in df_peaks.groupby('event')]\n",
    "evnts, cnts = np.unique(df_peaks['event'], return_counts=True)\n",
    "df_peaks['co-occur'] = np.isin(df_peaks['event'], evnts[cnts>1])\n",
    "\n",
    "# make barcode plot for co-occuring events\n",
    "data = df_events[drivers[::-1]].T * df_events['n_extreme']\n",
    "_, ax = plt.subplots(1,1, figsize=(12,5))\n",
    "im = ax.imshow(data.where(data>0), interpolation='none', cmap='Blues', vmin=0, vmax=len(drivers))\n",
    "ax.set_aspect(2)\n",
    "ax.set_yticks(np.arange(len(drivers)))\n",
    "ax.set_yticklabels([labels[dvar].split('\\n')[0] for dvar in drivers[::-1]])\n",
    "# fig.colorbar(im, orientation='vertical')\n",
    "ax.set_xlabel('events timeline')\n",
    "xlabs = df_events.reset_index().groupby('year').first()[['event']]\n",
    "_ = ax.set_xticks(xlabs['event'].values[::2]-1.5)\n",
    "_ = ax.set_xticklabels(xlabs.index.values[::2], rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig(join(r'../../4_results', f'co-occurence_time{slag}.png'), dpi=300, bbox_axes='tight')\n",
    "\n",
    "\n",
    "# remove incomplete years\n",
    "check = df_events[['n_extreme', 'year']].groupby('year').sum()\n",
    "drop_years = check[check['n_extreme']!=len(drivers)].index.values\n",
    "if drop_years.size > 1:\n",
    "    df_events = df_events[~np.isin(df_events['year'], drop_years)]\n",
    "    print(f'ignore years with more/less events than drivers: {drop_years}')\n",
    "print(f'no. of valid years: {np.unique(df_events.year).size}')\n",
    "print(f'no. of events: {df_events.index.size}; (>1 extreme: {df_events[df_events.n_extreme>1].index.size})')\n",
    "print(f'max time lag: {df_events.time_lag.max().days} days')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf5b3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_events_dist = df_events.groupby(drivers + ['n_extreme']).size().reset_index(name='count')\n",
    "df_events_dist = df_events_dist.sort_values('count', ascending=False)\n",
    "\n",
    "\n",
    "fig, (ax1, ax) = plt.subplots(2,1, figsize=(7,3), sharex=True, gridspec_kw={'height_ratios': [2, 1], 'hspace':0.0})\n",
    "colors = ['k' for _ in range(5)] + ['orange' for _ in range(df_events_dist.index.size-5)]\n",
    "df_events_dist.reset_index()['count'].plot.bar(ax=ax1, color=colors)\n",
    "ax1.set_ylabel('count [-]')\n",
    "\n",
    "x, y = np.where(df_events_dist[drivers])\n",
    "colors = ['k' for _ in range(5)] + ['orange' for _ in range(x.size-5)]\n",
    "ax.scatter(x, y, color=colors)\n",
    "ax.set_ylim([-0.5, len(drivers)-0.5])\n",
    "ax.set_yticks(np.arange(len(drivers)))\n",
    "ax.set_yticklabels([labels[dvar].split('\\n')[0] for dvar in drivers])\n",
    "ax.set_xlabel('event type')\n",
    "ax.set_xticklabels('')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# plt.savefig(join(r'../../4_results', f'co-occurence_hist{slag}.png'), dpi=300, bbox_axes='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef4568d",
   "metadata": {},
   "source": [
    "## dependence modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f79ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyvinecopulib as pv\n",
    "\n",
    "# Transform copula data using the empirical distribution\n",
    "df_uobs = pd.DataFrame(columns=drivers, data = pv.to_pseudo_obs(df_bm[drivers].values))\n",
    "# family_set, postfix = [pv.BicopFamily(0)], '_indep' # independent only\n",
    "family_set, postfix = [pv.BicopFamily(x) for x in np.arange(0,11)], ''\n",
    "\n",
    "# fit copula\n",
    "controls = pv.FitControlsVinecop(\n",
    "    family_set=family_set, \n",
    "    # parametric_method='itau',\n",
    "    # nonparametric_method='quadratic'\n",
    "    threshold=0.05, # tau threshold   (0.0 default!)\n",
    "    selection_criterion='aic', # loglik, bic, aic (bic default!)\n",
    "    # tree_criterion='tau',  # tau, hoeffd, rho, mcor -> no effect ?!\n",
    "    show_trace=True,\n",
    ")\n",
    "cop = pv.Vinecop(data=df_uobs.values, controls=controls)#, structure=pv.RVineStructure(len(drivers)))\n",
    "# cop.structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9724c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "cop.structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f645f24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "m = cop.matrix\n",
    "n = m.shape[0]\n",
    "v = drivers\n",
    "for t in range(n-1):\n",
    "    # print(f'** Tree: {t:d}')\n",
    "    for e in range(n-1-t):\n",
    "        p1, p2 = v[int(m[n-1-e,e]-1)], v[int(m[t,e]-1)]\n",
    "        px = [v[int(p-1)] for p in m[:t,e]]\n",
    "        c = cop.get_pair_copula(t,e)\n",
    "        tau = cop.get_tau(t,e)  # NOTE: diffferent from scipy.stats method ?\n",
    "        pxs = f' | ' + ','.join(px) if px else ''\n",
    "        edge = f'{p1},{p2}{pxs}'\n",
    "        cstr = c.str().replace(f'\\n',',')\n",
    "        print(f'{p1},{p2}{pxs}: {cstr}; tau = {tau:.5f}')\n",
    "        data.append([t+1, e, edge, [p1,p2], px, c.str().split(',')[0], c.parameters.flatten(), tau])\n",
    "\n",
    "df_cop = pd.DataFrame(\n",
    "    data=data,\n",
    "    columns=['tree', 'edge#', 'edge', 'pair', 'conditional', 'copula', 'parameters', 'tau']\n",
    ")\n",
    "df_cop.set_index(['tree', 'edge#']).to_csv('copula.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91ff71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from the copula\n",
    "seeds = np.arange(len(drivers), dtype=int)\n",
    "np.random.RandomState(np.sum(seeds))\n",
    "n_sim = 30000\n",
    "df_usim, postfix = pd.DataFrame(data=cop.simulate(n_sim, seeds=seeds), columns=drivers), '_30k'\n",
    "df_usim, postfix = pd.DataFrame(data=np.random.random((n_sim, len(drivers))), columns=drivers), '_indep1_30k'\n",
    "df_usim, postfix = pd.DataFrame(data=np.tile(np.random.random((n_sim, 1)), len(drivers)), columns=drivers), '_fulldep1_30k'\n",
    "\n",
    "# transform back based on marginal distributions\n",
    "df_sim = pd.DataFrame()\n",
    "s = df_usim.index.size\n",
    "for dvar in drivers:\n",
    "    df_sim[dvar] = np.maximum(0, dists[dvar].ppf(df_usim[dvar].values))\n",
    "\n",
    "# df_sim_q = pd.DataFrame()\n",
    "# df_sim_q[dvar] = np.quantile(df_bm[dvar], df_usim[dvar])\n",
    "    \n",
    "df_sim.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2a9a36",
   "metadata": {},
   "source": [
    "## create stochastic event set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7762ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine co-occurence and simulations\n",
    "from itertools import cycle\n",
    "event_cycle = cycle(df_events.groupby('year'))\n",
    "sim_events = []\n",
    "for year, row in df_sim[drivers].iterrows():\n",
    "    event_yr = next(event_cycle)[1][drivers] \n",
    "    events = (event_yr * row)\n",
    "    events['year'] = year+1\n",
    "    sim_events.append(events)\n",
    "\n",
    "df_sim_events0 = pd.concat(sim_events, axis=0, ignore_index=True)\n",
    "df_sim_events = df_sim_events0\n",
    "df_sim_events0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d15d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# required for simulating h_tws & plot comparing sim with obs events\n",
    "xval_norm = {}\n",
    "thresh_lst = df_sim.quantile(0.01)\n",
    "s = df_sim_events0.index.size\n",
    "df_sim_events = df_sim_events0.copy(deep=True)\n",
    "for dvar in drivers:\n",
    "    # we randomly sample non-extremes from all values below the rp1 threshold\n",
    "    thresh = dists[dvar].ppf(1-1/1.01)\n",
    "    xval_norm = ds[dvar].where(ds[dvar] < thresh,drop=True).to_series().dropna()\n",
    "    x0 = xval_norm.sample(s, replace=True, random_state=np.sum(seeds)) \n",
    "    xs = df_sim_events0[dvar]\n",
    "    df_sim_events[dvar] = np.where(xs==0, x0, xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd354fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine waterlevel components\n",
    "tide = ds['t'].dropna('time').to_series()  # daily high tide\n",
    "tide_am = ds['t'].resample(time='AS').max('time').dropna('time').to_series()\n",
    "# combine surge with random daily high tide\n",
    "df_sim_events['t'] = tide.sample(s, replace=True, random_state=np.sum(seeds)).values\n",
    "\n",
    "hname = 'h_tsw' if 'w' in ds else 'h_ts' \n",
    "df_sim_events[hname] = df_sim_events['t'] + df_sim_events['s'] \n",
    "if 'w' in ds:\n",
    "    df_sim_events[hname] = df_sim_events[hname] + 0.2 * df_sim_events['w']\n",
    "# at least AM tide for events with extreme\n",
    "htot_am_idx = df_sim_events[[hname, 'year']].groupby('year').idxmax().values.flatten()\n",
    "h_am = np.maximum(df_sim_events[hname], tide_am.sample(s, replace=True, random_state=np.sum(seeds)).values)\n",
    "df_sim_events.loc[htot_am_idx, hname] = np.maximum(df_sim_events.loc[htot_am_idx, hname], h_am.loc[htot_am_idx])\n",
    "\n",
    "# get AM waterlevel\n",
    "df_sim_am_idx = df_sim_events.groupby('year').idxmax()\n",
    "df_sim_am = df_sim_events.groupby('year').max()\n",
    "df_sim_am.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b873af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eva import emperical_dist\n",
    "# get h rps\n",
    "df_sim_am0 = pd.read_csv(join(rdir, 'sim_AM.csv'), index_col=0)\n",
    "dists['h0'] = emperical_dist(df_sim_am0['h_tsw'].values, df_sim_am0['h_tsw'].size)\n",
    "n, nyears = df_sim_events.index.size, df_sim_am.index.size\n",
    "rps = (1/(1-np.arange(n)/n)*(nyears/n))\n",
    "df_sim_events.loc[df_sim_events['h_tsw'].sort_values().index, 'h_tsw'] = dists['h0'].ppf(1-1/rps)\n",
    "df_sim_am['h_tsw'] = df_sim_events.loc[df_sim_am_idx['h_tsw'].values, 'h_tsw'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c435835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results important to keep return values of h_tsw consistent with sensitivity analysis\n",
    "df_sim_am.round(3).to_csv(join(rdir, f'sim_AM{postfix}{slag}x.csv'))\n",
    "df_sim_events.round(3).to_csv(join(rdir, f'sim_EVENTS{postfix}{slag}x.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e0b2cb",
   "metadata": {},
   "source": [
    "### plot stochastic event set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d04de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get h rps\n",
    "df_sim_am0 = pd.read_csv(join(rdir, 'sim_AM.csv'), index_col=0)\n",
    "dists['h_tsw'] = emperical_dist(df_sim_am0['h_tsw'].values, df_sim_am0['h_tsw'].size)\n",
    "\n",
    "RPS = np.concatenate([[1.1], _RPS])\n",
    "df_rps = pd.DataFrame(columns=dists.keys(), index=RPS)\n",
    "for dvar in dists:\n",
    "    df_rps[dvar] = dists[dvar].ppf(1-1/RPS)\n",
    "\n",
    "df_rps.index = np.floor(RPS).astype(int)\n",
    "df_rps.index.name = 'rps'\n",
    "df_rps.to_csv(join(rdir, f'marginal_rps.csv'))\n",
    "df_rps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78505f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kendalltau, pearsonr, gaussian_kde\n",
    "n = len(drivers)-1\n",
    "fig, axes = plt.subplots(n,n, sharex=False, sharey=False, figsize=(n*2.5,n*2.5), gridspec_kw={'hspace':0.0, 'wspace':0.0})\n",
    "\n",
    "# df_uobs_cooc = df_uobs.where(df_peaks[drivers][df_peaks['co-occur']].resample(period).max().reset_index(drop=True).notna())\n",
    "df_obs_cooc = df_peaks[drivers][df_peaks['co-occur']].resample(period).max()\n",
    "\n",
    "for r in range(n):\n",
    "    for c in range(n):\n",
    "        if c > r:\n",
    "            axes[r,c].set_visible(False)\n",
    "            continue\n",
    "\n",
    "for _, c0 in df_cop.iterrows():\n",
    "    c = c0.edge \n",
    "    r = c + c0.tree\n",
    "    ax = axes[r,c]\n",
    "    xlab, ylab = c0.pair\n",
    "    x, y = df_bm[xlab].values, df_bm[ylab].values\n",
    "    xmin, xmax = np.quantile(x, [0,0.95])\n",
    "    ymin, ymax = np.quantile(y, [0,0.95])\n",
    "    ymax = ymax + (ymax-ymin)*0.4\n",
    "    \n",
    "    # if len(c0) > 0 and c0.copula != 'Independence':\n",
    "    xx, yy = np.mgrid[xmin:xmax:100j, ymin:ymax:100j]\n",
    "    positions = np.vstack([xx.ravel(), yy.ravel()])\n",
    "    # kernel = gaussian_kde(df_bm[[xlab,ylab]].values.T, bw_method=0.5)\n",
    "    kernel = gaussian_kde(df_sim[[xlab,ylab]].values.T, bw_method=0.5)\n",
    "    f = np.reshape(kernel(positions).T, xx.shape)\n",
    "    cmap = 'Blues' if  c0.copula != 'Independence' else 'Greens'\n",
    "    ax.contourf(xx, yy, f, cmap=cmap, alpha=0.5)\n",
    "\n",
    "    # plot events\n",
    "    x, y = df_bm[xlab].values, df_bm[ylab].values\n",
    "    xcooc, ycooc = df_obs_cooc[xlab].values, df_obs_cooc[ylab].values\n",
    "    ax.scatter(xcooc, ycooc, s=13, color='darkorange', label='compound', zorder=2)\n",
    "    ax.scatter(x, y, s=12, color='black', label='other', alpha=0.9, zorder=1)\n",
    "    if r == n-1 and c == n-2:\n",
    "        ax.legend(title='AM events', loc='lower left', bbox_to_anchor=(1.1, 1.1))\n",
    "    kwargs = dict(        \n",
    "        horizontalalignment='left',\n",
    "        verticalalignment='top',\n",
    "        transform=ax.transAxes,\n",
    "        bbox=dict(facecolor='white', alpha=0.5, lw=0.1)\n",
    "    )\n",
    "    # tau, ptau = kendalltau(x, y)\n",
    "    # rho, prho = pearsonr(x, y)\n",
    "    # tsig = ('**' if ptau < 0.05 else '*') if ptau < 0.10 else ''\n",
    "    # rsig = ('**' if prho < 0.05 else '*') if prho < 0.10 else ''\n",
    "    # name = c0.copula if c0.copula != 'Independence' else 'Indep.'\n",
    "    cs = ' | ' + ','.join(c0.conditional) if c0.tree > 0 else ''\n",
    "    ps = ','.join(c0.pair[::-1])\n",
    "    ns = f'{c0.copula}' + r' ($\\tau$=' + f'{c0.tau:.2f})' if c0.copula != 'Independence' else c0.copula\n",
    "    txt = f'[{ps}{cs}]\\n{ns}' #{tsig}; ' + r'$\\rho$='+f'{rho:.2f}{rsig})'\n",
    "    ax.text(0.02, 0.98, txt, **kwargs)\n",
    "\n",
    "    if c == 0:\n",
    "        ax.set_ylabel(labels[ylab].replace('\\n', f' ({ylab})\\n'))\n",
    "    else:\n",
    "        ax.set_yticklabels('')\n",
    "    if r == n-1:\n",
    "        ax.set_xlabel(labels[xlab].replace('\\n', f' ({xlab})\\n'))\n",
    "    ax.set_ylim([ymin, ymax])\n",
    "    ax.set_xlim([xmin, xmax])\n",
    "\n",
    "# plt.savefig(join(r'../../4_results', f'copula_autofit{postfix}.png'), dpi=300, bbox_axes='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ea3ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get maximum values within time window for non_extremes\n",
    "df_peaks0_filled = pd.DataFrame()\n",
    "for dvar in df_peaks0.columns:\n",
    "    df_peaks0_filled[dvar] = df_peaks0[dvar].where(df_peaks0[dvar].notna(), ds[dvar].sel(time=df_peaks0.index))\n",
    "    \n",
    "\n",
    "dvars = drivers + [hname]\n",
    "rm = {k:v for k,v in labels.items() if k in dvars}\n",
    "rm_lab = {k: v.replace(' (','\\n(') for k,v in rm.items()}\n",
    "df_obs_events = df_peaks0_filled[dvars].dropna(how='any')\n",
    "df_merged = pd.concat([\n",
    "    df_sim_events[dvars],\n",
    "    df_obs_events, \n",
    "    ], ignore_index=True, axis=0).rename(columns=rm)\n",
    "colors = np.hstack([\n",
    "    np.full(df_sim_events.index.size, \"k\"),\n",
    "    np.full(df_obs_events.index.size, \"r\"), \n",
    "    ])\n",
    "sizes = np.hstack([\n",
    "    np.full(df_sim_events.index.size, 5),\n",
    "    np.full(df_obs_events.index.size, 30), \n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdae5c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "axes = pd.plotting.scatter_matrix(df_merged.rename(columns=rm_lab), color=colors, s=sizes, alpha=1.0, diagonal=None, figsize=(10,10))\n",
    "for i, dvar in enumerate(dvars):\n",
    "    axes[i,i].lines.pop(0)\n",
    "    xmin, xmax = df_merged[rm[dvar]].quantile([0.0, 0.998]).values\n",
    "    rp1 = dists[dvar].ppf(1-1/1.01)\n",
    "    # rp1 = thresh_lst[dvar]\n",
    "    for j in range(len(dvars)):\n",
    "        axes[j,i].set_xlim([xmin, xmax])\n",
    "        axes[j,i].axvline(rp1, color='c', ls='-', label='rp1')#, lw=0.5)\n",
    "        if j != i: \n",
    "            axes[i,j].set_ylim([xmin, xmax])\n",
    "            axes[i,j].axhline(rp1, color='c', ls='--')#, lw=0.5)\n",
    "    # fix ticks ..\n",
    "    xticks = axes[-1,i].get_xticks()\n",
    "    xticks = xticks[np.logical_and(xticks>xmin, xticks<xmax)]\n",
    "    axes[-1,i].set_xticks(xticks)\n",
    "    axes[-1,i].set_xticklabels(xticks, rotation=0)\n",
    "    if i == 0:\n",
    "        ymin,ymax = axes[0,0].get_ylim()\n",
    "        axes[i,i].set_yticks(xticks*(ymax-ymin)/(xmax-xmin))\n",
    "        axes[i,i].set_yticklabels(xticks)\n",
    "    df_sim_events[dvar].plot.kde(ax=axes[i,i], color='k', label='sim', legend=i==0)\n",
    "    df_obs_events[dvar].plot.kde(ax=axes[i,i], color='r', label='obs', legend=i==0)\n",
    "    if i == 0:\n",
    "        axes[i,i].set_ylabel(rm[dvar])\n",
    "    else:\n",
    "        axes[i,i].yaxis.set_visible(False)\n",
    "\n",
    "for r in range(len(dvars)):\n",
    "    for c in range(len(dvars)):\n",
    "        if c >= r:\n",
    "            axes[r,c].set_visible(False)\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b125a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = pd.concat([\n",
    "    df_sim_am[dvars],\n",
    "    df_bm[dvars], \n",
    "    ], ignore_index=True, axis=0).rename(columns=rm_lab)\n",
    "colors = np.hstack([\n",
    "    np.full(df_sim_am.index.size, \"k\"),\n",
    "    np.full(df_bm.index.size, \"r\"), \n",
    "    ])\n",
    "sizes = np.hstack([\n",
    "    np.full(df_sim_am.index.size, 8),\n",
    "    np.full(df_bm.index.size, 30), \n",
    "    ])\n",
    "axes = pd.plotting.scatter_matrix(df_merged, color=colors, s=sizes, alpha=1.0, diagonal=None, figsize=(10,10))\n",
    "for i, dvar in enumerate(dvars):\n",
    "    xmin, xmax = df_merged[rm_lab[dvar]].quantile([0.02, 0.998]).values\n",
    "    for j in range(len(dvars)):\n",
    "        axes[j,i].set_xlim([xmin, xmax])\n",
    "        if j != i: \n",
    "            axes[i,j].set_ylim([xmin, xmax])\n",
    "    # fix ticks ..\n",
    "    xticks = axes[-1,i].get_xticks()\n",
    "    xticks = xticks[np.logical_and(xticks>xmin, xticks<xmax)]\n",
    "    axes[-1,i].set_xticks(xticks)\n",
    "    axes[-1,i].set_xticklabels(xticks, rotation=0)\n",
    "    if i == 0:\n",
    "        ymin,ymax = axes[0,0].get_ylim()\n",
    "        axes[i,i].set_yticks(xticks*(ymax-ymin)/(xmax-xmin))\n",
    "        axes[i,i].set_yticklabels(xticks)\n",
    "    \n",
    "    # axes[i,i].lines.pop(0)\n",
    "    # df_sim_am[dvar].plot.kde(ax=axes[i,i], color='k', label='sim', legend=i==0)\n",
    "    # if dvar != 's':\n",
    "    #     df_bm[dvar].plot.kde(ax=axes[i,i], color='r', label='obs', legend=i==0)\n",
    "    # else:\n",
    "    #     df_bm[dvar][:-2].plot.kde(ax=axes[i,i], color='r', label='obs', legend=i==0)\n",
    "    # if i == 0:\n",
    "    #     axes[i,i].set_ylabel(rm_lab[dvar])\n",
    "    # else:\n",
    "    #     axes[i,i].yaxis.set_visible(False)\n",
    "    \n",
    "for r in range(len(dvars)):\n",
    "    for c in range(len(dvars)):\n",
    "        if c >= r:\n",
    "            axes[r,c].set_visible(False)\n",
    "            continue\n",
    "\n",
    "axes[1,0].plot(0,0, f'.r', label='sim')\n",
    "axes[1,0].plot(0,0, f'.k', label='obs')\n",
    "axes[1,0].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(left=0.1,right=1,bottom=0.1,top=1, wspace=0, hspace=0)\n",
    "plt.savefig(join(r'../../4_results', f'stochastic_events_AM{postfix}.png'), dpi=300, bbox_axes='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f230a5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "38648ff479392915c1a5d77722aa6edad827edf2098e3798b9c4282ba45e9fb7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 ('compound_risk')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
