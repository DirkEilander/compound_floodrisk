{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b097498",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "from os.path import join\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f136bfa0",
   "metadata": {},
   "source": [
    "## read and align data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88119fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data directory\n",
    "ddir = r'../../1_data/2_forcing'\n",
    "rdir = r\"../../4_results\"\n",
    "\n",
    "# data labels\n",
    "labels = {\n",
    "    'qb': 'Discharge Buzi\\n[m3/s]',\n",
    "    'qp': 'Discharge Pungwe\\n[m3/s]',\n",
    "    'p': 'Rainfall\\n[mm/hr]',\n",
    "    'h_tsw': 'Total waterlevel (incl. wave setup)\\n[m+MSL]',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d7658e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eva import get_peaks, get_peak_hydrographs\n",
    "\n",
    "# discharge\n",
    "fnq = join(ddir, 'cama_discharge_beira_daily.nc')\n",
    "daq = xr.open_dataset(fnq)['discharge'].load()\n",
    "dsq = xr.merge([\n",
    "    daq.sel(index=1).rename('qb').reset_coords(drop=True),\n",
    "    daq.sel(index=4).rename('qp').reset_coords(drop=True)\n",
    "])\n",
    "# fnq = r'../../3_models/wflow/run_vito_ksath1000/output_src.nc'\n",
    "# daq = xr.open_dataset(fnq)['q_river'].load()\n",
    "# dsq = xr.merge([\n",
    "#     daq.sel(index=0).rename('qb').reset_coords(drop=True),\n",
    "#     daq.sel(index=3).rename('qp').reset_coords(drop=True)\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc33d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GTSM waterlevels + ERA5 waves\n",
    "# contains: \"waterlevel\" (tide+surge), \"tide\", \"surge\", \"shww\"\n",
    "fnh = join(ddir, 'reanalysis_gtsm_v1_beira_extended.nc')\n",
    "dsh0 = xr.open_dataset(fnh).load()\n",
    "dsh0 = dsh0.rename({'waterlevel': 'h_ts', 'surge': 's', 'tide': 't', 'shww': 'w'})\n",
    "dsh0['h_tsw'] = dsh0['h_ts'] + 0.2*dsh0['w']\n",
    "dsh0['sw'] = dsh0['s'].fillna(0) + 0.2*dsh0['w']\n",
    "# skew surge (not used)\n",
    "# high_tide = get_peaks(dsh0['t'].load(), period='12H').dropna('time').reindex_like(dsh0, 'nearest')\n",
    "# dsh0['ss'] = dsh0['h_ts'] - high_tide\n",
    "# dsh0['ssw'] = dsh0['h_tsw'] - high_tide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be459f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ERA5 precipitation\n",
    "fnp = join(ddir, 'era5_precip_beira_hourly_spatialmean.nc')\n",
    "dap0 = xr.open_dataset(fnp, chunks='auto')['precip'].load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4bae96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read timeseries and peaks data\n",
    "period='AS-AUG'\n",
    "\n",
    "ds = xr.open_dataset(join(ddir, 'beira_drivers_daily.nc'))\n",
    "df_peaks0 = pd.read_csv(join(rdir, 'drivers_am_peaks.csv'), index_col=0, parse_dates=['time'])\n",
    "ds_peaks = df_peaks0.to_xarray().reindex_like(ds)\n",
    "df_bm = df_peaks0.resample(period).max().dropna()\n",
    "\n",
    "drivers = ['qb', 'qp', 'p', 's', 'w']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7728c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read distributions\n",
    "from eva import get_frozen_dist, rps_dist, emperical_dist, _RPS\n",
    "\n",
    "dist_params = pd.read_csv(join(rdir, 'marginal_params.csv'), index_col=0).rename({'h_tsw': 'h_tsw0'})\n",
    "dists = {}\n",
    "for dvar, row in dist_params.iterrows():\n",
    "    params = row[-2:] if row[0] == 'gumb' else row[-3:]\n",
    "    dists[dvar] = get_frozen_dist(params, row[0])\n",
    "\n",
    "# surge\n",
    "df_surge_emp_dist = pd.read_csv(join(rdir, 'marginal_surge.csv'), index_col=0)\n",
    "dists['s'] = rps_dist(df_surge_emp_dist['rp[year]'].values, df_surge_emp_dist['surge[m]'].values)\n",
    "\n",
    "# get h rps\n",
    "df_sim_am0 = pd.read_csv(join(rdir, 'sim_AM.csv'), index_col=0)\n",
    "dists['h_tsw'] = emperical_dist(df_sim_am0['h_tsw'].values, df_sim_am0['h_tsw'].size)\n",
    "\n",
    "df_rps = pd.DataFrame(columns=dists.keys(), index=_RPS)\n",
    "df_rps.index.name = 'rps'\n",
    "for dvar in dists:\n",
    "    df_rps[dvar] = dists[dvar].ppf(1-1/_RPS)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c6aadff4",
   "metadata": {},
   "source": [
    "## analyse lag-times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb86a80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# correlation ufunc function\n",
    "# from http://xarray.pydata.org/en/stable/dask.html#automatic-parallelization\n",
    "def _covariance(x, y):\n",
    "    return np.nanmean(\n",
    "        (x - np.nanmean(x, axis=-1, keepdims=True))\n",
    "        * (y - np.nanmean(y, axis=-1, keepdims=True)),\n",
    "        axis=-1,\n",
    "    )\n",
    "\n",
    "\n",
    "def _pearson_correlation(x, y):\n",
    "    return _covariance(x, y) / (np.nanstd(x, axis=-1) * np.nanstd(y, axis=-1))\n",
    "\n",
    "def pearson_correlation(sim, obs, dim=\"time\"):\n",
    "    \"\"\"Returns the Pearson correlation coefficient of two time series.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sim : xarray DataArray\n",
    "        simulations time series\n",
    "    obs : xarray DataArray\n",
    "        observations time series\n",
    "    dim : str, optional\n",
    "        name of time dimension in sim and obs (the default is 'time')\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    xarray DataArray\n",
    "        the pearson correlation coefficient\n",
    "    \"\"\"\n",
    "    # wrap numpy function\n",
    "    kwargs = dict(\n",
    "        input_core_dims=[[dim], [dim]], dask=\"parallelized\", output_dtypes=[float]\n",
    "    )\n",
    "    pearsonr = xr.apply_ufunc(_pearson_correlation, sim, obs, **kwargs)\n",
    "    pearsonr.name = \"pearson_coef\"\n",
    "    return pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7124e868",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "def time_lag_crosscorr(\n",
    "    sim, obs, quantile=None, lags=np.arange(-10,11,1), t_unit='days', dim='time'\n",
    "):\n",
    "    \"\"\"Returns the time lag between two time series based on a lag time \n",
    "    with the maximum pearson correlation.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sim : xarray DataArray\n",
    "        simulations time series\n",
    "    obs : xarray DataArray\n",
    "        observations time series\n",
    "    quantile : numpy ndarray, optional\n",
    "        quantile based threshold (the default is None, which does not use any threshold)\n",
    "    lags : numpy ndarray, optional\n",
    "        range of considered lag times (the default is np.arange(-10,11,1))\n",
    "    t_unit : str, optional\n",
    "        time unit used to parse lags to timedelta format (the default is 'days')\n",
    "    dim : str, optional\n",
    "        name of time dimension in sim and obs (the default is 'time')\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    xarray DataSet\n",
    "        lag time and associated correlation coefficient\n",
    "    \"\"\"\n",
    "\n",
    "    if quantile:\n",
    "        obs.load()        \n",
    "        obs = obs.where(obs>=obs.quantile(quantile, dim=dim))\n",
    "    # loop through time lags and calculate cross correlation\n",
    "    r = []\n",
    "    lags = np.asarray(lags)\n",
    "    time_org = sim[dim].to_index()\n",
    "    for dt in lags:\n",
    "        time_new = time_org + timedelta(**{t_unit: float(dt)})\n",
    "        ts = slice(max(time_org.min(), time_new.min()), min(time_org.max(), time_new.max()))\n",
    "        sim[dim] = time_new\n",
    "        r.append(pearson_correlation(sim.sel(**{dim:ts}), obs.sel(**{dim:ts})))\n",
    "    sim[dim] = time_org # reset time\n",
    "    pearsonr = xr.concat(r, dim='dt')\n",
    "    pearsonr['dt'] = xr.Variable('dt', lags)\n",
    "    # get maximum cross corr\n",
    "    pearsonr_max = pearsonr.max(dim='dt')\n",
    "    pearsonr_max.name = 'lag_rho'\n",
    "    pearsonr_max.attrs.update(description='maximum pearson coefficient for given time lag')\n",
    "    # get lag time of maximum cross corr\n",
    "    # NOTE that we assume a evenly spaced lag times\n",
    "    lag = xr.where(\n",
    "        np.isfinite(pearsonr).sum(dim='dt')==lags.size,\n",
    "        pearsonr.argmax(dim='dt', skipna=False), \n",
    "        np.nan)*np.diff(lags)[0] + lags.min()\n",
    "    lag.name = 'lag'\n",
    "    lag.attrs.update(description='time lag with maximum pearson coefficient', unit=t_unit)\n",
    "    # merge max cross corr and lag tiem\n",
    "    return xr.merge([lag, pearsonr_max]), pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60aef732",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_dvar = 'qb'\n",
    "dt_lst =[]\n",
    "fig, ax = plt.subplots(1,1)\n",
    "dvar_lst = ['qp', 'p', 's', 'w']\n",
    "for dvar in dvar_lst:\n",
    "    da_out, pearsonr = time_lag_crosscorr(ds[ref_dvar], ds[dvar])\n",
    "    dt_lst.append(da_out.reset_coords(drop=True).compute())\n",
    "    # rlist.append(rlist)\n",
    "    pearsonr.plot(label=dvar, ax=ax)\n",
    "ds_dt = xr.concat(dt_lst, dim='dvar')\n",
    "ds_dt['dvar'] = xr.IndexVariable('dvar', dvar_lst)\n",
    "df_timelags = ds_dt['lag'].to_series()\n",
    "df_timelags.to_csv(r'../../4_results/lagtimes.csv')\n",
    "ax.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3f247335",
   "metadata": {},
   "source": [
    "## create model events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8bd6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c98611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# discharge: qb, qp\n",
    "# get design hydrographs by vertical averaging normalized AM peak hydrographs\n",
    "df_rps.loc[0,:] = 0\n",
    "df_rps = df_rps.sort_index()\n",
    "q_event_lst = []\n",
    "for dvar in ['qp', 'qb']:\n",
    "    # update RP0 based on wet season average\n",
    "    m = dsq[dvar].time.dt.month\n",
    "    df_rps.loc[0,dvar] = dsq[dvar].isel(time=np.logical_or(m>=11, m<=4)).mean('time').compute().item()\n",
    "    # create events\n",
    "    daq_hydrograph0 = get_peak_hydrographs(dsq[dvar], ds_peaks[dvar], wdw_size=21).compute()\n",
    "    daq_hydrograph = daq_hydrograph0.mean('peak')\n",
    "    # if dvar in df_timelags:\n",
    "    #     daq_hydrograph['time'] = daq_hydrograph['time'] + df_timelags[dvar]\n",
    "    daq_events = df_rps[dvar].to_xarray() * daq_hydrograph\n",
    "    daq_events['time'] = daq_events['time']*24\n",
    "    events_dict[dvar] = daq_events\n",
    "    q_event_lst.append(daq_events)\n",
    "dsq_events = xr.merge(q_event_lst).dropna('time')\n",
    "dsq_events['time'].attrs.update(units='hour')\n",
    "_ = dsq_events['qb'].sel(rps=100).plot.line(x='time')\n",
    "_ = dsq_events['qp'].sel(rps=100).plot.line(x='time')\n",
    "# dsq_events.to_netcdf(fnq.replace('.nc', '_events.nc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068cdbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "daq_hydrograph0.reset_coords(drop=True).rename('q').to_dataframe().unstack(0).plot(color='k', lw=0.5, alpha=0.5, legend=False)\n",
    "daq_hydrograph.to_series().plot(color='r', lw=2, legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a743b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coastal\n",
    "def sort_center(x, dim):\n",
    "    a = x.get_axis_num(dim)\n",
    "    n = x[dim].size\n",
    "    x_sorted = np.apply_along_axis(np.sort, a, x)\n",
    "    idx = [None if i!=a else range(n) for i in range(x.ndim)]\n",
    "    reorder = np.append(np.arange(0, n, 2), np.arange(1, n, 2)[::-1])\n",
    "    return xr.DataArray(np.take_along_axis(x_sorted, reorder[idx], a), x.coords)\n",
    "\n",
    "# get design hydrographs by horizontal averaging normalized AM peak hydrographs\n",
    "dasw = dsh0['sw']#.rolling(time=48, center=True).mean('time')\n",
    "dasw_peaks = get_peaks(dasw, \"BM\", min_dist=6*24*5, period='AS-AUG')\n",
    "dasw_hydrographs = get_peak_hydrographs(\n",
    "    dasw,\n",
    "    dasw_peaks, \n",
    "    wdw_size=int(6*24*5), \n",
    "    normalize=True,\n",
    "    # n_peaks=20\n",
    ")\n",
    "dasw_hydrographs = np.maximum(0,dasw_hydrographs.dropna('peak'))\n",
    "dasw_hydrographs['time'] = dasw_hydrographs['time']/6  # hr\n",
    "dasw_hydrograph = sort_center(dasw_hydrographs, dim='time').mean('peak') # horizontal averaging\n",
    "# dasw_hydrograph = dasw_hydrographs.mean('peak') # vertical averaging\n",
    "\n",
    "da_mhws_peaks = get_peaks(dsh0['t'], \"BM\", min_dist=6*24*10, period=\"29.5D\")\n",
    "da_mhws_hydrographs = get_peak_hydrographs(\n",
    "    dsh0['t'], da_mhws_peaks, \n",
    "    wdw_size=int(6*24*14.5), \n",
    "    normalize=False,\n",
    ")\n",
    "da_mhws_hydrographs['time'] = da_mhws_hydrographs['time']/6 # hr\n",
    "da_mhws_hydrograph = da_mhws_hydrographs.mean('peak')\n",
    "mhws = 3.8 # highest astronomical tide IHO tidal constituents\n",
    "df_rps.loc[0,'h_tsw'] = mhws\n",
    "da_mhws_hydrograph = da_mhws_hydrograph/da_mhws_hydrograph.max('time')*mhws\n",
    "dasw_hydrograph = dasw_hydrograph.reindex(time=da_mhws_hydrograph.time, fill_value=0)\n",
    "\n",
    "da_h_events = da_mhws_hydrograph + dasw_hydrograph * (df_rps['h_tsw'].to_xarray()-mhws)\n",
    "# da_h_events['time'] = da_h_events['time'] + df_timelags[['s','w']].min()*24\n",
    "da_h_events['time'].attrs.update(units='hour')\n",
    "_ = da_h_events.sel(rps=[0,2,500]).plot.line(x='time')\n",
    "# da_h_events.to_netcdf(fnh.replace('.nc', '_events.nc'))\n",
    "events_dict['h_tsw'] = da_h_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a6dc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "dasw_hydrographs.reset_coords(drop=True).rename('sw').to_dataframe().unstack(0).plot(color='k', lw=0.5, alpha=0.5, legend=False)\n",
    "dasw_hydrograph.reindex(time=dasw_hydrographs.time).to_series().plot(color='r', lw=2, legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5e77ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eva import eva_idf, get_hyetograph\n",
    "\n",
    "# precip\n",
    "# get design events from IDF curves and alternating block method\n",
    "durations=np.array([1, 2, 3, 6, 12, 24], dtype=int)\n",
    "da_p_bm = eva_idf(dap0, durations=durations, distribution='gumb', rps=df_rps.index.values)\n",
    "da_p_events = get_hyetograph(da_p_bm['return_values'], dt=1, length=durations[-1])\n",
    "# da_p_events['time'] = da_p_events['time'] + df_timelags['p']*24\n",
    "da_p_events['time'].attrs.update(units='hour')\n",
    "_ = da_p_events.sel(rps=[2,500]).plot.line(x='time')\n",
    "# da_p_events.to_netcdf(fnp.replace('.nc', '_events.nc'))\n",
    "events_dict['p'] = da_p_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194c04b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lagtimes relative to qb\n",
    "df_timelags['h_tsw'] = df_timelags['s']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1e04cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import ascii_uppercase as letters\n",
    "\n",
    "rps = [0,2,100]\n",
    "dsq_events['qb'].sel(rps=rps)\n",
    "n = 4\n",
    "fig, axes = plt.subplots(n, 1, figsize=(10, 3*n), sharex=True)\n",
    "\n",
    "for i, dvar in enumerate(labels):\n",
    "    dt0 = 0\n",
    "    if dvar in df_timelags:\n",
    "        dt0 = df_timelags[dvar]*24\n",
    "    df0 = events_dict[dvar].sel(rps=rps).transpose('rps', ...).to_series().unstack(0)\n",
    "    df0.columns = ['non-flood', '2-year', '100-year']\n",
    "    df0.columns.name = 'design event'\n",
    "    df0.index = (df0.index.values+dt0)/24\n",
    "    df0.plot(ax=axes[i], legend=i==0)\n",
    "    axes[i].set_ylabel(labels[dvar])\n",
    "# dsq_events['qp'].sel(rps=rps).to_series().unstack(0).plot(ax=axes[1], legend=False)\n",
    "# axes[1].set_ylabel(labels['qp'])\n",
    "# da_p_events.sel(rps=rps).to_series().unstack().plot(ax=axes[2], legend=False)\n",
    "# axes[2].set_ylabel(labels['p'])\n",
    "# da_h_events.sel(rps=rps).to_series().unstack().plot(ax=axes[3], legend=False)\n",
    "# axes[3].set_ylabel(labels['h_tsw'])\n",
    "    title = labels[dvar].split('\\n')[0]\n",
    "    axes[i].set_title(f'{letters[i]}) {title}')\n",
    "    axes[i].grid()\n",
    "\n",
    "axes[3].set_xlabel('time relative to Buzi discharge peak [day]')\n",
    "axes[3].set_xlim([-6, 4])\n",
    "plt.savefig(join(r'../../4_results', f'drivers_events.png'), dpi=300, bbox_axes='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6071dcc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hydromt-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "3808d5b5b54949c7a0a707a38b0a689040fa9c90ab139a050e41373880719ab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
