{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b097498",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "from os.path import join\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from stats_eva import get_peaks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f136bfa0",
   "metadata": {},
   "source": [
    "## read and align data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d7658e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data, rasample to daily values and align\n",
    "ddir = r'../01_forcing'\n",
    "# daq = xr.open_dataset(join(ddir, 'cama_discharge_beira_daily.nc'), chunks={'time':30})['discharge']\n",
    "daq = xr.open_dataset(join(ddir, 'glofas_discharge_beira.nc'), chunks={'time':30})['discharge']\n",
    "dsh_hr = xr.open_dataset(join(ddir, 'reanalysis_gtsm_v1_beira_extended.nc'), chunks={'time':6*24*100})#.drop_vars('waterlevel')\n",
    "# dsh_hr['surge'] = np.maximum(0, dsh_hr['surge'])\n",
    "dsh_hr['tide'] = dsh_hr['waterlevel'] - dsh_hr['surge']\n",
    "dsh_hr['sw'] = dsh_hr['surge'] + dsh_hr['shww']*0.2\n",
    "dsh_hr['waterlevel_tsw'] = dsh_hr['tide'] + dsh_hr['sw']\n",
    "dsh_hr = dsh_hr.rename({'surge': 's', 'shww': 'w', 'tide': 't'})\n",
    "dap_hr = xr.open_dataset(join(ddir, 'era5_precip_beira_hourly_spatialmean.nc'), chunks={'time':24*30})['precip']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59daa005",
   "metadata": {},
   "outputs": [],
   "source": [
    "dsh = dsh_hr.resample(time='1D', label='right').max('time')\n",
    "dap = dap_hr.resample(time='1D', label='right').sum('time')\n",
    "dates = pd.date_range('19800101', '20210201', freq='D')\n",
    "ds = xr.merge([\n",
    "    daq.sel(index=1).rename('qb'),\n",
    "    daq.sel(index=4).rename('qp'),\n",
    "    dsh[['s', 't', 'w', 'sw']],\n",
    "    dsh['waterlevel'].rename('h'),\n",
    "    dsh['waterlevel_tsw'].rename('h_tsw'),\n",
    "    dap.rename('p')\n",
    "], compat='override').reset_coords(drop=True).reindex(time=dates)\n",
    "ds.attrs = {}\n",
    "ds.to_netcdf(join(ddir, 'beira_drivers_daily2_glofas.nc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4bae96",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddir = r'../01_forcing'\n",
    "ds = xr.open_dataset(join(ddir, 'beira_drivers_daily1_glofas.nc'))\n",
    "# ds = ds.drop_vars(['h']).rename({'h_tsw': 'h'})\n",
    "# ds = ds.drop_vars(['w', 'sw', 'h_tsw'])\n",
    "# ds = ds.sel(time=slice('19800101', '20181231'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67842fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {\n",
    "    'qb': 'Discharge Buzi\\n[m3/s]',\n",
    "    'qp': 'Discharge Pungwe\\n[m3/s]',\n",
    "    'p': 'Rainfall\\n[mm]',\n",
    "    's': 'Surge\\n[m]',\n",
    "    'ss': 'Skew surge\\n[m]',\n",
    "    't': 'Tide\\n[m+MSL]',\n",
    "    'w': 'Sign. wave height\\n[m]',\n",
    "    'h': 'Total waterlevel\\n[m+MSL]',\n",
    "    'sw': 'Non-tidal residual\\n[m]'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72478be3",
   "metadata": {},
   "source": [
    "## get annual maxima peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a304f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings\n",
    "period='AS-AUG'\n",
    "\n",
    "ds_peaks = xr.Dataset(coords=ds.coords)\n",
    "for dvar in ds.data_vars.keys():\n",
    "    if dvar == 't': continue\n",
    "    ds_peaks[dvar] = get_peaks(ds[dvar], period=period, min_dist=14, min_sample_size=90)\n",
    "ds_peaks = ds_peaks.reset_coords(drop=True).dropna('time', how='all')\n",
    "\n",
    "# peaks with dates\n",
    "df_peaks0 = ds_peaks.to_dataframe()  \n",
    "\n",
    "# get maximum values within time window for non_extremes\n",
    "df_peaks0_filled = pd.DataFrame()\n",
    "ds_tmax = ds.rolling(time=7).max('time').sel(time=df_peaks0.index)\n",
    "# ds_tmax = ds.sel(time=df_peaks0.index)\n",
    "for dvar in df_peaks0.columns:\n",
    "    df_peaks0_filled[dvar] = df_peaks0[dvar].where(df_peaks0[dvar].notna(), ds_tmax[dvar])\n",
    "\n",
    "# peaks with regular spaced interval\n",
    "df_bm = ds_peaks.resample(time=period).max('time').to_dataframe().dropna()\n",
    "n = len(ds_peaks.data_vars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb83b997",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(n, 1, figsize=(12, 3*n), sharex=True)\n",
    "for i, dvar in enumerate(ds_peaks.data_vars.keys()):\n",
    "    ds[dvar].to_series().plot(ax=axes[i], color='k')\n",
    "    df_peaks0[dvar].plot(ax=axes[i], color='r', marker='.', lw=0)\n",
    "    axes[i].set_ylabel(labels[dvar])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88495437",
   "metadata": {},
   "source": [
    "## fit eva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695c7d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stats_eva import lmoment_fitopt, get_frozen_dist, plot_return_values\n",
    "dparams = ['shape', 'loc', 'scale']\n",
    "distributions = ['gev', 'gumb']#[1:]\n",
    "\n",
    "fig, axes = plt.subplots(n, 1, figsize=(8, 3*n), sharex=True)\n",
    "\n",
    "df_eva = pd.DataFrame(columns=['dist'] + dparams)\n",
    "for i, dvar in enumerate(ds_peaks.data_vars.keys()):\n",
    "    x = ds_peaks[dvar].dropna('time').values\n",
    "    params, dist = lmoment_fitopt(x, distributions=distributions, criterium='AIC')\n",
    "    df_eva.loc[dvar, dparams[-len(params):]] = params\n",
    "    df_eva.loc[dvar, 'dist'] = dist\n",
    "    _ = plot_return_values(x, params, dist, ax=axes[i])\n",
    "    axes[i].set_title(dvar)\n",
    "    axes[i].set_ylim([x.min()*0.9, axes[i].get_ylim()[1]])\n",
    "    axes[i].set_ylabel(labels[dvar])\n",
    "    if i < n-1:\n",
    "        axes[i].set_xlabel('')\n",
    "df_eva"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f0311f",
   "metadata": {},
   "source": [
    "## co-occurence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1310e86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE that `max_time_lag` is the timelag between two consecutive extremes\n",
    "# the total time lag between all four events could theoretically be n times as long\n",
    "## settings\n",
    "drivers = ['qb', 'qp', 'p', 's']\n",
    "max_time_lag = {\n",
    "    'qp': pd.Timedelta('5D'),\n",
    "    'qb': pd.Timedelta('5D'),\n",
    "    'p': pd.Timedelta('1D'),\n",
    "    's': pd.Timedelta('1D'),\n",
    "    'w': pd.Timedelta('1D'),\n",
    "    'h': pd.Timedelta('1D'),\n",
    "    'sw': pd.Timedelta('1D'),\n",
    "}\n",
    "\n",
    "# reduce daily events to selected drivers\n",
    "df_peaks = df_peaks0[drivers].dropna(how='all')\n",
    "\n",
    "# combine daily events to event sets based on maximum time_lag\n",
    "df_peaks['dt'] = np.hstack([[0], np.diff(df_peaks.index)])\n",
    "df_peaks['max_dt'] = (df_peaks[drivers].notna()*np.array([max_time_lag[d] for d in drivers])).max(axis=1)\n",
    "# df_peaks['max_dt'] = [pd.Timedelta(days=d) for d in df_peaks['max_dt'].dt.days.rolling(2, min_periods=1).max().values]\n",
    "df_peaks['event'] = np.cumsum(~(df_peaks['dt']<df_peaks['max_dt']))\n",
    "df_events = df_peaks.groupby('event').apply(lambda x: x.notna().sum()).drop(columns=['dt', 'event'])\n",
    "df_events['n_extreme'] = df_events[drivers].sum(axis=1)\n",
    "df_events['time'] = df_peaks.reset_index().groupby('event').first()['time']\n",
    "offset = pd.Timedelta(days=df_bm.index.dayofyear.min()-1)\n",
    "df_events['year'] = (df_events['time'] - offset).dt.year\n",
    "df_events['time_lag'] = [evnt.iloc[1:,:]['dt'].sum() for _, evnt in df_peaks.groupby('event')]\n",
    "evnts, cnts = np.unique(df_peaks['event'], return_counts=True)\n",
    "df_peaks['co-occur'] = np.isin(df_peaks['event'], evnts[cnts>1])\n",
    "\n",
    "# make barcode plot for co-occuring events\n",
    "data = df_events[drivers].T * df_events['n_extreme']\n",
    "_, ax = plt.subplots(1,1, figsize=(12,5))\n",
    "ax.imshow(data, interpolation='none', cmap='Blues', vmin=0, vmax=len(drivers))\n",
    "ax.set_aspect(2)\n",
    "ax.set_yticks(np.arange(len(drivers)))\n",
    "ax.set_yticklabels([labels[dvar].split('\\n')[0] for dvar in drivers])\n",
    "\n",
    "xlabs = df_events.reset_index().groupby('year').first()[['event']]\n",
    "_ = ax.set_xticks(xlabs['event'].values[::2]-1.5)\n",
    "_ = ax.set_xticklabels(xlabs.index.values[::2], rotation=45)\n",
    "\n",
    "# remove incomplete years\n",
    "check = df_events[['n_extreme', 'year']].groupby('year').sum()\n",
    "drop_years = check[check['n_extreme']!=len(drivers)].index.values\n",
    "if drop_years.size > 1:\n",
    "    df_events = df_events[~np.isin(df_events['year'], drop_years)]\n",
    "    print(f'ignore years with more/less events than drivers: {drop_years}')\n",
    "print(f'no. of valid years: {np.unique(df_events.year).size}')\n",
    "print(f'no. of events: {df_events.index.size}; (>1 extreme: {df_events[df_events.n_extreme>1].index.size})')\n",
    "print(f'max time lag: {df_events.time_lag.max().days} days')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc4b733",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_events_dist = df_events.groupby(drivers + ['n_extreme']).size().reset_index(name='count')\n",
    "df_events_dist = df_events_dist.sort_values('count', ascending=False)\n",
    "\n",
    "\n",
    "fig, (ax1, ax) = plt.subplots(2,1, figsize=(7,3), sharex=True, gridspec_kw={'height_ratios': [2, 1], 'hspace':0.0})\n",
    "df_events_dist.reset_index()['count'].plot.bar(ax=ax1, color='k')\n",
    "ax1.set_ylabel('count [-]')\n",
    "\n",
    "x, y = np.where(df_events_dist[drivers])\n",
    "ax.scatter(x, y, color='k')\n",
    "ax.set_ylim([-0.5, len(drivers)-0.5])\n",
    "ax.set_yticks(np.arange(len(drivers)))\n",
    "ax.set_yticklabels([labels[dvar].split('\\n')[0] for dvar in drivers])\n",
    "ax.set_xlabel('event type')\n",
    "ax.set_xticklabels('')\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2a9a36",
   "metadata": {},
   "source": [
    "## multivariate dependence modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f23306",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyvinecopulib as pv\n",
    "\n",
    "# Transform copula data using the empirical distribution\n",
    "df_uobs = pd.DataFrame(columns=drivers, data = pv.to_pseudo_obs(df_bm[drivers].values))\n",
    "\n",
    "# fit copula\n",
    "controls = pv.FitControlsVinecop(\n",
    "    family_set=[pv.BicopFamily(x) for x in np.arange(0,11)],\n",
    "    # parametric_method='itau',\n",
    "    # nonparametric_method='quadratic'\n",
    ")\n",
    "cop = pv.Vinecop(data=df_uobs.values, controls=controls, structure=pv.RVineStructure(len(drivers)))\n",
    "# cop.structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04473ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kendalltau, pearsonr\n",
    "\n",
    "data = []\n",
    "m = cop.matrix\n",
    "n = m.shape[0]\n",
    "v = drivers\n",
    "for t in range(n-1):\n",
    "    # print(f'** Tree: {t:d}')\n",
    "    for e in range(n-1-t):\n",
    "        p1, p2 = v[int(m[n-1-e,e]-1)], v[int(m[t,e]-1)]\n",
    "        px = [v[int(p-1)] for p in m[:t,e]]\n",
    "        c = cop.get_pair_copula(t,e)\n",
    "        tau0, ptau = kendalltau(df_uobs[p1], df_uobs[p2])\n",
    "        tau = cop.get_tau(t,e)  # NOTE: diffferent from scipy.stats method ?\n",
    "        pxs = f' | ' + ','.join(px) if px else ''\n",
    "        cstr = c.str().replace(f'\\n',',')\n",
    "        print(f'{p1},{p2}{pxs}: {cstr}; tau = {tau:.5f} ({tau0:.3f}, {ptau:.3f})')\n",
    "        data.append([t, e, [p1,p2], px, c.str().split(',')[0], c.parameters.flatten(), tau])\n",
    "\n",
    "df_cop = pd.DataFrame(\n",
    "    data=data,\n",
    "    columns=['tree', 'edge', 'pair', 'conditional', 'copula', 'parameters', 'tau']\n",
    ")\n",
    "df_cop.set_index(['tree', 'edge'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0ebe3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import percentileofscore\n",
    "seeds = np.arange(len(drivers), dtype=int)\n",
    "\n",
    "n = len(drivers)-1\n",
    "fig, axes = plt.subplots(n,n, sharex=False, sharey=False, figsize=(8,8), gridspec_kw={'hspace':0.0, 'wspace':0.0})\n",
    "\n",
    "# df_uobs_cooc = df_uobs.where(df_peaks[drivers][df_peaks['co-occur']].resample(period).max().reset_index(drop=True).notna())\n",
    "df_obs_cooc = df_peaks[drivers][df_peaks['co-occur']].resample(period).max()\n",
    "\n",
    "df_usim = pd.DataFrame(data=cop.simulate(10000, seeds=seeds), columns=drivers)\n",
    "df_sim = pd.DataFrame(columns=drivers)\n",
    "for dvar in drivers:\n",
    "    df_sim[dvar] = np.quantile(df_bm[dvar], df_usim[dvar])\n",
    "\n",
    "\n",
    "for r in range(n):\n",
    "    for c in range(n):\n",
    "        ax = axes[r,c]\n",
    "        if c > r:\n",
    "            ax.set_visible(False)\n",
    "            continue\n",
    "        xlab, ylab = drivers[c], drivers[r+1]\n",
    "        x, y = df_bm[xlab].values, df_bm[ylab].values\n",
    "        xmin, xmax = np.quantile(x, [0,0.95])\n",
    "        ymin, ymax = np.quantile(y, [0,0.95])\n",
    "\n",
    "        c0 = df_cop[\n",
    "            [\n",
    "                ([xlab, ylab] == p0 or [ylab, xlab] == p0) \n",
    "                for p0 in df_cop['pair'].values\n",
    "            ]\n",
    "        ].squeeze()\n",
    "        \n",
    "\n",
    "        if len(c0) > 0 and c0.copula != 'Independence':\n",
    "            xx, yy = np.mgrid[xmin:xmax:100j, ymin:ymax:100j]\n",
    "            positions = np.vstack([xx.ravel(), yy.ravel()])\n",
    "            # TODO: use copula pdf in uspace instead of\n",
    "            # bicop = cop.get_pair_copula(c0.tree,c0.edge)\n",
    "            # u = np.stack([\n",
    "            #     np.vectorize(lambda x: percentileofscore(df_bm[xlab], x))(xx.ravel()),\n",
    "            #     np.vectorize(lambda x: percentileofscore(df_bm[ylab], x))(yy.ravel()),\n",
    "            # ]).T/100.\n",
    "            # f = bicop.pdf(u).reshape(xx.shape)\n",
    "            kernel = gaussian_kde(df_bm[[xlab,ylab]].values.T, bw_method=0.5)\n",
    "            f = np.reshape(kernel(positions).T, xx.shape)\n",
    "            cmap = 'Blues' if  c0.copula != 'Independence' else 'Greens'\n",
    "            ax.contourf(xx, yy, f, cmap=cmap, alpha=0.5)\n",
    "\n",
    "        # plot events\n",
    "        x, y = df_bm[xlab].values, df_bm[ylab].values\n",
    "        xcooc, ycooc = df_obs_cooc[xlab].values, df_obs_cooc[ylab].values\n",
    "        ax.scatter(xcooc, ycooc, s=13, color='darkorange', label='compound', zorder=2)\n",
    "        ax.scatter(x, y, s=12, color='black', label='other', alpha=0.9, zorder=1)\n",
    "        if r == n-1 and c == n-2:\n",
    "            ax.legend(title='AM events', loc='lower left', bbox_to_anchor=(1.1, 1.1))\n",
    "        kwargs = dict(        \n",
    "            horizontalalignment='center',\n",
    "            verticalalignment='top',\n",
    "            transform=ax.transAxes,\n",
    "            bbox=dict(facecolor='white', alpha=0.5, lw=0.1)\n",
    "        )\n",
    "        tau, ptau = kendalltau(x, y)\n",
    "        tsig = ('**' if ptau < 0.05 else '*') if ptau < 0.10 else ''\n",
    "        txt = f'{c0.copula} ' + r'($\\tau$='+f'{tau:.2f}{tsig})'\n",
    "        ax.text(0.5, 0.95, txt, **kwargs)\n",
    "\n",
    "        if c == 0:\n",
    "            ax.set_ylabel(labels[ylab])\n",
    "        else:\n",
    "            ax.set_yticklabels('')\n",
    "        if r == n-1:\n",
    "            ax.set_xlabel(labels[xlab])\n",
    "        ax.set_ylim([ymin, ymax])\n",
    "        ax.set_xlim([xmin, xmax])\n",
    "    \n",
    "# plt.savefig(join(r'../FIGURES', 'copula_autofit.png'), dpi=300, bbox_axes='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7bb7b1",
   "metadata": {},
   "source": [
    "## Create stochastic event set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b433958",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import interp1d\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "class emperical_dist(object):\n",
    "    def __init__(self, data, nyears):\n",
    "        self.data = np.sort(data)[::-1]  # descending\n",
    "        nevents = data.size\n",
    "        self.freq = np.arange(1,nevents+1)/(nevents+1)*(nevents/nyears)\n",
    "        self.data = self.data[self.freq<1]\n",
    "        self.freq = self.freq[self.freq<1]\n",
    "\n",
    "    def ppf(self, q):\n",
    "        return interp1d(\n",
    "            1-self.freq, \n",
    "            self.data, \n",
    "            bounds_error = False, \n",
    "            fill_value=(self.data[-1], self.data[0])\n",
    "        )(q)\n",
    "        \n",
    "    def pdf(self, x, **kwargs):\n",
    "        return gaussian_kde(self.data, **kwargs)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1594c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use marginal distributions to transform quantiles back to normal space\n",
    "extr_dist = {}\n",
    "for dvar in df_eva.index:\n",
    "    params = df_eva.loc[dvar, dparams].dropna()\n",
    "    dist = df_eva.loc[dvar, 'dist']\n",
    "    extr_dist[dvar] = get_frozen_dist(params, dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a41904",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_coastrp = pd.read_csv(join(ddir, 'annual_T_convolution_coastrp_local.csv'), index_col=['T'])\n",
    "df_coastrp['annual_exc_prob'] = 1/df_coastrp.index\n",
    "df_coastrp['cdf'] = 1-df_coastrp['annual_exc_prob']\n",
    "coastrp_cdf = interp1d(df_coastrp['cdf'], df_coastrp['local_coastrp'], bounds_error = False, fill_value = df_coastrp['local_coastrp'].max()+0.01) #We interpolate the empirical CDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85057a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Surge from COAST-RP\n",
    "fileS = 'Beira_STORM_surges.nc' #We look at the 3000 yr of data\n",
    "da_surge = xr.open_dataarray(join(ddir, fileS))\n",
    "extr_dist['s'] = emperical_dist(da_surge.values.flatten(), nyears=3000)\n",
    "\n",
    "ds_coastrp = xr.open_dataset(join(ddir, 'COAST-RP.nc'))\n",
    "ds_coastrp = ds_coastrp.where(ds_coastrp['station_id']=='id_coast_glob_18328', drop=True).set_coords('station_id').squeeze()\n",
    "da =xr.concat([ds_coastrp[dvar] for dvar in ds_coastrp.data_vars], dim='rps').rename('storm_tide')\n",
    "da['rps'] = xr.IndexVariable('rps', [int(dvar.split('_')[-1]) for dvar in ds_coastrp.data_vars])\n",
    "coastrp = da.to_series()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b8bb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tide_hr = dsh_hr['t'].resample(time='1H', label='left').first()\n",
    "tide_hr_wdw = tide_hr.rolling(time=72).construct('window').isel(time=slice(72,-72))\n",
    "tide_hr_wdw = tide_hr_wdw.where(~np.isnan(tide_hr_wdw).any('window'), drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba07b4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "surge_hr = dsh_hr['s'].resample(time='1H', label='left').first()\n",
    "surge_hr_wdw = get_peak_hydrographs(surge_hr, get_peaks(surge_hr, period='3D'), 72).load().rename({'time': 'window'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964b4147",
   "metadata": {},
   "outputs": [],
   "source": [
    "dsurge = xr.DataArray(dims=['window'], data=np.hstack([np.linspace(0,1,25), np.linspace(1-1/46,0,47)]))\n",
    "(tide_hr_wdw.isel(time=4)+dsurge*2).max('window')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05598515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# high_tide = dsh_hr['t'].load().resample(time='29.5D').max()\n",
    "# df_tide=high_tide.dropna('time').to_series()\n",
    "# df_tide = dsh_hr['t'].load().dropna('time').to_series()\n",
    "\n",
    "n=100\n",
    "surge = xr.DataArray(dims=['time'], data=np.repeat(df_sim[['s']].values,n))\n",
    "idx = np.random.randint(0, tide_hr_wdw.time.size, surge.size)\n",
    "tide = tide_hr_wdw.isel(time=idx).drop('time')\n",
    "h = (tide+dsurge*surge).max('window').values\n",
    "\n",
    "fig, ax = plt.subplots(1,1)\n",
    "# coast_rp\n",
    "x = df_bm['h'].values\n",
    "rps = _get_return_periods(x)\n",
    "ax.scatter(rps, x, color='purple', label='reanalysis')\n",
    "\n",
    "# x = df_coastrp['local_coastrp'].values\n",
    "# rps = df_coastrp.index.values\n",
    "# ax.scatter(rps, x, color='m', label='local_coastrp')\n",
    "\n",
    "# ax.plot(coastrp.index, coastrp.values, '--k', label='coastrp')\n",
    "\n",
    "rps = _get_return_periods(h)\n",
    "ax.plot(rps, h, '.g', label='simulated')\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlabel('return period [years]')\n",
    "ax.set_ylabel('surge [m]')\n",
    "ax.set_ylim([3.5, 7])\n",
    "ax.legend()\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7762ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from the copula\n",
    "random_state=1234\n",
    "seeds = np.arange(len(drivers), dtype=int)\n",
    "n_sim = 10000\n",
    "df_usim = pd.DataFrame(data=cop.simulate(n_sim, seeds=seeds), columns=drivers)\n",
    "\n",
    "# transform extremes based on dvar marginal\n",
    "df_sim = pd.DataFrame()\n",
    "for dvar in drivers:\n",
    "    df_sim[dvar] = extr_dist[dvar].ppf(df_usim[dvar].values).round(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317bbc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine co-occurence and simulations\n",
    "from itertools import cycle\n",
    "event_cycle = cycle(df_events.groupby('year'))\n",
    "sim_events = []\n",
    "for i, row in df_sim.iterrows():\n",
    "    event_yr = next(event_cycle)[1][drivers] \n",
    "    events = (event_yr * row)\n",
    "    events['year'] = int(i+1)\n",
    "    sim_events.append(events)\n",
    "\n",
    "df_sim_events = pd.concat(sim_events, axis=0, ignore_index=True)\n",
    "df_sim_events_am = df_sim_events>0\n",
    "df_sim_events_am['year'] = df_sim_events['year']\n",
    "df_sim_events.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06e435a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # to sample non-extreme values use time lag window around events where\n",
    "# # another driver is extreme, but dvar is not\n",
    "xval_norm = {}\n",
    "for dvar in drivers:\n",
    "    # dates = df_peaks.index[df_peaks[dvar].isna()]\n",
    "    # if dvar not in ['s', 'w']:\n",
    "    #     xval = ds[dvar].rolling(time=max_time_lag[dvar].days).construct('window').sel(time=dates)\n",
    "    # else:\n",
    "    #     xval = dsh_hr[dvar].load().rolling(time=6*24*max_time_lag[dvar].days).construct('window').sel(time=dates)\n",
    "    # xval = xval.stack({'time1':('time', 'window')}).to_series().dropna()\n",
    "    # xval_norm[dvar] = xval\n",
    "    xval_norm[dvar] = ds[dvar].where(ds[dvar]<ds[dvar].quantile(0.8),drop=True).to_series().dropna()\n",
    "\n",
    "# # sample random value around extremes with non-extreme dvar\n",
    "s = df_sim_events.index.size\n",
    "for dvar in drivers:\n",
    "    x = xval_norm[dvar].sample(s, replace=True, random_state=random_state).values\n",
    "    df_sim_events[dvar] = df_sim_events[dvar].where(df_sim_events_am[dvar]>0, x)\n",
    "\n",
    "df_sim_events.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c862e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stats_eva import get_peak_hydrographs\n",
    "hydrographs = {}\n",
    "for dvar in ['s']:\n",
    "    da = dsh_hr[dvar].load()\n",
    "    hydrographs[dvar] = get_peak_hydrographs(da, get_peaks(da, period=period), 6*24*2).mean('peak').to_series()\n",
    "df_hydro = pd.DataFrame(hydrographs)\n",
    "df_hydro.index = df_hydro.index/6\n",
    "df_hydro.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53fcdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get tide timeseries\n",
    "high_tide = dsh_hr['t'].load().resample(time='12H').max()\n",
    "df_tide1=high_tide.dropna('time').to_series()\n",
    "# df_tide = ds['t'].dropna('time').to_series()\n",
    "\n",
    "# high_tide = dsh_hr['t'].resample(time='14D').max()\n",
    "# df_tide = high_tide.dropna('time').to_series()\n",
    "\n",
    "# high_tide = ds['t']\n",
    "# df_tide_am = high_tide.where(high_tide>high_tide.quantile(0.98)).dropna('time').to_series()\n",
    "df_tide_am = high_tide.resample(time='AS').max('time').dropna('time').to_series()\n",
    "# df_tide_am = da_tide_am.where(~np.isin(da_tide_am.time.dt.year,drop_years)).dropna('time').to_series()\n",
    "\n",
    "\n",
    "fn = join('../02_data', 'reanalysis_gtsm_v1_beira_extended.nc', 'tide_peaks.csv')\n",
    "df_tide = pd.read_csv(fn, index_col = 'index', parse_dates = True)['high_tide']\n",
    "df_tide_am = df_tide.resample('AS').max().iloc[:-2]\n",
    "df_tide, df_tide1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b550129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one dependent water component\n",
    "hname = 's'\n",
    "\n",
    "delta_s = 1.0#hydrographs['s'].sample(n_sim, replace=True, random_state=random_state).values\n",
    "tide = df_tide.sample(n_sim, replace=True, random_state=random_state+1).values\n",
    "h = tide + df_sim[hname].values*delta_s\n",
    "df_sim['h0'] = h\n",
    "\n",
    "# at least AM tide for events with extreme\n",
    "tide_am = df_tide_am.sample(n_sim, replace=True, random_state=random_state).values\n",
    "h = np.maximum(tide_am, h)\n",
    "\n",
    "# df_sim_events['h'] = 0\n",
    "# df_sim_events.loc[df_sim_events_am[hname]>0, 'h'] = h\n",
    "df_sim['h'] = h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdc5021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# more than one dependent water component\n",
    "# combine surge, waves with random tide\n",
    "\n",
    "s = df_sim_events.index.size\n",
    "tide = df_tide.sample(s, replace=True, random_state=random_state).values\n",
    "delta_s = hydrographs['s'].sample(s, replace=True, random_state=random_state+10).values\n",
    "delta_w = hydrographs['w'].sample(s, replace=True, random_state=random_state).values\n",
    "surge = np.maximum(0, df_sim_events['s'].values*delta_s)\n",
    "wave_setup = (df_sim_events['w']*0.2).values\n",
    "wave_setup = np.where(df_sim_events_am['w'].values>0, wave_setup*delta_w, wave_setup)\n",
    "# df_sim_events['h'] = 0\n",
    "df_sim_events['h'] = tide + surge + wave_setup \n",
    "print(surge.max(), wave_setup.max(), (surge+wave_setup).max())\n",
    "\n",
    "# at least AM tide for events with extreme\n",
    "imax = df_sim_events[['h', 'year']].groupby('year').idxmax().values.flatten()\n",
    "tide_am = df_tide_am.sample(n_sim, replace=True, random_state=random_state).values\n",
    "df_sim_events.loc[imax, 'h'] = np.maximum(tide_am, df_sim_events.loc[imax, 'h'])\n",
    "\n",
    "df_sim['h'] = df_sim_events.loc[imax, 'h'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9dd94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "da_surge.values.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a8c281",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# qs = np.linspace(0,1,10001)\n",
    "# x0 = np.quantile(extr_dist['s'].data, qs)\n",
    "# x1 = np.quantile(df_sim['s'], qs)\n",
    "\n",
    "# fig, ax = plt.subplots(1,1)\n",
    "# ax.scatter(x0,x1)\n",
    "# ax.plot([0,4],[0,4], '--k')\n",
    "# ax.set_xlabel('simulated surge [m]')\n",
    "# ax.set_ylabel('coastrp surge [m]')\n",
    "# ax.set_title('QQ-plot surge')\n",
    "\n",
    "# plt.savefig(join(r'../FIGURES', 'surge_qq.png'), dpi=300, bbox_axes='tight')\n",
    "\n",
    "fig, ax = plt.subplots(1,1)\n",
    "x0 = np.sort(da_surge.values)\n",
    "rps0 = _get_return_periods(x0, extremes_rate=x0.size/3000)\n",
    "\n",
    "x1 = np.sort(df_sim['s'].values)\n",
    "rps1 = _get_return_periods(x1)\n",
    "ax.plot(rps0, x0, '.k', label='coastrp')\n",
    "ax.plot(rps0, x0, '.k', label='coastrp')\n",
    "ax.plot(rps1, x1, '.g', label='simulated')\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlabel('return period [years]')\n",
    "ax.set_ylabel('surge [m]')\n",
    "ax.legend()\n",
    "ax.grid()\n",
    "\n",
    "plt.savefig(join(r'../FIGURES', 'surge_rps.png'), dpi=300, bbox_axes='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827dbbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dvar = 'h'\n",
    "# from stats_eva import _get_return_periods\n",
    "\n",
    "fig, ax = plt.subplots(1,1)\n",
    "\n",
    "# coast_rp\n",
    "x = df_coastrp['coast_rp'].values\n",
    "rps = df_coastrp.index.values\n",
    "rps_gumb = -np.log(-np.log(1.0 - 1.0 / rps))\n",
    "ax.scatter(rps_gumb, x, color='purple', label='coastrp')\n",
    "\n",
    "x = df_coastrp['local_coastrp'].values\n",
    "rps = df_coastrp.index.values\n",
    "rps_gumb = -np.log(-np.log(1.0 - 1.0 / rps))\n",
    "ax.scatter(rps_gumb, x, color='m', label='local_coastrp')\n",
    "\n",
    "# simulated\n",
    "x_sim = df_sim['h0'].values\n",
    "rps_sim = _get_return_periods(x_sim)\n",
    "rps_sim_gumb = -np.log(-np.log(1.0 - 1.0 / rps_sim))\n",
    "ax.scatter(rps_sim_gumb, x_sim, color='g', label='simulated (uncorrected)')\n",
    "\n",
    "x_sim = df_sim['h'].values\n",
    "rps_sim = _get_return_periods(x_sim)\n",
    "rps_sim_gumb = -np.log(-np.log(1.0 - 1.0 / rps_sim))\n",
    "ax.scatter(rps_sim_gumb, x_sim, color='k', label='simulated (corrected)')\n",
    "\n",
    "# plot_return_values(df_bm[dvar].values, ax=ax, color='g')\n",
    "ax.legend()\n",
    "# ax.set_xlim([1,500])\n",
    "ax.set_ylim([3,7])\n",
    "\n",
    "_rps = np.array([1.1,2,5,10,20,50,100,500])\n",
    "_rps_gumb = -np.log(-np.log(1.0 - 1.0 / _rps))\n",
    "ax.set_xticks(_rps_gumb)\n",
    "ax.set_xticklabels([f'{rp:.1f}' for rp in _rps])\n",
    "ax.set_xlim([rps_gumb.min()*1.2, _rps_gumb[-1]*1.2])\n",
    "ax.set_xlabel(\"Return period [years]\")\n",
    "ax.set_ylabel(\"Waterlevel [m+MSL]\")\n",
    "ax.grid()\n",
    "\n",
    "plt.savefig(join(r'../FIGURES', 'waterlevel_events1.png'), dpi=300, bbox_axes='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ea3ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dvars = ['qb', 'qp', 'p', 'h']\n",
    "# df_obs_events = df_peaks0_filled[dvars].dropna(how='any')\n",
    "# print(len(df_obs_events))\n",
    "# rm = {k:v for k,v in labels.items() if k in dvars}\n",
    "# df_merged = pd.concat([\n",
    "#     df_sim_events[dvars],\n",
    "#     df_obs_events, \n",
    "#     ], ignore_index=True, axis=0).rename(columns=rm)\n",
    "# colors = np.hstack([\n",
    "#     np.full(df_sim_events.index.size, \"k\"),\n",
    "#     np.full(df_obs_events.index.size, \"r\"), \n",
    "#     ])\n",
    "# sizes = np.hstack([\n",
    "#     np.full(df_sim_events.index.size, 5),\n",
    "#     np.full(df_obs_events.index.size, 30), \n",
    "#     ])\n",
    "# axes = pd.plotting.scatter_matrix(df_merged, color=colors, s=sizes, alpha=1.0, diagonal='kde', figsize=(10,10))\n",
    "# for i, dvar in enumerate(dvars):\n",
    "#     axes[i,i].lines.pop(0)\n",
    "#     xmin, xmax = df_merged[rm[dvar]].quantile([0.0, 0.998]).values\n",
    "#     rp1 = extr_dist[dvar].ppf(1-1/1.1)\n",
    "#     for j in range(len(dvars)):\n",
    "#         axes[j,i].set_xlim([xmin, xmax])\n",
    "#         axes[j,i].axvline(rp1, color='c', ls='-', label='rp1')#, lw=0.5)\n",
    "#         if j != i: \n",
    "#             axes[i,j].set_ylim([xmin, xmax])\n",
    "#             axes[i,j].axhline(rp1, color='c', ls='--')#, lw=0.5)\n",
    "#     # fix ticks ..\n",
    "#     xticks = axes[-1,i].get_xticks()\n",
    "#     xticks = xticks[np.logical_and(xticks>xmin, xticks<xmax)]\n",
    "#     axes[-1,i].set_xticks(xticks)\n",
    "#     axes[-1,i].set_xticklabels(xticks, rotation=0)\n",
    "#     if i == 0:\n",
    "#         ymin,ymax = axes[0,0].get_ylim()\n",
    "#         axes[i,i].set_yticks(xticks*(ymax-ymin)/(xmax-xmin))\n",
    "#         axes[i,i].set_yticklabels(xticks)\n",
    "#     df_sim_events[dvar].plot.kde(ax=axes[i,i], color='k', label='sim', legend=i==0)\n",
    "#     df_obs_events[dvar].plot.kde(ax=axes[i,i], color='r', label='obs', legend=i==0)\n",
    "#     if i == 0:\n",
    "#         axes[i,i].set_ylabel(rm[dvar])\n",
    "#     else:\n",
    "#         axes[i,i].yaxis.set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b125a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dvars = ['qb', 'qp', 'p', 'h']\n",
    "rm = {k:v for k,v in labels.items() if k in dvars}\n",
    "\n",
    "df_sim = df_sim#events.groupby('year').max()\n",
    "df_merged = pd.concat([\n",
    "    df_sim[dvars],\n",
    "    df_bm[dvars], \n",
    "    ], ignore_index=True, axis=0).rename(columns=rm)\n",
    "colors = np.hstack([\n",
    "    np.full(df_sim.index.size, \"k\"),\n",
    "    np.full(df_bm.index.size, \"r\"), \n",
    "    ])\n",
    "sizes = np.hstack([\n",
    "    np.full(df_sim.index.size, 5),\n",
    "    np.full(df_bm.index.size, 30), \n",
    "    ])\n",
    "axes = pd.plotting.scatter_matrix(df_merged, color=colors, s=sizes, alpha=1.0, diagonal='kde', figsize=(10,10))\n",
    "for i, dvar in enumerate(dvars):\n",
    "    axes[i,i].lines.pop(0)\n",
    "    xmin, xmax = df_merged[rm[dvar]].quantile([0.0, 0.995]).values\n",
    "    for j in range(len(dvars)):\n",
    "        axes[j,i].set_xlim([xmin, xmax])\n",
    "        if j != i: \n",
    "            axes[i,j].set_ylim([xmin, xmax])\n",
    "    # fix ticks ..\n",
    "    xticks = axes[-1,i].get_xticks()\n",
    "    xticks = xticks[np.logical_and(xticks>xmin, xticks<xmax)]\n",
    "    axes[-1,i].set_xticks(xticks)\n",
    "    axes[-1,i].set_xticklabels(xticks, rotation=0)\n",
    "    if i == 0:\n",
    "        ymin,ymax = axes[0,0].get_ylim()\n",
    "        axes[i,i].set_yticks(xticks*(ymax-ymin)/(xmax-xmin))\n",
    "        axes[i,i].set_yticklabels(xticks)\n",
    "    df_sim[dvar].plot.kde(ax=axes[i,i], color='k', label='sim', legend=i==0)\n",
    "    df_bm[dvar].plot.kde(ax=axes[i,i], color='r', label='obs', legend=i==0)\n",
    "    if i == 0:\n",
    "        axes[i,i].set_ylabel(rm[dvar])\n",
    "    else:\n",
    "        axes[i,i].yaxis.set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ef4ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "idxmax = df_sim_events.groupby('year').idxmax()\n",
    "for dvar in df_sim_events.columns:\n",
    "    if dvar not in extr_dist: continue\n",
    "    df_sim_events[f'{dvar}_rp'] = 1.0\n",
    "    imax = idxmax[dvar].values\n",
    "    df_sim_events.loc[imax, f'{dvar}_rp'] = np.minimum(500, 1/extr_dist[dvar].sf(df_sim_events.loc[imax, dvar]))\n",
    "# df_sim_events[df_sim_events['year'] == 15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f60eae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all scenarios used for linear interpolating damages\n",
    "rps = np.array([1,2,5,10,50,100,500], dtype=int)\n",
    "cols = ['qb_rp', 'qp_rp', 'p_rp', 'h_rp']\n",
    "scens = []\n",
    "\n",
    "values = df_sim_events.loc[:,cols].values\n",
    "for event in values:\n",
    "    rps_lst = []\n",
    "    for rp in event:\n",
    "        if rp in rps:\n",
    "            rps_lst.append([int(rp), int(rp)])\n",
    "        else:\n",
    "            rps_lst.append([rps[rps<rp][-1], rps[rps>rp][0]])\n",
    "    for rps0 in zip(*rps_lst):\n",
    "        scens.append(list(rps0))\n",
    "# # include univariate\n",
    "# rps0 = np.zeros(4, dtype=int)\n",
    "# for i in range(4):\n",
    "#     for rp in rps[1:]:\n",
    "#         _rps = rps0.tolist()\n",
    "#         _rps[i] = rp\n",
    "#         scens.append(_rps)\n",
    "# # include full dependence\n",
    "# for rp in rps:\n",
    "#     scens.append(np.full(4, rp, dtype=int).tolist())\n",
    "##\n",
    "df_scen = pd.DataFrame(data=np.vstack(scens), columns=cols).value_counts().rename('count').reset_index()\n",
    "df_scen['scen'] = [\n",
    "    f\"qb{qb_rp:03d}_qp{qp_rp:03d}_h{h_rp:03d}_p{p_rp:03d}\" \n",
    "    for i, (qb_rp, qp_rp, p_rp, h_rp) in df_scen[cols].iterrows()\n",
    "]\n",
    "df_scen\n",
    "print(df_scen.index.size)\n",
    "# df_scen.to_csv(fn0.replace('.csv', '_scenCount.csv'))\n",
    "df_scen.sort_values('count', ascending=False).head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22360231",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
