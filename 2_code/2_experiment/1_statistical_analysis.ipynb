{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b097498",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "from os.path import join\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f136bfa0",
   "metadata": {},
   "source": [
    "## read and align data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88119fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data directory\n",
    "ddir = r'../../1_data/2_forcing'\n",
    "\n",
    "# data labels\n",
    "labels = {\n",
    "    'qb': 'Discharge Buzi\\n[m3/s]',\n",
    "    'qp': 'Discharge Pungwe\\n[m3/s]',\n",
    "    'p': 'Rainfall\\n[mm/hr]',\n",
    "    't': 'Tide\\n[m+MSL]',\n",
    "    's': 'Surge\\n[m]',\n",
    "    'w': 'Sign. wave height\\n[m]',\n",
    "    'h_ts': 'Total waterlevel\\n[m+MSL]',\n",
    "    'h_tsw': 'Total waterlevel (incl. wave setup)\\n[m+MSL]',\n",
    "    'ss': 'Skew surge\\n[m]',\n",
    "    'ssw': 'Skew surge (incl. wave setup)\\n[m]',\n",
    "    'sw': 'Non-tidal residual\\n[m]'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d7658e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hydromt\n",
    "from eva import eva_block_maxima, get_peaks, get_peak_hydrographs\n",
    "\n",
    "# discharge\n",
    "fnq = join(ddir, 'cama_discharge_beira_daily.nc')\n",
    "daq = xr.open_dataset(fnq)['discharge'].load()\n",
    "dsq = xr.merge([\n",
    "    daq.sel(index=1).rename('qb').reset_coords(drop=True),\n",
    "    daq.sel(index=4).rename('qp').reset_coords(drop=True)\n",
    "])\n",
    "# fnq = r'../../3_models/wflow/run_vito_ksath1000/output_src.nc'\n",
    "# daq = xr.open_dataset(fnq)['q_river'].load()\n",
    "# dsq = xr.merge([\n",
    "#     daq.sel(index=0).rename('qb').reset_coords(drop=True),\n",
    "#     daq.sel(index=3).rename('qp').reset_coords(drop=True)\n",
    "# ])\n",
    "\n",
    "# save qbankfull at river inflow locations of SFINCS\n",
    "dsq_eva = eva_block_maxima(\n",
    "    daq, period = 'AS-AUG', min_dist = 14,\n",
    ")\n",
    "gdf_qbf = dsq_eva['return_values'].sel(rps=2).to_dataset().vector.to_gdf().rename(columns={'return_values': 'qbankfull'})\n",
    "gdf_qbf.to_file(fnq.replace('.nc', '_qbf.geojson'), driver='GeoJSON')\n",
    "gdf_qbf['qbankfull']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc33d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GTSM waterlevels + ERA5 waves\n",
    "# contains: \"waterlevel\" (tide+surge), \"tide\", \"surge\", \"shww\"\n",
    "fnh = join(ddir, 'reanalysis_gtsm_v1_beira_extended.nc')\n",
    "dsh0 = xr.open_dataset(fnh).load()\n",
    "dsh0 = dsh0.rename({'waterlevel': 'h_ts', 'surge': 's', 'tide': 't', 'shww': 'w'})\n",
    "dsh0['h_tsw'] = dsh0['h_ts'] + 0.2*dsh0['w']\n",
    "dsh0['sw'] = dsh0['s'].fillna(0) + 0.2*dsh0['w']\n",
    "# skew surge (not used)\n",
    "# high_tide = get_peaks(dsh0['t'].load(), period='12H').dropna('time').reindex_like(dsh0, 'nearest')\n",
    "# dsh0['ss'] = dsh0['h_ts'] - high_tide\n",
    "# dsh0['ssw'] = dsh0['h_tsw'] - high_tide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be459f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eva import eva_idf, get_hyetograph\n",
    "# ERA5 precipitation\n",
    "fnp = join(ddir, 'era5_precip_beira_hourly_spatialmean.nc')\n",
    "dap0 = xr.open_dataset(fnp, chunks='auto')['precip'].load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59daa005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resample to day\n",
    "dates = pd.date_range('19800102', '20210201', freq='D')\n",
    "# dap = dap0.resample(time='1D', label='right').sum('time')\n",
    "# dsh = dsh0.resample(time='1D', label='right').max('time')\n",
    "dap = dap0.rolling(time=24, center=True, min_periods=1).mean('time').reindex(time=dates)\n",
    "dsh = dsh0.rolling(time=6*24, center=True, min_periods=1).max('time').reindex(time=dates)\n",
    "\n",
    "# merge all variables to singlge dataset\n",
    "ds = xr.merge([\n",
    "    dsq.reindex(time=dates),\n",
    "    dsh,\n",
    "    dap.rename('p')\n",
    "], compat='override').reset_coords(drop=True)#.reindex(time=dates)\n",
    "for var in ds.data_vars:\n",
    "    long_name, unit = labels[var].split('\\n')\n",
    "    ds[var].attrs.update({'long_name': long_name, 'unit': unit[1:-1]})\n",
    "ds.attrs = {}\n",
    "encoding = {var: {'zlib': True} for var in ds.data_vars}\n",
    "# ds.to_netcdf(join(ddir, 'beira_drivers_daily.nc'), encoding=encoding)\n",
    "ds = ds[['qb', 'qp', 'p', 'h_tsw', 't', 's', 'w']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4bae96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds = xr.open_dataset(join(ddir, 'beira_drivers_daily.nc'))\n",
    "ds = ds[['qb', 'qp', 'p', 'h_tsw', 't', 's', 'w']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72478be3",
   "metadata": {},
   "source": [
    "## get annual maxima peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a304f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eva import get_peak_hydrographs, get_peaks\n",
    "\n",
    "# settings\n",
    "period='AS-AUG'\n",
    "\n",
    "ds_peaks = xr.Dataset(coords=ds.coords)\n",
    "for dvar in ds.data_vars.keys():\n",
    "    if dvar == 't': continue\n",
    "    ds_peaks[dvar] = get_peaks(ds[dvar], period=period, min_dist=14, min_sample_size=0)\n",
    "\n",
    "# peaks with dates\n",
    "df_peaks0 = ds_peaks.reset_coords(drop=True).dropna('time', how='all').to_dataframe()  \n",
    "\n",
    "# get maximum values within time window for non_extremes\n",
    "df_peaks0_filled = pd.DataFrame()\n",
    "ds_tmax = ds.rolling(time=7).max('time').sel(time=df_peaks0.index)\n",
    "# ds_tmax = ds.sel(time=df_peaks0.index)\n",
    "for dvar in df_peaks0.columns:\n",
    "    df_peaks0_filled[dvar] = df_peaks0[dvar].where(df_peaks0[dvar].notna(), ds_tmax[dvar])\n",
    "\n",
    "# peaks with regular spaced interval\n",
    "df_bm = ds_peaks.resample(time=period).max('time').to_dataframe().dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb83b997",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(ds_peaks.data_vars)\n",
    "fig, axes = plt.subplots(n, 1, figsize=(12, 3*n), sharex=True)\n",
    "for i, dvar in enumerate(ds_peaks.data_vars.keys()):\n",
    "    ds[dvar].to_series().plot(ax=axes[i], color='k')\n",
    "    df_peaks0[dvar].plot(ax=axes[i], color='r', marker='.', lw=0)\n",
    "    axes[i].set_ylabel(labels[dvar])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88495437",
   "metadata": {},
   "source": [
    "## fit uni-variate eva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd53e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import interp1d\n",
    "from scipy.stats import gaussian_kde, rankdata\n",
    "\n",
    "class emperical_dist(object):\n",
    "    def __init__(self, data, nyears):\n",
    "        self.data = np.sort(data)[::-1]  # descending\n",
    "        nevents = data.size\n",
    "        self.freq = np.arange(1,nevents+1)/(nevents+1)*(nevents/nyears)\n",
    "        self.data = self.data[self.freq<1]\n",
    "        self.freq = self.freq[self.freq<1]\n",
    "\n",
    "    def ppf(self, q):\n",
    "        return interp1d(\n",
    "            1-self.freq, \n",
    "            self.data, \n",
    "            bounds_error = False, \n",
    "            fill_value=(self.data[-1], self.data[0])\n",
    "        )(q)\n",
    "        \n",
    "    def pdf(self, x, **kwargs):\n",
    "        return gaussian_kde(self.data, **kwargs)(x)\n",
    "\n",
    "    def sf(self, x):\n",
    "        return interp1d(\n",
    "            self.data,\n",
    "            self.freq,\n",
    "            bounds_error = False, \n",
    "            fill_value=(self.freq[-1], self.freq[0])\n",
    "        )(x)\n",
    "\n",
    "class rps_dist(object):\n",
    "    def __init__(self, rps, rvs):\n",
    "        self.rps = rps\n",
    "        self.rvs = rvs\n",
    "\n",
    "    def ppf(self, q):\n",
    "        return interp1d(\n",
    "            1-1/self.rps,\n",
    "            self.rvs,\n",
    "            bounds_error = False, \n",
    "            fill_value=(self.rvs[-1], self.rvs[0])\n",
    "        )(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cc3ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eva import lmoment_fitopt, get_frozen_dist, _get_return_periods, _RPS\n",
    "\n",
    "# prepare surge extremes\n",
    "fileS = 'Beira_STORM_surges.nc' #We look at the 3000 yr of data\n",
    "da_surge = xr.open_dataarray(join(ddir, fileS))#[-3000:]\n",
    "\n",
    "x_etc = ds_peaks['s'].dropna('time').to_series().sort_values().values[:-2]  # filter 2 TCs\n",
    "params, dist = lmoment_fitopt(x_etc, distributions=['gev', 'gumb'], criterium='AIC')\n",
    "dist_etc = get_frozen_dist(params, dist)\n",
    "\n",
    "x_tc = da_surge.values.flatten()\n",
    "dist_tc = emperical_dist(x_tc, 3000)\n",
    "\n",
    "# combine: rp(x) = 1 / (1/rp(x_TC) + 1/rp(x_ETC))\n",
    "xs = np.arange(0.1, np.max(x_tc), 0.1)\n",
    "rp_tc = 1/dist_tc.sf(xs)\n",
    "rp_etc = 1/(1-dist_etc.cdf(xs))\n",
    "rp_tot = 1/(1/rp_etc + 1/rp_tc)\n",
    "dist_surge = rps_dist(rp_tot, xs)\n",
    "\n",
    "# plot\n",
    "fgumbplot = lambda x: -np.log(-np.log(1.0 - 1.0 / x))\n",
    "\n",
    "fig, ax = plt.subplots(1,1)\n",
    "ax.plot(fgumbplot(_get_return_periods(x_etc)), x_etc, '.b', label='ETC')\n",
    "ax.plot(fgumbplot(_get_return_periods(x_tc, extremes_rate=x_tc.size/3000)), x_tc, '.r', label='TC')\n",
    "ax.plot(fgumbplot(rp_tot), xs, '--k', lw=2, label='combined')\n",
    "ax.set_ylabel(\"Return value\")\n",
    "ax.set_xticks(fgumbplot(_RPS))\n",
    "ax.set_xticklabels(_RPS)\n",
    "ax.set_xlabel(\"Return period\")\n",
    "ax.set_xlim([fgumbplot(1.1), fgumbplot(1000)])\n",
    "ax.grid()\n",
    "ax.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695c7d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eva import lmoment_fitopt, get_frozen_dist, plot_return_values, _RPS, _get_return_values\n",
    "fgumbplot = lambda x: -np.log(-np.log(1.0 - 1.0 / x))\n",
    "\n",
    "n = len(ds_peaks.data_vars)\n",
    "dparams = ['shape', 'loc', 'scale']\n",
    "distributions = ['gev', 'gumb']#[1:]\n",
    "\n",
    "fig, axes = plt.subplots(n, 1, figsize=(8, 3*n), sharex=True)\n",
    "\n",
    "df_eva = pd.DataFrame(columns=['dist'] + dparams)\n",
    "df_rps = pd.DataFrame(index=np.hstack([[1.1], _RPS]))\n",
    "df_rps.index.name = 'rps'\n",
    "dists = {}\n",
    "\n",
    "# use marginal distributions to transform quantiles back to normal space\n",
    "for dvar in df_eva.index:\n",
    "    params = df_eva.loc[dvar, dparams].dropna()\n",
    "    dist = df_eva.loc[dvar, 'dist']\n",
    "\n",
    "for i, dvar in enumerate(ds_peaks.data_vars.keys()):\n",
    "\n",
    "    if dvar == 's':\n",
    "        dists[dvar] = dist_surge\n",
    "        df_rps[dvar] = dist_surge.ppf(1-1/df_rps.index.values)\n",
    "        axes[i].plot(fgumbplot(_get_return_periods(x_etc)), x_etc, '.b', label='ETC')\n",
    "        axes[i].plot(fgumbplot(_get_return_periods(x_tc, extremes_rate=x_tc.size/3000)), x_tc, '.r', label='TC')\n",
    "        axes[i].plot(fgumbplot(rp_tot), xs, '--k', lw=2, label='combined')\n",
    "        axes[i].set_ylabel(\"Return value\")\n",
    "        axes[i].set_xticks(fgumbplot(_RPS))\n",
    "        axes[i].set_xticklabels(_RPS)\n",
    "        axes[i].set_xlabel(\"Return period\")\n",
    "        axes[i].grid()        \n",
    "        axes[i].legend()\n",
    "    else:\n",
    "        if dvar == 'p':\n",
    "            durations=np.array([1, 2, 3, 6, 12, 24], dtype=int)\n",
    "            da_p_bm = eva_idf(dap0, durations=durations, distribution='gumb', rps=df_rps.index.values)\n",
    "            da_p_bm0 = da_p_bm.sel(duration=24)\n",
    "            x = da_p_bm0['peaks'].dropna('time').values\n",
    "            params = da_p_bm0['parameters'].values[1:]\n",
    "            dist = da_p_bm0['distribution'].item()\n",
    "        else:\n",
    "            x = ds_peaks[dvar].dropna('time').values\n",
    "            params, dist = lmoment_fitopt(x, distributions=distributions, criterium='AIC')\n",
    "        dists[dvar] = get_frozen_dist(params, dist)\n",
    "        df_eva.loc[dvar, dparams[-len(params):]] = params\n",
    "        df_eva.loc[dvar, 'dist'] = dist\n",
    "        df_rps[dvar] = _get_return_values(params, dist, rps=df_rps.index.values)\n",
    "        _ = plot_return_values(x, params, dist, ax=axes[i])\n",
    "        axes[i].set_ylim([x.min()*0.9, axes[i].get_ylim()[1]])\n",
    "\n",
    "    axes[i].set_xlim([0.01, fgumbplot(1000)])\n",
    "    axes[i].set_title(dvar)\n",
    "    \n",
    "    axes[i].set_ylabel(labels[dvar])\n",
    "    if i < n-1:\n",
    "        axes[i].set_xlabel('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a769fac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "df_eva"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6aadff4",
   "metadata": {},
   "source": [
    "## analyse lag-times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7124e868",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "from hydromt.stats import pearson_correlation \n",
    "\n",
    "def time_lag_crosscorr(\n",
    "    sim, obs, quantile=None, lags=np.arange(-10,11,1), t_unit='days', dim='time'\n",
    "):\n",
    "    \"\"\"Returns the time lag between two time series based on a lag time \n",
    "    with the maximum pearson correlation.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sim : xarray DataArray\n",
    "        simulations time series\n",
    "    obs : xarray DataArray\n",
    "        observations time series\n",
    "    quantile : numpy ndarray, optional\n",
    "        quantile based threshold (the default is None, which does not use any threshold)\n",
    "    lags : numpy ndarray, optional\n",
    "        range of considered lag times (the default is np.arange(-10,11,1))\n",
    "    t_unit : str, optional\n",
    "        time unit used to parse lags to timedelta format (the default is 'days')\n",
    "    dim : str, optional\n",
    "        name of time dimension in sim and obs (the default is 'time')\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    xarray DataSet\n",
    "        lag time and associated correlation coefficient\n",
    "    \"\"\"\n",
    "\n",
    "    if quantile:\n",
    "        obs.load()        \n",
    "        obs = obs.where(obs>=obs.quantile(quantile, dim=dim))\n",
    "    # loop through time lags and calculate cross correlation\n",
    "    r = []\n",
    "    lags = np.asarray(lags)\n",
    "    time_org = sim[dim].to_index()\n",
    "    for dt in lags:\n",
    "        time_new = time_org + timedelta(**{t_unit: float(dt)})\n",
    "        ts = slice(max(time_org.min(), time_new.min()), min(time_org.max(), time_new.max()))\n",
    "        sim[dim] = time_new\n",
    "        r.append(pearson_correlation(sim.sel(**{dim:ts}), obs.sel(**{dim:ts})))\n",
    "    sim[dim] = time_org # reset time\n",
    "    pearsonr = xr.concat(r, dim='dt')\n",
    "    pearsonr['dt'] = xr.Variable('dt', lags)\n",
    "    # get maximum cross corr\n",
    "    pearsonr_max = pearsonr.max(dim='dt')\n",
    "    pearsonr_max.name = 'lag_rho'\n",
    "    pearsonr_max.attrs.update(description='maximum pearson coefficient for given time lag')\n",
    "    # get lag time of maximum cross corr\n",
    "    # NOTE that we assume a evenly spaced lag times\n",
    "    lag = xr.where(\n",
    "        xr.ufuncs.isfinite(pearsonr).sum(dim='dt')==lags.size,\n",
    "        pearsonr.argmax(dim='dt', skipna=False), \n",
    "        np.nan)*np.diff(lags)[0] + lags.min()\n",
    "    lag.name = 'lag'\n",
    "    lag.attrs.update(description='time lag with maximum pearson coefficient', unit=t_unit)\n",
    "    # merge max cross corr and lag tiem\n",
    "    return xr.merge([lag, pearsonr_max])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60aef732",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_dvar = 'qb'\n",
    "dt_lst =[]\n",
    "dvar_lst = ['qp', 'p', 's', 'w']\n",
    "for dvar in dvar_lst:\n",
    "    da_out = time_lag_crosscorr(ds[ref_dvar], ds[dvar]).reset_coords(drop=True).compute()\n",
    "    dt_lst.append(da_out)\n",
    "ds_dt = xr.concat(dt_lst, dim='dvar')\n",
    "ds_dt['dvar'] = xr.IndexVariable('dvar', dvar_lst)\n",
    "df_timelags = ds_dt['lag'].to_series()\n",
    "# df_timelags.to_csv(r'../../4_results/lagtimes.csv')\n",
    "df_timelags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f0311f",
   "metadata": {},
   "source": [
    "## analyse co-occurence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae742fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE that `max_time_lag` is the timelag between two consecutive extremes\n",
    "# the total time lag between all four events could theoretically be the sum of all lagtime per driver\n",
    "## settings\n",
    "drivers = [v for v in ['qb', 'qp', 'p', 's', 'w'] if v in ds]\n",
    "max_time_lag = {\n",
    "    'qp': pd.Timedelta('5D'),\n",
    "    'qb': pd.Timedelta('5D'),\n",
    "}\n",
    "dt0 = pd.Timedelta('2D')  # default time lag\n",
    "\n",
    "# reduce daily events to selected drivers\n",
    "df_peaks = df_peaks0[drivers].dropna(how='all')\n",
    "\n",
    "# combine daily events to event sets based on maximum time_lag\n",
    "df_peaks['dt'] = np.hstack([[0], np.diff(df_peaks.index)])\n",
    "df_peaks['max_dt'] = (df_peaks[drivers].notna()*np.array([max_time_lag.get(d,dt0) for d in drivers])).max(axis=1)\n",
    "# df_peaks['max_dt'] = [pd.Timedelta(days=d) for d in df_peaks['max_dt'].dt.days.rolling(2, min_periods=1).max().values]\n",
    "df_peaks['event'] = np.cumsum(~(df_peaks['dt']<df_peaks['max_dt']))\n",
    "df_events = df_peaks.groupby('event').apply(lambda x: x.notna().sum()).drop(columns=['dt', 'event'])\n",
    "df_events['n_extreme'] = df_events[drivers].sum(axis=1)\n",
    "df_events['time'] = df_peaks.reset_index().groupby('event').first()['time']\n",
    "offset = pd.Timedelta(days=df_bm.index.dayofyear.min()-1)\n",
    "df_events['year'] = (df_events['time'] - offset).dt.year\n",
    "df_events['time_lag'] = [evnt.iloc[1:,:]['dt'].sum() for _, evnt in df_peaks.groupby('event')]\n",
    "evnts, cnts = np.unique(df_peaks['event'], return_counts=True)\n",
    "df_peaks['co-occur'] = np.isin(df_peaks['event'], evnts[cnts>1])\n",
    "\n",
    "# make barcode plot for co-occuring events\n",
    "data = df_events[drivers[::-1]].T * df_events['n_extreme']\n",
    "_, ax = plt.subplots(1,1, figsize=(12,5))\n",
    "im = ax.imshow(data.where(data>0), interpolation='none', cmap='Blues', vmin=0, vmax=len(drivers))\n",
    "ax.set_aspect(2)\n",
    "ax.set_yticks(np.arange(len(drivers)))\n",
    "ax.set_yticklabels([labels[dvar].split('\\n')[0] for dvar in drivers[::-1]])\n",
    "# fig.colorbar(im, orientation='vertical')\n",
    "ax.set_xlabel('events timeline')\n",
    "xlabs = df_events.reset_index().groupby('year').first()[['event']]\n",
    "_ = ax.set_xticks(xlabs['event'].values[::2]-1.5)\n",
    "_ = ax.set_xticklabels(xlabs.index.values[::2], rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig(join(r'../../4_results', 'co-occurence_time.png'), dpi=300, bbox_axes='tight')\n",
    "\n",
    "\n",
    "# remove incomplete years\n",
    "check = df_events[['n_extreme', 'year']].groupby('year').sum()\n",
    "drop_years = check[check['n_extreme']!=len(drivers)].index.values\n",
    "if drop_years.size > 1:\n",
    "    df_events = df_events[~np.isin(df_events['year'], drop_years)]\n",
    "    print(f'ignore years with more/less events than drivers: {drop_years}')\n",
    "print(f'no. of valid years: {np.unique(df_events.year).size}')\n",
    "print(f'no. of events: {df_events.index.size}; (>1 extreme: {df_events[df_events.n_extreme>1].index.size})')\n",
    "print(f'max time lag: {df_events.time_lag.max().days} days')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf5b3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_events_dist = df_events.groupby(drivers + ['n_extreme']).size().reset_index(name='count')\n",
    "df_events_dist = df_events_dist.sort_values('count', ascending=False)\n",
    "\n",
    "\n",
    "fig, (ax1, ax) = plt.subplots(2,1, figsize=(7,3), sharex=True, gridspec_kw={'height_ratios': [2, 1], 'hspace':0.0})\n",
    "df_events_dist.reset_index()['count'].plot.bar(ax=ax1, color='k')\n",
    "ax1.set_ylabel('count [-]')\n",
    "\n",
    "x, y = np.where(df_events_dist[drivers])\n",
    "ax.scatter(x, y, color='k')\n",
    "ax.set_ylim([-0.5, len(drivers)-0.5])\n",
    "ax.set_yticks(np.arange(len(drivers)))\n",
    "ax.set_yticklabels([labels[dvar].split('\\n')[0] for dvar in drivers])\n",
    "ax.set_xlabel('event type')\n",
    "ax.set_xticklabels('')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# plt.savefig(join(r'../../4_results', 'co-occurence_hist.png'), dpi=300, bbox_axes='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef4568d",
   "metadata": {},
   "source": [
    "## dependence modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f79ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyvinecopulib as pv\n",
    "\n",
    "# Transform copula data using the empirical distribution\n",
    "df_uobs = pd.DataFrame(columns=drivers, data = pv.to_pseudo_obs(df_bm[drivers].values))\n",
    "\n",
    "# fit copula\n",
    "controls = pv.FitControlsVinecop(\n",
    "    # family_set=[pv.BicopFamily(x) for x in np.arange(0,11)],\n",
    "    family_set=[pv.BicopFamily(0)], # idependent only\n",
    "    # parametric_method='itau',\n",
    "    # nonparametric_method='quadratic'\n",
    "    threshold=0.05, # tau threshold   (0.0 default!)\n",
    "    selection_criterion='aic', # loglik, bic, aic (bic default!)\n",
    "    # tree_criterion='tau',  # tau, hoeffd, rho, mcor -> no effect ?!\n",
    "    show_trace=True,\n",
    ")\n",
    "cop = pv.Vinecop(data=df_uobs.values, controls=controls)#, structure=pv.RVineStructure(len(drivers)))\n",
    "# cop.structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f645f24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "m = cop.matrix\n",
    "n = m.shape[0]\n",
    "v = drivers\n",
    "for t in range(n-1):\n",
    "    # print(f'** Tree: {t:d}')\n",
    "    for e in range(n-1-t):\n",
    "        p1, p2 = v[int(m[n-1-e,e]-1)], v[int(m[t,e]-1)]\n",
    "        px = [v[int(p-1)] for p in m[:t,e]]\n",
    "        c = cop.get_pair_copula(t,e)\n",
    "        tau = cop.get_tau(t,e)  # NOTE: diffferent from scipy.stats method ?\n",
    "        pxs = f' | ' + ','.join(px) if px else ''\n",
    "        cstr = c.str().replace(f'\\n',',')\n",
    "        print(f'{p1},{p2}{pxs}: {cstr}; tau = {tau:.5f}')\n",
    "        data.append([t, e, [p1,p2], px, c.str().split(',')[0], c.parameters.flatten(), tau])\n",
    "\n",
    "df_cop = pd.DataFrame(\n",
    "    data=data,\n",
    "    columns=['tree', 'edge', 'pair', 'conditional', 'copula', 'parameters', 'tau']\n",
    ")\n",
    "df_cop.set_index(['tree', 'edge'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91ff71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from the copula\n",
    "seeds = np.arange(len(drivers), dtype=int)\n",
    "n_sim = 10000\n",
    "df_usim = pd.DataFrame(data=cop.simulate(n_sim, seeds=seeds), columns=drivers)\n",
    "\n",
    "# transform back based on marginal distributions\n",
    "df_sim = pd.DataFrame()\n",
    "s = df_usim.index.size\n",
    "for dvar in drivers:\n",
    "    df_sim[dvar] = np.maximum(0, dists[dvar].ppf(df_usim[dvar].values))\n",
    "\n",
    "# df_sim_q = pd.DataFrame()\n",
    "# df_sim_q[dvar] = np.quantile(df_bm[dvar], df_usim[dvar])\n",
    "    \n",
    "df_sim.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d4a0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kendalltau, pearsonr, gaussian_kde\n",
    "n = len(drivers)-1\n",
    "fig, axes = plt.subplots(n,n, sharex=False, sharey=False, figsize=(n*2.5,n*2.5), gridspec_kw={'hspace':0.0, 'wspace':0.0})\n",
    "\n",
    "# df_uobs_cooc = df_uobs.where(df_peaks[drivers][df_peaks['co-occur']].resample(period).max().reset_index(drop=True).notna())\n",
    "df_obs_cooc = df_peaks[drivers][df_peaks['co-occur']].resample(period).max()\n",
    "\n",
    "for r in range(n):\n",
    "    for c in range(n):\n",
    "        if c > r:\n",
    "            axes[r,c].set_visible(False)\n",
    "            continue\n",
    "\n",
    "for _, c0 in df_cop.iterrows():\n",
    "    c = c0.edge \n",
    "    r = c + c0.tree\n",
    "    ax = axes[r,c]\n",
    "    xlab, ylab = c0.pair\n",
    "    x, y = df_bm[xlab].values, df_bm[ylab].values\n",
    "    xmin, xmax = np.quantile(x, [0,0.95])\n",
    "    ymin, ymax = np.quantile(y, [0,0.95])\n",
    "    ymax = ymax + (ymax-ymin)*0.4\n",
    "    \n",
    "    # if len(c0) > 0 and c0.copula != 'Independence':\n",
    "    xx, yy = np.mgrid[xmin:xmax:100j, ymin:ymax:100j]\n",
    "    positions = np.vstack([xx.ravel(), yy.ravel()])\n",
    "    # kernel = gaussian_kde(df_bm[[xlab,ylab]].values.T, bw_method=0.5)\n",
    "    kernel = gaussian_kde(df_sim[[xlab,ylab]].values.T, bw_method=0.5)\n",
    "    f = np.reshape(kernel(positions).T, xx.shape)\n",
    "    cmap = 'Blues' if  c0.copula != 'Independence' else 'Greens'\n",
    "    ax.contourf(xx, yy, f, cmap=cmap, alpha=0.5)\n",
    "\n",
    "    # plot events\n",
    "    x, y = df_bm[xlab].values, df_bm[ylab].values\n",
    "    xcooc, ycooc = df_obs_cooc[xlab].values, df_obs_cooc[ylab].values\n",
    "    ax.scatter(xcooc, ycooc, s=13, color='darkorange', label='compound', zorder=2)\n",
    "    ax.scatter(x, y, s=12, color='black', label='other', alpha=0.9, zorder=1)\n",
    "    if r == n-1 and c == n-2:\n",
    "        ax.legend(title='AM events', loc='lower left', bbox_to_anchor=(1.1, 1.1))\n",
    "    kwargs = dict(        \n",
    "        horizontalalignment='left',\n",
    "        verticalalignment='top',\n",
    "        transform=ax.transAxes,\n",
    "        bbox=dict(facecolor='white', alpha=0.5, lw=0.1)\n",
    "    )\n",
    "    # tau, ptau = kendalltau(x, y)\n",
    "    # rho, prho = pearsonr(x, y)\n",
    "    # tsig = ('**' if ptau < 0.05 else '*') if ptau < 0.10 else ''\n",
    "    # rsig = ('**' if prho < 0.05 else '*') if prho < 0.10 else ''\n",
    "    # name = c0.copula if c0.copula != 'Independence' else 'Indep.'\n",
    "    cs = ' | ' + ','.join(c0.conditional) if c0.tree > 0 else ''\n",
    "    ps = ','.join(c0.pair[::-1])\n",
    "    ns = f'{c0.copula}' + r' ($\\tau$=' + f'{c0.tau:.2f})' if c0.copula != 'Independence' else c0.copula\n",
    "    txt = f'[{ps}{cs}]\\n{ns}' #{tsig}; ' + r'$\\rho$='+f'{rho:.2f}{rsig})'\n",
    "    ax.text(0.02, 0.98, txt, **kwargs)\n",
    "\n",
    "    if c == 0:\n",
    "        ax.set_ylabel(labels[ylab].replace('\\n', f' ({ylab})\\n'))\n",
    "    else:\n",
    "        ax.set_yticklabels('')\n",
    "    if r == n-1:\n",
    "        ax.set_xlabel(labels[xlab].replace('\\n', f' ({xlab})\\n'))\n",
    "    ax.set_ylim([ymin, ymax])\n",
    "    ax.set_xlim([xmin, xmax])\n",
    "    \n",
    "# plt.savefig(join(r'../../4_results', 'copula_autofit.png'), dpi=300, bbox_axes='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2a9a36",
   "metadata": {},
   "source": [
    "## create stochastic event set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7762ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine co-occurence and simulations\n",
    "from itertools import cycle\n",
    "event_cycle = cycle(df_events.groupby('year'))\n",
    "sim_events = []\n",
    "for year, row in df_sim[drivers].iterrows():\n",
    "    event_yr = next(event_cycle)[1][drivers] \n",
    "    events = (event_yr * row)\n",
    "    events['year'] = year+1\n",
    "    sim_events.append(events)\n",
    "\n",
    "df_sim_events0 = pd.concat(sim_events, axis=0, ignore_index=True)\n",
    "df_sim_events = df_sim_events0\n",
    "df_sim_events0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d15d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# required for simulating h_tws & plot comparing sim with obs events\n",
    "xval_norm = {}\n",
    "thresh_lst = df_sim.quantile(0.01)\n",
    "s = df_sim_events0.index.size\n",
    "df_sim_events = df_sim_events0.copy(deep=True)\n",
    "for dvar in drivers:\n",
    "    # we randomly sample non-extremes from all values below the rp1 threshold\n",
    "    thresh = dists[dvar].ppf(1-1/1.01)\n",
    "    xval_norm = ds[dvar].where(ds[dvar] < thresh,drop=True).to_series().dropna()\n",
    "    x0 = xval_norm.sample(s, replace=True, random_state=np.sum(seeds)) \n",
    "    xs = df_sim_events0[dvar]\n",
    "    df_sim_events[dvar] = np.where(xs==0, x0, xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd354fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine waterlevel components\n",
    "tide = ds['t'].dropna('time').to_series()  # daily high tide\n",
    "tide_am = ds['t'].resample(time='AS').max('time').dropna('time').to_series()\n",
    "# combine surge with random daily high tide\n",
    "df_sim_events['t'] = tide.sample(s, replace=True, random_state=np.sum(seeds)).values\n",
    "\n",
    "hname = 'h_tsw' if 'w' in ds else 'h_ts' \n",
    "df_sim_events[hname] = df_sim_events['t'] + df_sim_events['s'] \n",
    "if 'w' in ds:\n",
    "    df_sim_events[hname] = df_sim_events[hname] + 0.2 * df_sim_events['w']\n",
    "# at least AM tide for events with extreme\n",
    "htot_am_idx = df_sim_events[[hname, 'year']].groupby('year').idxmax().values.flatten()\n",
    "h_am = np.maximum(df_sim_events[hname], tide_am.sample(s, replace=True, random_state=np.sum(seeds)).values)\n",
    "df_sim_events.loc[htot_am_idx, hname] = np.maximum(df_sim_events.loc[htot_am_idx, hname], h_am.loc[htot_am_idx])\n",
    "\n",
    "# get AM waterlevel\n",
    "df_sim_am = df_sim_events.groupby('year').max()\n",
    "df_sim_am.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c435835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results important to keep return values of h_tsw consistent with sensitivity analysis\n",
    "df_sim_am.round(3).to_csv('../../4_results/sim_AM_indep.csv')\n",
    "df_sim_events.round(3).to_csv('../../4_results/sim_EVENTS_indep.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ea3ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dvars = drivers + [hname]\n",
    "rm = {k:v for k,v in labels.items() if k in dvars}\n",
    "rm_lab = {k: v.replace(' (','\\n(') for k,v in rm.items()}\n",
    "df_obs_events = df_peaks0_filled[dvars].dropna(how='any')\n",
    "df_merged = pd.concat([\n",
    "    df_sim_events[dvars],\n",
    "    df_obs_events, \n",
    "    ], ignore_index=True, axis=0).rename(columns=rm)\n",
    "colors = np.hstack([\n",
    "    np.full(df_sim_events.index.size, \"k\"),\n",
    "    np.full(df_obs_events.index.size, \"r\"), \n",
    "    ])\n",
    "sizes = np.hstack([\n",
    "    np.full(df_sim_events.index.size, 5),\n",
    "    np.full(df_obs_events.index.size, 30), \n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdae5c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "axes = pd.plotting.scatter_matrix(df_merged, color=colors, s=sizes, alpha=1.0, diagonal='kde', figsize=(10,10))\n",
    "for i, dvar in enumerate(dvars):\n",
    "    axes[i,i].lines.pop(0)\n",
    "    xmin, xmax = df_merged[rm[dvar]].quantile([0.0, 0.998]).values\n",
    "    rp1 = dists[dvar].ppf(1-1/1.01)\n",
    "    # rp1 = thresh_lst[dvar]\n",
    "    for j in range(len(dvars)):\n",
    "        axes[j,i].set_xlim([xmin, xmax])\n",
    "        axes[j,i].axvline(rp1, color='c', ls='-', label='rp1')#, lw=0.5)\n",
    "        if j != i: \n",
    "            axes[i,j].set_ylim([xmin, xmax])\n",
    "            axes[i,j].axhline(rp1, color='c', ls='--')#, lw=0.5)\n",
    "    # fix ticks ..\n",
    "    xticks = axes[-1,i].get_xticks()\n",
    "    xticks = xticks[np.logical_and(xticks>xmin, xticks<xmax)]\n",
    "    axes[-1,i].set_xticks(xticks)\n",
    "    axes[-1,i].set_xticklabels(xticks, rotation=0)\n",
    "    if i == 0:\n",
    "        ymin,ymax = axes[0,0].get_ylim()\n",
    "        axes[i,i].set_yticks(xticks*(ymax-ymin)/(xmax-xmin))\n",
    "        axes[i,i].set_yticklabels(xticks)\n",
    "    df_sim_events[dvar].plot.kde(ax=axes[i,i], color='k', label='sim', legend=i==0)\n",
    "    df_obs_events[dvar].plot.kde(ax=axes[i,i], color='r', label='obs', legend=i==0)\n",
    "    if i == 0:\n",
    "        axes[i,i].set_ylabel(rm[dvar])\n",
    "    else:\n",
    "        axes[i,i].yaxis.set_visible(False)\n",
    "\n",
    "for r in range(len(dvars)):\n",
    "    for c in range(len(dvars)):\n",
    "        if c > r:\n",
    "            axes[r,c].set_visible(False)\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b125a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = pd.concat([\n",
    "    df_sim_am[dvars],\n",
    "    df_bm[dvars], \n",
    "    ], ignore_index=True, axis=0).rename(columns=rm_lab)\n",
    "colors = np.hstack([\n",
    "    np.full(df_sim_am.index.size, \"k\"),\n",
    "    np.full(df_bm.index.size, \"r\"), \n",
    "    ])\n",
    "sizes = np.hstack([\n",
    "    np.full(df_sim_am.index.size, 8),\n",
    "    np.full(df_bm.index.size, 30), \n",
    "    ])\n",
    "axes = pd.plotting.scatter_matrix(df_merged, color=colors, s=sizes, alpha=1.0, diagonal=None, figsize=(10,10))\n",
    "for i, dvar in enumerate(dvars):\n",
    "    xmin, xmax = df_merged[rm_lab[dvar]].quantile([0.0, 0.998]).values\n",
    "    for j in range(len(dvars)):\n",
    "        axes[j,i].set_xlim([xmin, xmax])\n",
    "        if j != i: \n",
    "            axes[i,j].set_ylim([xmin, xmax])\n",
    "    # fix ticks ..\n",
    "    xticks = axes[-1,i].get_xticks()\n",
    "    xticks = xticks[np.logical_and(xticks>xmin, xticks<xmax)]\n",
    "    axes[-1,i].set_xticks(xticks)\n",
    "    axes[-1,i].set_xticklabels(xticks, rotation=0)\n",
    "    if i == 0:\n",
    "        ymin,ymax = axes[0,0].get_ylim()\n",
    "        axes[i,i].set_yticks(xticks*(ymax-ymin)/(xmax-xmin))\n",
    "        axes[i,i].set_yticklabels(xticks)\n",
    "    \n",
    "    # axes[i,i].lines.pop(0)\n",
    "    # df_sim_am[dvar].plot.kde(ax=axes[i,i], color='k', label='sim', legend=i==0)\n",
    "    # if dvar != 's':\n",
    "    #     df_bm[dvar].plot.kde(ax=axes[i,i], color='r', label='obs', legend=i==0)\n",
    "    # else:\n",
    "    #     df_bm[dvar][:-2].plot.kde(ax=axes[i,i], color='r', label='obs', legend=i==0)\n",
    "    # if i == 0:\n",
    "    #     axes[i,i].set_ylabel(rm_lab[dvar])\n",
    "    # else:\n",
    "    #     axes[i,i].yaxis.set_visible(False)\n",
    "    \n",
    "for r in range(len(dvars)):\n",
    "    for c in range(len(dvars)):\n",
    "        if c >= r:\n",
    "            axes[r,c].set_visible(False)\n",
    "            continue\n",
    "\n",
    "axes[1,0].plot(0,0, f'.r', label='sim')\n",
    "axes[1,0].plot(0,0, f'.k', label='obs')\n",
    "axes[1,0].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(left=0.1,right=1,bottom=0.1,top=1, wspace=0, hspace=0)\n",
    "# plt.savefig(join(r'../../4_results', 'stochastic_events_AM.png'), dpi=300, bbox_axes='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f247335",
   "metadata": {},
   "source": [
    "## create forcing time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b279c38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_center(x, dim):\n",
    "    a = x.get_axis_num(dim)\n",
    "    n = x[dim].size\n",
    "    x_sorted = np.apply_along_axis(np.sort, a, x)\n",
    "    idx = [None if i!=a else range(n) for i in range(x.ndim)]\n",
    "    reorder = np.append(np.arange(0, n, 2), np.arange(1, n, 2)[::-1])\n",
    "    return xr.DataArray(np.take_along_axis(x_sorted, reorder[idx], a), x.coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c98611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# discharge: qb, qp\n",
    "# get design hydrographs by vertical averaging normalized AM peak hydrographs\n",
    "df_rps.loc[0,:] = 0\n",
    "df_rps = df_rps.sort_index()\n",
    "q_event_lst = []\n",
    "for dvar in ['qp', 'qb']:\n",
    "    # update RP0 based on wet season average\n",
    "    m = ds[dvar].time.dt.month\n",
    "    df_rps.loc[0,dvar] = ds[dvar].isel(time=np.logical_or(m>=11, m<=4)).mean('time').compute().item()\n",
    "    # create events\n",
    "    daq_hydrograph = get_peak_hydrographs(ds[dvar], ds_peaks[dvar], wdw_size=21).compute().mean('peak')\n",
    "    # if dvar in df_timelags:\n",
    "    #     daq_hydrograph['time'] = daq_hydrograph['time'] + df_timelags[dvar]\n",
    "    daq_events = df_rps[dvar].to_xarray() * daq_hydrograph\n",
    "    q_event_lst.append(daq_events)\n",
    "dsq_events = xr.merge(q_event_lst).dropna('time')\n",
    "dsq_events['time'] = dsq_events['time']*24\n",
    "dsq_events['time'].attrs.update(units='hour')\n",
    "_ = dsq_events['qb'].sel(rps=100).plot.line(x='time')\n",
    "_ = dsq_events['qp'].sel(rps=100).plot.line(x='time')\n",
    "# dsq_events.to_netcdf(fnq.replace('.nc', '_events.nc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34eec72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coastal\n",
    "\n",
    "# update RPS h_tsw based on stochastic event set\n",
    "df_sim_am0 = pd.read_csv('../../4_results/sim_AM_indep.csv', index_col=0)\n",
    "dists['h_tsw'] = emperical_dist(df_sim_am0['h_tsw'].values, df_sim_am0['h_tsw'].size)\n",
    "df_rps.loc[:,'h_tsw'] = dists['h_tsw'].ppf(1-1/df_rps.index.values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a743b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get design hydrographs by horizontal averaging normalized AM peak hydrographs\n",
    "dasw = dsh0['sw']#.rolling(time=48, center=True).mean('time')\n",
    "dasw_peaks = get_peaks(dasw, \"BM\", min_dist=6*24*5, period='AS-AUG')\n",
    "dasw_hydrographs = get_peak_hydrographs(\n",
    "    dasw,\n",
    "    dasw_peaks, \n",
    "    wdw_size=int(6*24*5), \n",
    "    normalize=True,\n",
    "    # n_peaks=20\n",
    ")\n",
    "dasw_hydrographs = np.maximum(0,dasw_hydrographs.dropna('peak'))\n",
    "dasw_hydrographs['time'] = dasw_hydrographs['time']/6  # hr\n",
    "dasw_hydrograph = sort_center(dasw_hydrographs, dim='time').mean('peak') # horizontal averaging\n",
    "# dasw_hydrograph = dasw_hydrographs.mean('peak') # vertical averaging\n",
    "\n",
    "da_mhws_peaks = get_peaks(dsh0['t'], \"BM\", min_dist=6*24*10, period=\"29.5D\")\n",
    "da_mhws_hydrographs = get_peak_hydrographs(\n",
    "    dsh0['t'], da_mhws_peaks, \n",
    "    wdw_size=int(6*24*14.5), \n",
    "    normalize=False,\n",
    ")\n",
    "da_mhws_hydrographs['time'] = da_mhws_hydrographs['time']/6 # hr\n",
    "da_mhws_hydrograph = da_mhws_hydrographs.mean('peak')\n",
    "mhws = 3.8 # highest astronomical tide IHO tidal constituents\n",
    "df_rps.loc[0,'h_tsw'] = mhws\n",
    "da_mhws_hydrograph = da_mhws_hydrograph/da_mhws_hydrograph.max('time')*mhws\n",
    "dasw_hydrograph = dasw_hydrograph.reindex(time=da_mhws_hydrograph.time, fill_value=0)\n",
    "\n",
    "da_h_events = da_mhws_hydrograph + dasw_hydrograph * (df_rps['h_tsw'].to_xarray()-mhws)\n",
    "# da_h_events['time'] = da_h_events['time'] + df_timelags[['s','w']].min()*24\n",
    "da_h_events['time'].attrs.update(units='hour')\n",
    "_ = da_h_events.sel(rps=[0,2,500]).plot.line(x='time')\n",
    "# da_h_events.to_netcdf(fnh.replace('.nc', '_events.nc'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5e77ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# precip\n",
    "# get design events from IDF curves and alternating block method\n",
    "da_p_events = get_hyetograph(da_p_bm['return_values'], dt=1, length=durations[-1])\n",
    "# da_p_events['time'] = da_p_events['time'] + df_timelags['p']*24\n",
    "da_p_events['time'].attrs.update(units='hour')\n",
    "_ = da_p_events.sel(rps=[2,500]).plot.line(x='time')\n",
    "# da_p_events.to_netcdf(fnp.replace('.nc', '_events.nc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc4d6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sim_events.index.name = 'event'\n",
    "df_sim_events1 = df_sim_events.reset_index().set_index(['event','year']).drop(columns=['t','s','w'])\n",
    "for dvar in df_sim_events1.columns:\n",
    "    thresh = dists[dvar].ppf(1-1/1.01)\n",
    "    v = df_sim_events1[dvar]\n",
    "    df_sim_events1[f'{dvar}_rp'] = np.minimum(500, np.where(v>thresh, 1/dists[dvar].sf(v), 0))\n",
    "df_sim_events1.round(3).to_csv(r'../../4_results/sim_EVENTS_rp.csv')\n",
    "df_sim_events1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16df056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all scenarios used for linear interpolating damages\n",
    "from itertools import product\n",
    "\n",
    "def interpolator(coords1d, point) :\n",
    "    dims = len(point)\n",
    "    imax = len(coords1d)-1\n",
    "    indices = []\n",
    "    sub_coords = []\n",
    "    for j in range(dims):\n",
    "        idx = np.digitize([point[j]], coords1d)[0]\n",
    "        indices += [[max(0,idx - 1), min(imax, idx)]]\n",
    "        sub_coords += [coords1d[indices[-1]]]\n",
    "    coords_out = np.array([j for j in product(*sub_coords)])\n",
    "    return coords_out\n",
    "\n",
    "rps = np.hstack([[0], _RPS])\n",
    "rps = np.array([  0,   2,   5,  10,  25,  50, 100, 250, 500])\n",
    "cols = ['qb_rp', 'qp_rp', 'p_rp', 'h_tsw_rp']\n",
    "print(len(rps)**len(cols))\n",
    "scens = []\n",
    "\n",
    "values = df_sim_events1.loc[:,cols].values\n",
    "for event in values:\n",
    "    scens.append(interpolator(rps, event))\n",
    "# include univariate\n",
    "rps0 = np.zeros(4, dtype=int)\n",
    "for i in range(4):\n",
    "    for rp in rps[1:]:\n",
    "        _rps = rps0.tolist()\n",
    "        _rps[i] = rp\n",
    "        scens.append(_rps)\n",
    "# include full dependence\n",
    "for rp in rps:\n",
    "    scens.append(np.full(4, rp, dtype=int).tolist())\n",
    "##\n",
    "df_scen = pd.DataFrame(data=np.vstack(scens), columns=cols).value_counts().rename('count').reset_index()\n",
    "df_scen['scen'] = [\n",
    "    f\"qb{qb_rp:03.0f}_qp{qp_rp:03.0f}_h{h_rp:03.0f}_p{p_rp:03.0f}\" \n",
    "    for i, (qb_rp, qp_rp, p_rp, h_rp) in df_scen[cols].iterrows()\n",
    "]\n",
    "df_scen\n",
    "print(df_scen.index.size)\n",
    "df_scen.to_csv(r'../../4_results/sim_SCEN_indep.csv')\n",
    "df_scen.sort_values('count', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eddbb104",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scen0 = pd.read_csv(r'../../4_results/sim_SCEN.csv', index_col=0)\n",
    "df_scen[~np.isin(df_scen['scen'], df_scen0['scen'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb6cc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_rps.round(3).to_csv(r'../../4_results/rps.csv')\n",
    "df_rps"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "38648ff479392915c1a5d77722aa6edad827edf2098e3798b9c4282ba45e9fb7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 ('compound_risk')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
