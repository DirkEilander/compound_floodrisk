{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045de6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hydromt_sfincs import SfincsModel, utils\n",
    "import numpy as np\n",
    "from os.path import join\n",
    "import hydromt\n",
    "import glob\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "import xarray as xr\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94124cce",
   "metadata": {},
   "source": [
    "## read flood and model data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5c9431",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdir = r\"../../3_models/sfincs\"\n",
    "rdir = r\"../../4_results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814c2c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod0 = SfincsModel(join(mdir, '00_base_riv'), mode='r')\n",
    "rivmsk = mod0.staticmaps['rivmsk'].raster.flipud()==1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2d7240",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_cols = ['h_rp','p_rp','qb_rp','qp_rp']\n",
    "ds = xr.open_zarr(join(rdir, 'hmax.zarr')).swap_dims({'index':'scen'}).set_coords(index_cols)\n",
    "da = ds['hmax'].fillna(0)\n",
    "da.attrs.update(_FillValue=da.attrs['nodatavals'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd6aa44",
   "metadata": {},
   "source": [
    "## Bias correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be8389e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bias correct for Q and H but make sure to keep P\n",
    "nodata = da.attrs['_FillValue']\n",
    "# get Q2 + MSHW map for bias correction\n",
    "bias, da0 = 'q2', da.sel(scen=['qb002_qp002_h000_p000']).squeeze(drop=True)\n",
    "mask = np.logical_and(da0!=nodata, ~rivmsk)\n",
    "# account for minimal P in all scenarios\n",
    "pscen = [f'qb000_qp000_h000_p{s[-3:]}' for s in da.scen.values]\n",
    "dap = da.sel(scen=pscen).reset_coords(drop=True).drop('scen')\n",
    "dap = np.maximum(0, dap - da.sel(scen=['qb000_qp000_h000_p000']).squeeze(drop=True))\n",
    "# take maximum of pscen and bias corrected scens\n",
    "da1 = np.maximum(dap, da-da0).where(mask, 0)\n",
    "# da1.attrs.update(**da.attrs)\n",
    "# da1 = da1.chunk({'x':-1, 'y':-1, 'scen':10})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65dd0e4e",
   "metadata": {},
   "source": [
    "## calculate compound flood depths based on 5000 yrs sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8bbbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_sample = pd.read_csv(join(rdir, r'sim_EVENTS_rp.csv'), index_col=0).rename(columns={'h_tsw_rp': 'h_rp', 'h_tsw': 'h'})\n",
    "\n",
    "\n",
    "df_flood = da1.reset_coords().drop(['band', 'spatial_ref', 'x', 'y', 'hmax']).to_dataframe()\n",
    "# df_flood[index_cols] = np.maximum(1, df_flood[index_cols])\n",
    "ds_flood_rp = df_flood.reset_index().drop(columns=index_cols).set_index(\n",
    "    pd.MultiIndex.from_frame(df_flood[index_cols])\n",
    ").to_xarray()\n",
    "ds_flood_rp['index'] = ds_flood_rp['index'].fillna(-1).astype(int)\n",
    "ds_flood_rp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8eade7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sim_events1 = pd.read_csv(join(rdir, r'sim_EVENTS_rp.csv'), index_col=0)#.rename(columns={'h_tsw_rp': 'h_rp', 'h_tsw': 'h'})\n",
    "\n",
    "# get all scenarios used for linear interpolating damages\n",
    "rps = np.array([  0,  2,   5,  10,  25,  50, 100, 250, 500])\n",
    "cols = ['qb_rp', 'qp_rp', 'p_rp', 'h_tsw_rp']\n",
    "print(len(rps)**len(cols))\n",
    "scens = []\n",
    "\n",
    "values = df_sim_events1.loc[:,cols].values\n",
    "for event in values:\n",
    "    rps_lst = []\n",
    "    for rp in event:\n",
    "        if rp in rps:\n",
    "            rps_lst.append([int(rp), int(rp)])\n",
    "        else:\n",
    "            rps_lst.append([rps[rps<=rp][-1], rps[rps>=rp][0]])\n",
    "    for rps0 in zip(*rps_lst):\n",
    "        scens.append(list(rps0))\n",
    "# include univariate\n",
    "rps0 = np.zeros(4, dtype=int)\n",
    "for i in range(4):\n",
    "    for rp in rps[1:]:\n",
    "        _rps = rps0.tolist()\n",
    "        _rps[i] = rp\n",
    "        scens.append(_rps)\n",
    "# include full dependence\n",
    "for rp in rps:\n",
    "    scens.append(np.full(4, rp, dtype=int).tolist())\n",
    "##\n",
    "df_scen = pd.DataFrame(data=np.vstack(scens), columns=cols).value_counts().rename('count').reset_index()\n",
    "df_scen['scen'] = [\n",
    "    f\"qb{qb_rp:03.0f}_qp{qp_rp:03.0f}_h{h_rp:03.0f}_p{p_rp:03.0f}\" \n",
    "    for i, (qb_rp, qp_rp, p_rp, h_rp) in df_scen[cols].iterrows()\n",
    "]\n",
    "df_scen\n",
    "print(df_scen.index.size)\n",
    "# df_scen.to_csv(r'../../4_results/sim_SCEN.csv')\n",
    "df_scen.sort_values('count', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b732a555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_scen.set_index('scen').loc\n",
    "['qb005_qp050_h500_p002'] in df_scen['scen'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63049e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a0 = ds_flood_rp.sel(ds0_rp, method='nearest')\n",
    "a0.where(a0['index']<0, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faee3c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get nearest scen for each sample\n",
    "ds0_rp = df_sample[index_cols + ['year']].to_xarray().set_coords('year')\n",
    "df_sample['index'] = ds_flood_rp['index'].sel(ds0_rp, method='nearest')\n",
    "# df_sample['scen'] = df_flood.reset_index().loc[df_sample['index'].values, 'scen'].values\n",
    "\n",
    "df_sample[df_sample['index']<0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159ce9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "_scen_unique, idxs_reverse = np.unique(df_sample['scen'], return_inverse=True)\n",
    "print(_scen_unique.size)\n",
    "flddph = da1.sel(scen=_scen_unique).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd8ff96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask import delayed\n",
    "import dask\n",
    "rps=np.array([2, 5, 10, 50, 100, 500])\n",
    "cdf = np.arange(5000)/5000\n",
    "bins = df_sample[['year']].values.flatten()\n",
    "def _fldsrt(fld0, idxs_reverse=idxs_reverse, rp=rps, bins=bins, cdf=cdf):\n",
    "    if np.all(fld0==0):\n",
    "        return 0.0\n",
    "    fld0 = fld0[idxs_reverse]\n",
    "    df0 = pd.DataFrame(data={'fld0': fld0, 'bin':bins})\n",
    "    fld_max = df0[['fld0', 'bin']].groupby('bin').max().values.flatten()\n",
    "    fld_sorted = np.sort(fld_max)\n",
    "    return np.interp(1-1/rp, cdf, fld_sorted)\n",
    "data=np.apply_along_axis(_fldsrt, flddph.get_axis_num(\"scen\"), flddph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c5ae0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "da_cmpnd = xr.DataArray(\n",
    "    data=data, \n",
    "    dims=('rp', 'y', 'x'),\n",
    "    coords={'rp':rps, **da1.raster.coords}\n",
    ")\n",
    "da_cmpnd.raster.set_nodata(da1.raster.nodata)\n",
    "da_cmpnd.raster.set_crs(da1.raster.crs)\n",
    "for rp0 in rps:\n",
    "    da_cmpnd.sel(rp=rp0).raster.to_raster(join(r'../03_models/hmax', f'compound_{rp0:03d}.tif'), compress='lzw')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3cad74",
   "metadata": {},
   "source": [
    "## flood impact assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81ba072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# log-linear integration from FIAT\n",
    "def get_logcov(rp_lst):\n",
    "    f_lst = [1 / i for i in rp_lst]\n",
    "    lf = [np.log(1 / i) for i in rp_lst]\n",
    "    c = [(1 / (lf[i] - lf[i+1])) for i in range(len(rp_lst[:-1]))]\n",
    "    G = [(f_lst[i] * lf[i] - f_lst[i]) for i in range(len(rp_lst))]\n",
    "    a = [((1 + c[i] * lf[i+1]) * (f_lst[i] - f_lst[i+1]) + c[i] * (G[i+1] - G[i])) for i in range(len(rp_lst[:-1]))]\n",
    "    b = [(c[i] * (G[i] - G[i+1] + lf[i+1] * (f_lst[i+1] - f_lst[i]))) for i in range(len(rp_lst[:-1]))]\n",
    "    if len(rp_lst) == 1:\n",
    "        cov_lst = f_lst\n",
    "    else:\n",
    "        cov_lst = [b[0] if i == 0 else f_lst[i] + a[i-1] if i == len(rp_lst) - 1 else a[i-1] + b[i] for i in range(len(rp_lst))]\n",
    "\n",
    "    return cov_lst\n",
    "\n",
    "def loglin_trapz(exp, rp_lst=None):\n",
    "    if rp_lst is None:\n",
    "        rp_lst = (1/(1-np.arange(exp.size)/exp.size)).tolist()\n",
    "    exp_lst = np.sort(exp).tolist()\n",
    "    cov_lst = get_logcov(rp_lst)\n",
    "    risk = (np.asarray(cov_lst) * np.asarray(exp_lst)).sum()\n",
    "    return risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cd7d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "hmin = 0.15\n",
    "\n",
    "# small edit to set threshold for impact at 15cm\n",
    "df = pd.read_csv(join(mdir, '98_fiat', 'susceptibility', 'AF000.csv'), index_col=0)\n",
    "df.columns = ['factor']\n",
    "df.index.name = 'depth'\n",
    "df[df.index<hmin] = 0\n",
    "ds_exp = hydromt.open_mfraster(join(mdir, '98_fiat', 'exposure', '*.tif')).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f35ee45",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = dict()\n",
    "for postfix in ['_magInd_timeObs', '_magInd_timeInd', '_magObs_timeObs']:\n",
    "    fn = os.path.join('../02_data', f'modelled_dataset_5000_years{postfix}_rp.csv')\n",
    "    samples[postfix[1:]] = pd.read_csv(fn, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e38fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def flood_damage(da_flddph, da_exposure, df_susceptibility, **kwargs):\n",
    "    nodata = da_exposure.attrs['_FillValue']\n",
    "    da0 = df_susceptibility.to_xarray()['factor'].chunk({'depth':-1})\n",
    "    factor = np.minimum(1, da0.interp(depth=da_flddph, **kwargs))\n",
    "    damage = (factor * da_exposure).fillna(nodata).astype(np.float32)\n",
    "    damage.name = da_exposure.name\n",
    "    damage.attrs.update(**da_exposure.attrs)\n",
    "    return damage\n",
    "\n",
    "def flood_exposed(da_flddph, da_exposure, min_flddph=hmin):\n",
    "    exposed = xr.where(da_flddph>min_flddph,da_exposure,0.0).astype(np.float32)\n",
    "    exposed.attrs.update(**da_exposure.attrs)\n",
    "    exposed.name = da_exposure.name\n",
    "    return exposed\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76bc29a",
   "metadata": {},
   "source": [
    "## base scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbb2f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for single scen\n",
    "# scen='qb100_qp100_h000_p000'\n",
    "# da_dam = flood_damage(da1.sel(scen=scen), da_exp0, df).compute()\n",
    "# da_ppl = flood_exposed(da1.sel(scen=scen), da_exp1, 0.15).compute()\n",
    "# print(da_dam.sum().item()/1e6)\n",
    "# print(da_ppl.sum().item()/1e3)\n",
    "\n",
    "# root = glob.glob(join(mdir, f'*{scen}'))[0]\n",
    "# print(root)\n",
    "# da1.sel(scen=scen).raster.to_raster(join(root, 'gis', 'hmax_bias.tif'), compress='lzw')\n",
    "# fn_out = join(root, 'gis', f'{exposure}.tif') \n",
    "# da_dam.raster.to_raster(fn_out, compress='lzw')\n",
    "# fn_out = join(root, 'gis', f'population_count.tif') \n",
    "# da_ppl.raster.to_raster(fn_out, compress='lzw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb75a2ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rps = np.array([0,2,5,10,50,100,500], dtype=int)\n",
    "fns = os.path.join('../03_models', 'hmax', f'*.tif') \n",
    "ds = hydromt.open_mfraster(fns)\n",
    "da_qb = xr.concat([ds[f'qb{rp:03d}_qp000_h000_p000'] for rp in rps], dim='rps').rename('qb')\n",
    "da_qp = xr.concat([ds[f'qb000_qp{rp:03d}_h000_p000'] for rp in rps], dim='rps').rename('qp')\n",
    "da_h = xr.concat([ds[f'qb000_qp000_h{rp:03d}_p000'] for rp in rps], dim='rps').rename('h')\n",
    "da_p = xr.concat([ds[f'qb000_qp000_h000_p{rp:03d}'] for rp in rps], dim='rps').rename('p')\n",
    "da_c = xr.concat([da_p.isel(rps=0)] + [ds[f'compound_{rp:03d}'] for rp in rps[1:]], dim='rps').rename('cp')\n",
    "ds = xr.merge([da_qb, da_qp, da_h, da_p, da_c])\n",
    "rps[0] = 1\n",
    "ds['rps'] = xr.IndexVariable('rps', rps)\n",
    "ds['cof'] = xr.IndexVariable('rps', get_logcov(rps))\n",
    "ds = ds.set_coords('cof').load()\n",
    "ds_impact = xr.Dataset()\n",
    "for dvar in ds.data_vars:\n",
    "    ds_impact[f'{dvar}_dam'] = flood_damage(ds[dvar], ds_exp['buildings_value'], df)\n",
    "    ds_impact[f'{dvar}_pop'] = flood_exposed(ds[dvar], ds_exp['population_count'], hmin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9769a10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "da_dam = flood_damage(da1, ds_exp['buildings_value'], df)\n",
    "da_dam.attrs.update(unit='USD')\n",
    "da_ppl = flood_exposed(da1, ds_exp['population_count'], hmin)\n",
    "da_dam.attrs.update(unit='people')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49dae17",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = os.path.join('../02_data', f'modelled_dataset_5000_years_magObs_timeObs_scenCount.csv')\n",
    "df_scen = pd.read_csv(fn, index_col=0)\n",
    "ds_impact = xr.merge([da_dam, da_ppl])#.sel(scen=df_scen['scen'].values)\n",
    "ds_impact.chunk({'x':-1, 'y':-1, 'scen':10}).to_zarr(join(mdir, 'flood_impact.zarr'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd76cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "da_dam_agg = da_dam.sum(('x', 'y')).round(2)\n",
    "da_ppl_agg = da_ppl.sum(('x', 'y')).round(2)\n",
    "df_out = da_dam_agg.reset_coords().drop_vars(['band','spatial_ref']).to_dataframe()\n",
    "df_out[da_ppl.name] = da_ppl_agg.reset_coords(drop=True).to_dataframe()\n",
    "df_out.to_csv(join(mdir, '98_fiat', 'flood_impact_base.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c142260",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_cols = ['h_rp','p_rp','qb_rp','qp_rp']\n",
    "drop_cols = ['scen']#, 'h','p','qb','qp']\n",
    "exp_cols = ['buildings_value', 'population_count']\n",
    "\n",
    "# read impacts and convert to 4D xarray\n",
    "df_flood = pd.read_csv(join(mdir, '98_fiat', 'flood_impact_base.csv'), index_col=0)\n",
    "df_flood[index_cols] = np.maximum(1, df_flood[index_cols])\n",
    "df_flood = df_flood.reset_index()\n",
    "ds_flood_rp = df_flood.reset_index().drop(columns=index_cols+drop_cols).set_index(\n",
    "    pd.MultiIndex.from_frame(df_flood[index_cols])\n",
    ").to_xarray()\n",
    "\n",
    "# interpolate damages for each sample\n",
    "for key in samples:\n",
    "    ds0_rp = samples[key][index_cols + ['year']].to_xarray().set_coords('year')\n",
    "    ds0_flood = ds_flood_rp[cols].interp(ds0_rp, method='linear')\n",
    "    for col in cols:\n",
    "        samples[key][f'{col}_base'] = ds0_flood[col].to_series()#.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ac7867",
   "metadata": {},
   "source": [
    "## dikes rp 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79f303f",
   "metadata": {},
   "outputs": [],
   "source": [
    "qrp, hrp = 10, 10\n",
    "\n",
    "# interpolate damages for each sample\n",
    "for key in samples:\n",
    "    df0 = samples[key][index_cols + ['year']].copy()\n",
    "    df0['h_rp'] = np.where(df0['h_rp']<hrp,1,df0['h_rp'])\n",
    "    df0['qb_rp'] = np.where(df0['qb_rp']<hrp,1,df0['qb_rp'])\n",
    "    df0['qp_rp'] = np.where(df0['qp_rp']<hrp,1,df0['qp_rp'])\n",
    "    ds0_rp = df0.to_xarray().set_coords('year')\n",
    "    ds0_flood = ds_flood_rp[cols].interp(ds0_rp, method='linear')\n",
    "    for col in cols:\n",
    "        samples[key][f'{col}_dikes'] = ds0_flood[col].to_series()#.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eef9a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kort door de bocht!! \n",
    "ds_impact[f'cp_dam_dikes'] = ds_impact[f'cp_dam'].sel(rps=[10, 50, 100, 500])\n",
    "ds_impact[f'cp_pop_dikes'] = ds_impact[f'cp_pop'].sel(rps=[10, 50, 100, 500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0333db0a",
   "metadata": {},
   "source": [
    "## zoning rp 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9007ff0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct exposure with 5 year flood zone\n",
    "qrp, hrp = 5, 5\n",
    "da0 = da1.sel(scen=[f'qb{qrp:03d}_qp000_h000_p000', f'qb000_qp{qrp:03d}_h000_p000', f'qb000_qp000_h{hrp:03d}_p000']).max('scen').squeeze(drop=True)\n",
    "fld_bin = da0 > hmin\n",
    "ds_exp_zone = ds_exp.where(~fld_bin, 0)\n",
    "\n",
    "(ds_exp - ds_exp_zone).sum().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6104c90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "da_dam = flood_damage(da1, ds_exp_zone['buildings_value'], df)\n",
    "da_dam_agg = da_dam.sum(('x', 'y')).round(2)\n",
    "da_ppl = flood_exposed(da1, ds_exp_zone['population_count'], hmin)\n",
    "da_ppl_agg = da_ppl.sum(('x', 'y')).round(2)\n",
    "\n",
    "df_out = da_dam_agg.reset_coords().drop_vars(['band','spatial_ref']).to_dataframe()\n",
    "df_out[da_ppl.name] = da_ppl_agg.reset_coords(drop=True).to_dataframe()\n",
    "df_out.to_csv(join(mdir, '98_fiat', 'flood_impact_zoning.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1746225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read impacts and convert to 4D xarray\n",
    "df_flood = pd.read_csv(join(mdir, '98_fiat', 'flood_impact_zoning.csv'), index_col=0)\n",
    "df_flood[index_cols] = np.maximum(1, df_flood[index_cols])\n",
    "df_flood = df_flood.reset_index()\n",
    "ds_flood_rp = df_flood.reset_index().drop(columns=index_cols+drop_cols).set_index(\n",
    "    pd.MultiIndex.from_frame(df_flood[index_cols])\n",
    ").to_xarray()\n",
    "\n",
    "# interpolate damages for each sample\n",
    "for key in samples:\n",
    "    ds0_rp = samples[key][index_cols + ['year']].to_xarray().set_coords('year')\n",
    "    ds0_flood = ds_flood_rp[cols].interp(ds0_rp, method='linear')\n",
    "    for col in cols:\n",
    "        samples[key][f'{col}_zoning'] = ds0_flood[col].to_series()#.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87a3fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_impact[f'cp_dam_zoning'] = flood_damage(ds['cp'], ds_exp_zone['buildings_value'], df)\n",
    "ds_impact[f'cp_pop_zoning'] = flood_exposed(ds['cp'], ds_exp_zone['population_count'], hmin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1656fec5",
   "metadata": {},
   "source": [
    "## dry proofing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017e4f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify depth-damage curve\n",
    "hmin_dry = 0.5\n",
    "df_dry = df.copy()\n",
    "df_dry[df_dry.index<hmin_dry] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fc1cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "da_dam = flood_damage(da1, ds_exp['buildings_value'], df_dry)\n",
    "da_dam_agg = da_dam.sum(('x', 'y')).round(2)\n",
    "da_ppl = flood_exposed(da1, ds_exp['population_count'], hmin_dry)\n",
    "da_ppl_agg = da_ppl.sum(('x', 'y')).round(2)\n",
    "\n",
    "df_out = da_dam_agg.reset_coords().drop_vars(['band','spatial_ref']).to_dataframe()\n",
    "df_out[da_ppl.name] = da_ppl_agg.reset_coords(drop=True).to_dataframe()\n",
    "df_out.to_csv(join(mdir, '98_fiat', 'flood_impact_dryproofing.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e4606f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read impacts and convert to 4D xarray\n",
    "df_flood = pd.read_csv(join(mdir, '98_fiat', 'flood_impact_dryproofing.csv'), index_col=0)\n",
    "df_flood[index_cols] = np.maximum(1, df_flood[index_cols])\n",
    "df_flood = df_flood.reset_index()\n",
    "ds_flood_rp = df_flood.reset_index().drop(columns=index_cols+drop_cols).set_index(\n",
    "    pd.MultiIndex.from_frame(df_flood[index_cols])\n",
    ").to_xarray()\n",
    "\n",
    "# interpolate damages for each sample\n",
    "for key in samples:\n",
    "    ds0_rp = samples[key][index_cols + ['year']].to_xarray().set_coords('year')\n",
    "    ds0_flood = ds_flood_rp[cols].interp(ds0_rp, method='linear')\n",
    "    for col in cols:\n",
    "        samples[key][f'{col}_dryproofing'] = ds0_flood[col].to_series()#.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf03be04",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in samples:\n",
    "    df_sample = samples[key]\n",
    "    fn = os.path.join('../02_data', f'modelled_dataset_5000_years_{key}_rp_impact.csv')\n",
    "    df_sample.to_csv(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91067a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_impact[f'cp_dam_dryproofing'] = flood_damage(ds['cp'], ds_exp['buildings_value'], df_dry)\n",
    "ds_impact[f'cp_pop_dryproofing'] = flood_exposed(ds['cp'], ds_exp['population_count'], hmin_dry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfc4d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # combine outputs\n",
    "# cols = ['buildings_value', 'population_count']\n",
    "# drop_cols = ['h_rp','p_rp','qb_rp','qp_rp']\n",
    "# df_lst = []\n",
    "# for postfix in ['base', 'dikes', 'dryproofing', 'zoning']:\n",
    "#     dfi = pd.read_csv(join(mdir, '98_fiat', f'flood_impact_{postfix}.csv'), index_col=0)\n",
    "#     dfi = dfi.rename(columns={c: f'{c}_{postfix}' for c in cols})\n",
    "#     if postfix != 'base':\n",
    "#         dfi = dfi.drop(columns=drop_cols)\n",
    "#     df_lst.append(dfi)\n",
    "# df_out = pd.concat(df_lst, axis=1)\n",
    "# df_out.to_csv(join(mdir, '98_fiat', 'flood_impact.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d522ecf",
   "metadata": {},
   "source": [
    "## risk calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0101af",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = dict()\n",
    "index_cols = ['h_rp','p_rp','qb_rp','qp_rp']\n",
    "exp_cols = ['buildings_value','population_count']\n",
    "for postfix in ['_magInd_timeObs', '_magInd_timeInd', '_magObs_timeObs']:\n",
    "    fn = os.path.join('../02_data', f'modelled_dataset_5000_years{postfix}_rp_impact.csv')\n",
    "    samples[postfix[1:]] = pd.read_csv(fn, index_col=0)\n",
    "    \n",
    "df_flood = pd.read_csv(join(mdir, '98_fiat', 'flood_impact_base.csv'), index_col=0)\n",
    "df_flood = df_flood.rename(columns={c: f'{c}_base' for c in exp_cols})\n",
    "df_flood[index_cols] = np.maximum(1, df_flood[index_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52d551f",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_cols0 = df_flood.columns[-2:].values.tolist()\n",
    "risk = dict()\n",
    "dfs = dict()\n",
    "for col in index_cols:\n",
    "    zero_cols = [c for c in index_cols if c != col]\n",
    "    df0 = df_flood[np.all(df_flood[zero_cols]==1, axis=1)].sort_values(col)\n",
    "    risk[col.split('_')[0]] = pd.Series({c: loglin_trapz(df0[c], df0[col]) for c in exp_cols0})\n",
    "    dfs[col.split('_')[0]] = df0[[col] + exp_cols0].rename(columns={col: 'rp'})\n",
    "sum([df[exp_cols0[0]] for _, df in risk.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2ccfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = df_flood[np.all(np.diff(df_flood[index_cols], axis=1)==0, axis=1)].sort_values(col)\n",
    "dfs['fullDep'] = df0[['h_rp'] + exp_cols0].rename(columns={'h_rp': 'rp'})\n",
    "risk['fullDep'] = pd.Series({c: loglin_trapz(df0[c], df0['h_rp']) for c in exp_cols0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5a1841",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in samples:\n",
    "    df = samples[key]\n",
    "    \n",
    "    exp_cols = [c for c in df.columns if (c.startswith('buildings') or c.startswith('pop'))]\n",
    "    print(key, df.index.size)\n",
    "    \n",
    "#     df0 = df[['year'] + exp_cols].groupby('year').max().reset_index()\n",
    "#     dfs[f'{key}'] = df0\n",
    "#     risk[f'{key}'] = pd.Series({c: loglin_trapz(df0[c]) for c in exp_cols})\n",
    "\n",
    "    df0 = df[['year']+exp_cols].groupby('year').sum().sort_values(exp_cols[0]).reset_index()\n",
    "    dfs[f'{key}_sum'] = df0\n",
    "    risk[f'{key}_sum'] = pd.Series({c: loglin_trapz(df0[c]) for c in exp_cols})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5795db89",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_risk = pd.DataFrame(risk).T\n",
    "df_risk['buildings_value_base']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758e501c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_risk[['buildings_value_base', 'population_count_base']].rename(labs).to_csv('risk_base.csv')\n",
    "df_adapt = df_risk.loc[['magObs_timeObs_sum'], :].rename(labs)\n",
    "df_adapt.to_csv('risk_adapt.csv')\n",
    "df_adapt.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e0bdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "labs = {\n",
    "    'fullDep': 'full dependence',\n",
    "    'magObs_timeObs': 'obs. dependence',\n",
    "    'magInd_timeInd': 'full indep.',\n",
    "    'magInd_timeObs': 'independence',\n",
    "    'magObs_timeObs_sum': 'obs. dependence',\n",
    "    'magInd_timeInd_sum': 'full indep.',\n",
    "    'magInd_timeObs_sum': 'independence',\n",
    "    'p': 'pluvial', \n",
    "    'qp': 'fluvial Pungwe', \n",
    "    'qb': 'fluvial Buzi', \n",
    "    'h': 'coastal', \n",
    "}\n",
    "rps= [1,2,5,10,50,100,500]\n",
    "ylabs = {\n",
    "    'buildings_value': 'annual building damage [mil. USD]',\n",
    "    'population_count': 'annual people exposed [-]'\n",
    "}\n",
    "legend_titles = {\n",
    "    'buildings_value': '(EAD [mil. USD])',\n",
    "    'population_count': '(EAAP [x 1000])'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f2e82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "exp0, factor = 'population_count', 1e3\n",
    "exp0, factor = 'buildings_value', 1e6\n",
    "fig, ax = plt.subplots(1,1, figsize=(6,4))\n",
    "for key in ['qp', 'qb', 'h', 'p'][-4:]:\n",
    "    risk0 = risk[key][f'{exp0}_base']/factor\n",
    "    lab = f'{labs[key]} ({risk0:.2f})'\n",
    "#     lab = f'{labs[key]}'\n",
    "    ls = '-' if 'dep' in lab else '--'\n",
    "    zorder = 1 if 'dep' in lab else 2\n",
    "    df0 = dfs[key].copy()\n",
    "    if 'rp' not in df0:\n",
    "        df0 = df0.sort_values(f'{exp0}_base')\n",
    "        df0['rp'] = (1/(1-np.arange(df0.index.size)/df0.index.size))\n",
    "    (df0.set_index('rp')[f'{exp0}_base']/factor).plot(ls=ls, lw=1.5, ax=ax, label=lab, zorder=zorder)\n",
    "ax.set_xlim([1, 500])\n",
    "if 'build' in exp0:\n",
    "    ax.set_ylim([0, 150])\n",
    "else:\n",
    "    ax.set_ylim([0, 120])\n",
    "ax.set_xscale('log')\n",
    "ax.set_xticks(rps)\n",
    "ax.set_xticklabels(rps)\n",
    "ax.set_ylabel(f'{ylabs[exp0]}')\n",
    "ax.set_xlabel('return period [year]')\n",
    "# ax.legend(title='driver scenario', loc='upper left')\n",
    "ax.legend(title=f'driver scenario\\n {legend_titles[exp0]}', loc='upper left')\n",
    "ax.set_title('Univariate flood risk - base scenario')\n",
    "plt.savefig(os.path.join('../FIGURES', f'risk_curve_{exp0}_single.png'), dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0d509f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(6,4))\n",
    "for key in ['magObs_timeObs_sum', 'magInd_timeObs_sum', 'fullDep']:\n",
    "    risk0 = risk[key][f'{exp0}_base']/factor\n",
    "    lab = f'{labs[key]} ({risk0:.2f})'\n",
    "    ls = '--' if 'full' in key else '-'\n",
    "    lw = 3 if key == 'magObs_timeObs_sum' else 1.5\n",
    "    df0 = dfs[key].copy()\n",
    "    if 'rp' not in df0:\n",
    "        df0 = df0.sort_values(f'{exp0}_base')\n",
    "        df0['rp'] = (1/(1-np.arange(df0.index.size)/df0.index.size))\n",
    "    (df0.set_index('rp')[f'{exp0}_base']/factor).plot(ls=ls, lw=lw, ax=ax, label=lab)\n",
    "ax.set_xlim([1, 500])\n",
    "if 'build' in exp0:\n",
    "    ax.set_ylim([0, 300])\n",
    "else:\n",
    "    ax.set_ylim([0, 150])\n",
    "ax.set_xscale('log')\n",
    "ax.set_xticks(rps)\n",
    "ax.set_xticklabels(rps)\n",
    "ax.set_ylabel(f'{ylabs[exp0]}')\n",
    "ax.set_xlabel('return period [year]')\n",
    "legend = ax.legend(title=f'compound scenario\\n {legend_titles[exp0]}', loc='upper left')#, fontsize='large')\n",
    "ax.set_title('Compound flood risk - base scenario')\n",
    "plt.savefig(f'../FIGURES/risk_curve_{exp0}_compound.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774dc862",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(6, 4))\n",
    "key='magObs_timeObs_sum'\n",
    "for pf in ['base', 'dikes', 'dryproofing', 'zoning']:\n",
    "    exp = f'{exp0}_{pf}'\n",
    "    risk0 = risk[key][exp]/factor\n",
    "    pf = 'dry-proofing' if pf == 'dryproofing' else pf\n",
    "    lab = f'{pf} ({risk0:.2f})'\n",
    "#     ls = '-' if pf == 'base' else ':'\n",
    "    lw = 3 if pf == 'base' else 1.5\n",
    "    df0 = dfs[key].copy()\n",
    "    if 'rp' not in df0:\n",
    "        df0 = df0.sort_values(exp)\n",
    "        df0['rp'] = (1/(1-np.arange(df0.index.size)/df0.index.size))\n",
    "    (df0.set_index('rp')[exp]/factor).plot(ls='-', lw=lw, ax=ax, label=lab)\n",
    "ax.set_xlim([1, 500])\n",
    "if 'build' in exp0:\n",
    "    ax.set_ylim([0, 200])\n",
    "else:\n",
    "    ax.set_ylim([0, 140])\n",
    "ax.set_xscale('log')\n",
    "ax.set_xticks(rps)\n",
    "ax.set_xticklabels(rps)\n",
    "ax.set_ylabel(f'cumulative {ylabs[exp0]}')\n",
    "ax.set_xlabel('return period [year]')\n",
    "ax.legend(title=f'adaptation scenario\\n {legend_titles[exp0]}', loc='upper left')\n",
    "ax.set_title(f'Compound flood risk - {legend_titles[exp0][1:5]}')\n",
    "plt.savefig(f'../FIGURES/risk_curve_{exp0}_adaptation.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcf50bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "_df = samples['magObs_timeObs']\n",
    "np.sum(np.logical_and.reduce((_df['qb_rp']>10, _df['qp_rp']>10, _df['h_rp']>10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a35b0d",
   "metadata": {},
   "source": [
    "### RISK MAPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4990daf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_risk = (ds_impact * ds['cof']).sum('rps').compute()\n",
    "\n",
    "# bias correct to match totals\n",
    "df00 = df_adapt.T\n",
    "df00 = df00.rename({n:n.replace('buildings_value', 'cp_dam').replace('_base', '') for n in df00.index})\n",
    "df00 = df00.rename({n:n.replace('population_count', 'cp_pop').replace('_base', '') for n in df00.index})\n",
    "\n",
    "for dvar in df00.index:\n",
    "    ds_risk[dvar] = ds_risk[dvar]/ds_risk[dvar].sum().item()*df00.loc[dvar].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9527da60",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_risk = ds_risk.raster.reproject_like(mod0.staticmaps, 'nearest')\n",
    "for dvar in ds_risk.data_vars:\n",
    "    ds_risk[dvar] = ds_risk[dvar].where(mod0.staticmaps['msk']>0, -9999)\n",
    "    ds_risk[dvar].raster.set_nodata(-9999)\n",
    "ds_risk.raster.set_crs(mod0.crs)\n",
    "dir_out = os.path.join('../03_models', 'risk')\n",
    "ds_risk.raster.to_mapstack(dir_out, compress='lzw')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
