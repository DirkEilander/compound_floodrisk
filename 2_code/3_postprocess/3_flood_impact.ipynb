{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045de6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hydromt_sfincs import SfincsModel, utils\n",
    "import numpy as np\n",
    "from os.path import join\n",
    "import hydromt\n",
    "import glob\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94124cce",
   "metadata": {},
   "source": [
    "## read flood and model data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5c9431",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdir = r\"../../3_models\"\n",
    "rdir = r\"../../4_results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2d7240",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_cols = ['h_rp','p_rp','qb_rp','qp_rp']\n",
    "ds = xr.open_zarr(join(rdir, 'hmax.zarr'))\n",
    "da1 = ds['hmax']#.isel(scen=slice(0,100)).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8bbbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_sample = pd.read_csv(join(rdir, r'sim_EVENTS_rp.csv'), index_col=0).rename(columns={'h_tsw_rp': 'h_rp', 'h_tsw': 'h'})\n",
    "\n",
    "\n",
    "df_flood = da1.reset_coords().drop_vars(['spatial_ref', 'x', 'y', 'hmax']).to_dataframe()\n",
    "# df_flood[index_cols] = np.maximum(1, df_flood[index_cols])\n",
    "ds_flood_rp = df_flood.reset_index().drop(columns=index_cols).set_index(\n",
    "    pd.MultiIndex.from_frame(df_flood[index_cols])\n",
    ").to_xarray()\n",
    "ds_flood_rp['index'] = ds_flood_rp['index'].fillna(-1).astype(int)\n",
    "ds_flood_rp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3cad74",
   "metadata": {},
   "source": [
    "## flood impact assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81ba072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# log-linear integration from FIAT\n",
    "def get_logcov(rp_lst):\n",
    "    f_lst = [1 / i for i in rp_lst]\n",
    "    lf = [np.log(1 / i) for i in rp_lst]\n",
    "    c = [(1 / (lf[i] - lf[i+1])) for i in range(len(rp_lst[:-1]))]\n",
    "    G = [(f_lst[i] * lf[i] - f_lst[i]) for i in range(len(rp_lst))]\n",
    "    a = [((1 + c[i] * lf[i+1]) * (f_lst[i] - f_lst[i+1]) + c[i] * (G[i+1] - G[i])) for i in range(len(rp_lst[:-1]))]\n",
    "    b = [(c[i] * (G[i] - G[i+1] + lf[i+1] * (f_lst[i+1] - f_lst[i]))) for i in range(len(rp_lst[:-1]))]\n",
    "    if len(rp_lst) == 1:\n",
    "        cov_lst = f_lst\n",
    "    else:\n",
    "        cov_lst = [b[0] if i == 0 else f_lst[i] + a[i-1] if i == len(rp_lst) - 1 else a[i-1] + b[i] for i in range(len(rp_lst))]\n",
    "\n",
    "    return cov_lst\n",
    "\n",
    "def loglin_trapz(exp, rp_lst=None):\n",
    "    if rp_lst is None:\n",
    "        rp_lst = (1/(1-np.arange(exp.size)/exp.size)).tolist()\n",
    "    exp_lst = np.sort(exp).tolist()\n",
    "    cov_lst = get_logcov(rp_lst)\n",
    "    risk = (np.asarray(cov_lst) * np.asarray(exp_lst)).sum()\n",
    "    return risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cd7d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "hmin = 0.15\n",
    "\n",
    "# small edit to set threshold for impact at 15cm\n",
    "df = pd.read_csv(join(mdir, 'fiat', 'susceptibility', 'AF000.csv'), index_col=0)\n",
    "df.columns = ['factor']\n",
    "df.index.name = 'depth'\n",
    "df[df.index<hmin] = 0\n",
    "ds_exp = hydromt.open_mfraster(join(mdir, 'fiat', 'exposure', '*.tif')).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e38fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def flood_damage(da_flddph, da_exposure, df_susceptibility, **kwargs):\n",
    "    nodata = da_exposure.attrs['_FillValue']\n",
    "    da0 = df_susceptibility.to_xarray()['factor'].chunk({'depth':-1})\n",
    "    factor = np.minimum(1, da0.interp(depth=da_flddph, **kwargs))\n",
    "    damage = (factor * da_exposure).fillna(nodata).astype(np.float32)\n",
    "    damage.name = da_exposure.name\n",
    "    damage.attrs.update(**da_exposure.attrs)\n",
    "    return damage\n",
    "\n",
    "def flood_exposed(da_flddph, da_exposure, min_flddph=hmin):\n",
    "    exposed = xr.where(da_flddph>min_flddph,da_exposure,0.0).astype(np.float32)\n",
    "    exposed.attrs.update(**da_exposure.attrs)\n",
    "    exposed.name = da_exposure.name\n",
    "    return exposed\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76bc29a",
   "metadata": {},
   "source": [
    "## base scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb75a2ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# flood impact univariate and full dep compound scenario\n",
    "rps = np.array([0, 2,5,10,25,50,100,250,500], dtype=int)\n",
    "\n",
    "da_qb = hydromt.open_mfraster(os.path.join(rdir, 'hmax', f'qb_rp*.tif'), concat=True, concat_dim='rps')\n",
    "da_qp = hydromt.open_mfraster(os.path.join(rdir, 'hmax', f'qp_rp*.tif'), concat=True, concat_dim='rps')\n",
    "da_h = hydromt.open_mfraster(os.path.join(rdir, 'hmax', f'h_rp*.tif'), concat=True, concat_dim='rps')\n",
    "da_p = hydromt.open_mfraster(os.path.join(rdir, 'hmax', f'p_rp*.tif'), concat=True, concat_dim='rps')\n",
    "da_c0 = hydromt.open_mfraster(os.path.join(rdir, 'hmax', f'compound_fulldep_rp*.tif'), concat=True, concat_dim='rps').rename({'compound': 'fulldep'})\n",
    "da_c1 = hydromt.open_mfraster(os.path.join(rdir, 'hmax', f'compound_indep_rp*.tif'), concat=True, concat_dim='rps').rename({'compound': 'indep'})\n",
    "ds = xr.merge([da_qb, da_qp, da_h, da_p, da_c0, da_c1]).raster.flipud()\n",
    "# rps[0] = 1\n",
    "# ds['rps'] = xr.IndexVariable('rps', rps)\n",
    "# ds['cof'] = xr.IndexVariable('rps', get_logcov(rps))\n",
    "# ds = ds.set_coords('cof').load()\n",
    "ds_impact = xr.Dataset()\n",
    "for dvar in ds.data_vars:\n",
    "    ds_impact[f'{dvar}_dam'] = flood_damage(ds[dvar], ds_exp['buildings_value'], df).compute()\n",
    "    ds_impact[f'{dvar}_pop'] = flood_exposed(ds[dvar], ds_exp['population_count'], hmin).compute()\n",
    "\n",
    "ds_impact_agg = ds_impact.sum(('x', 'y')).round(2)\n",
    "df_out = ds_impact_agg.reset_coords(drop=True).to_dataframe()\n",
    "df_out.to_csv(join(rdir, 'flood_impact_uni.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9769a10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flood impact all simulation events\n",
    "da_dam = flood_damage(da1, ds_exp['buildings_value'], df)\n",
    "da_dam.attrs.update(unit='USD')\n",
    "da_ppl = flood_exposed(da1, ds_exp['population_count'], hmin)\n",
    "da_dam.attrs.update(unit='people')\n",
    "ds_impact = xr.merge([da_dam, da_ppl]).persist()\n",
    "# ds_impact.chunk({'x':-1, 'y':-1, 'scen':10}).to_zarr(join(rdir, 'flood_impact.zarr'))\n",
    "\n",
    "ds_impact_agg = ds_impact.sum(('x', 'y')).round(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36e80bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out = ds_impact_agg.reset_coords().drop_vars(['spatial_ref','index']).to_dataframe()\n",
    "df_out.to_csv(join(rdir, 'flood_impact_base.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c142260",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_cols = ['h_rp','p_rp','qb_rp','qp_rp']\n",
    "drop_cols = ['scen']#, 'h','p','qb','qp']\n",
    "exp_cols = {'buildings_value': 'dam', 'population_count': 'pop'}\n",
    "\n",
    "# read impacts and convert to 4D xarray\n",
    "df_flood = pd.read_csv(join(rdir, 'flood_impact_base.csv'), index_col=0)\n",
    "# df_flood[index_cols] = np.maximum(1, df_flood[index_cols])\n",
    "df_flood = df_flood.reset_index()\n",
    "ds_flood_rp = df_flood.reset_index().drop(columns=index_cols+drop_cols).set_index(\n",
    "    pd.MultiIndex.from_frame(df_flood[index_cols])\n",
    ").to_xarray()#.fillna(0)  # fillna required to avoid nans as xarray looks at neighbors even for exact simulated locations\n",
    "\n",
    "# fill missing (temp! )\n",
    "from scipy.interpolate import griddata\n",
    "grid = np.meshgrid(*(ds_flood_rp[dim].values for dim in ds_flood_rp.dims))\n",
    "\n",
    "for exp in exp_cols:\n",
    "    data = ds_flood_rp[exp].values\n",
    "    mask = np.isnan(data)\n",
    "    # interpolate data at nodata cells only\n",
    "    data[mask] = griddata(\n",
    "        points=tuple([dim[~mask] for dim in grid]),\n",
    "        values=data[~mask],\n",
    "        xi=tuple([dim[mask] for dim in grid]),\n",
    "        method='nearest',\n",
    "    )\n",
    "    ds_flood_rp[exp].data = data\n",
    "\n",
    "# interpolate damages for each sample\n",
    "rm = {'h_tsw_rp': 'h_rp', 'h_tsw': 'h'}\n",
    "samples = dict(\n",
    "    indep0 = pd.read_csv(join(rdir, r'sim_EVENTS_indep_rp.csv'), index_col=0).rename(columns=rm),\n",
    "    indep1 = pd.read_csv(join(rdir, r'sim_EVENTS_indep1_rp.csv'), index_col=0).rename(columns=rm),\n",
    "    fulldep1 = pd.read_csv(join(rdir, r'sim_EVENTS_fulldep1_rp.csv'), index_col=0).rename(columns=rm),\n",
    "    obs = pd.read_csv(join(rdir, r'sim_EVENTS_rp.csv'), index_col=0).rename(columns=rm),\n",
    ")\n",
    "for key in samples:\n",
    "    df0  = samples[key].copy()\n",
    "    # df0 = df0[df0['year']<=5000]\n",
    "    # df0[index_cols] = np.maximum(1, df0[index_cols])\n",
    "    ds0_rp = df0[index_cols + ['year']].to_xarray().set_coords('year')\n",
    "    ds0_flood = ds_flood_rp[exp_cols.keys()].interp(ds0_rp, method='linear')\n",
    "    for col in exp_cols:\n",
    "        df0[exp_cols[col]] = ds0_flood[col].to_series()#.fillna(0)\n",
    "    samples[key] = df0 #samples[key]#.dropna(axis=0, how='any')\n",
    "\n",
    "samples['indep1']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d522ecf",
   "metadata": {},
   "source": [
    "## risk calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a9e4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flood0 = pd.read_csv(join(rdir, 'flood_impact_uni.csv'), index_col=0)\n",
    "df_flood0.loc[1,:] = 0\n",
    "df_flood0.sort_index(inplace=True)\n",
    "df_flood0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b50256",
   "metadata": {},
   "outputs": [],
   "source": [
    "risk = dict(pop=dict(), dam=dict())\n",
    "dfs = dict()\n",
    "\n",
    "# univariate and full dep\n",
    "for col in df_flood0.columns:\n",
    "    dvar, exp = col.split('_')\n",
    "    risk[exp][dvar] = loglin_trapz(df_flood0[col], df_flood0.index.values)\n",
    "    \n",
    "# obs dep\n",
    "for key in samples:\n",
    "    for exp in exp_cols.values():\n",
    "\n",
    "        df0 = samples[key][['year', exp]].groupby('year').max().reset_index()\n",
    "        risk[exp][f'{key}'] = loglin_trapz(df0[exp])\n",
    "\n",
    "        # df0 = samples[key][['year', exp]].groupby('year').sum().reset_index()\n",
    "        # risk[exp][f'{key}_sum'] = loglin_trapz(df0[exp])\n",
    "\n",
    "pd.DataFrame(risk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52d551f",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_cols0 = df_flood.columns[-2:].values.tolist()\n",
    "risk = dict()\n",
    "dfs = dict()\n",
    "for col in index_cols:\n",
    "    zero_cols = [c for c in index_cols if c != col]\n",
    "    df0 = df_flood[np.all(df_flood[zero_cols]==1, axis=1)].sort_values(col)\n",
    "    risk[col.split('_')[0]] = pd.Series({c: loglin_trapz(df0[c], df0[col]) for c in exp_cols0})\n",
    "    dfs[col.split('_')[0]] = df0[[col] + exp_cols0].rename(columns={col: 'rp'})\n",
    "sum([df[exp_cols0[0]] for _, df in risk.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53dd6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2ccfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = df_flood[np.all(np.diff(df_flood[index_cols], axis=1)==0, axis=1)].sort_values(col)\n",
    "dfs['fulldep'] = df0[['h_rp'] + exp_cols0].rename(columns={'h_rp': 'rp'})\n",
    "risk['fulldep'] = pd.Series({c: loglin_trapz(df0[c], df0['h_rp']) for c in exp_cols0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5a1841",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in samples:\n",
    "    df = samples[key]\n",
    "    \n",
    "    exp_cols = [c for c in df.columns if (c.startswith('buildings') or c.startswith('pop'))]\n",
    "    print(key, df.index.size)\n",
    "    \n",
    "    df0 = df[['year'] + exp_cols].groupby('year').max().reset_index()\n",
    "    dfs[f'{key}'] = df0\n",
    "    risk[f'{key}'] = pd.Series({c: loglin_trapz(df0[c]) for c in exp_cols})\n",
    "\n",
    "    # df0 = df[['year']+exp_cols].groupby('year').sum().sort_values(exp_cols[0]).reset_index()\n",
    "    # dfs[f'{key}_sum'] = df0\n",
    "    # risk[f'{key}_sum'] = pd.Series({c: loglin_trapz(df0[c]) for c in exp_cols})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c05901",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(risk).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758e501c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_risk[['buildings_value_base', 'population_count_base']].rename(labs).to_csv('risk_base.csv')\n",
    "df_adapt = df_risk.loc[['magObs_timeObs_sum'], :].rename(labs)\n",
    "df_adapt.to_csv('risk_adapt.csv')\n",
    "df_adapt.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e0bdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "labs = {\n",
    "    'fulldep': 'full dependence',\n",
    "    'obs': 'obs. dependence',\n",
    "    # 'magInd_timeInd': 'full indep.',\n",
    "    # 'magInd_timeObs': 'independence',\n",
    "    # 'magObs_timeObs_sum': 'obs. dependence',\n",
    "    # 'magInd_timeInd_sum': 'full indep.',\n",
    "    # 'magInd_timeObs_sum': 'independence',\n",
    "    'p': 'pluvial', \n",
    "    'qp': 'fluvial Pungwe', \n",
    "    'qb': 'fluvial Buzi', \n",
    "    'h': 'coastal', \n",
    "}\n",
    "rps= [1,2,5,10,25,50,100,250,500]\n",
    "ylabs = {\n",
    "    'buildings_value': 'annual building damage [mil. USD]',\n",
    "    'population_count': 'annual people exposed [-]'\n",
    "}\n",
    "legend_titles = {\n",
    "    'buildings_value': '(EAD [mil. USD])',\n",
    "    'population_count': '(EAAP [x 1000])'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f2e82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "exp0, factor = 'population_count', 1e3\n",
    "exp0, factor = 'buildings_value', 1e6\n",
    "fig, ax = plt.subplots(1,1, figsize=(6,4))\n",
    "for key in ['qp', 'qb', 'h', 'p'][-4:]:\n",
    "    risk0 = risk[key][f'{exp0}_base']/factor\n",
    "    lab = f'{labs[key]} ({risk0:.2f})'\n",
    "#     lab = f'{labs[key]}'\n",
    "    ls = '-' if 'dep' in lab else '--'\n",
    "    zorder = 1 if 'dep' in lab else 2\n",
    "    df0 = dfs[key].copy()\n",
    "    if 'rp' not in df0:\n",
    "        df0 = df0.sort_values(f'{exp0}_base')\n",
    "        df0['rp'] = (1/(1-np.arange(df0.index.size)/df0.index.size))\n",
    "    (df0.set_index('rp')[f'{exp0}_base']/factor).plot(ls=ls, lw=1.5, ax=ax, label=lab, zorder=zorder)\n",
    "ax.set_xlim([1, 500])\n",
    "if 'build' in exp0:\n",
    "    ax.set_ylim([0, 450])\n",
    "else:\n",
    "    ax.set_ylim([0, 120])\n",
    "ax.set_xscale('log')\n",
    "ax.set_xticks(rps)\n",
    "ax.set_xticklabels(rps)\n",
    "ax.set_ylabel(f'{ylabs[exp0]}')\n",
    "ax.set_xlabel('return period [year]')\n",
    "# ax.legend(title='driver scenario', loc='upper left')\n",
    "ax.legend(title=f'driver scenario\\n {legend_titles[exp0]}', loc='upper left')\n",
    "ax.set_title('Univariate flood risk - base scenario')\n",
    "# plt.savefig(os.path.join('../FIGURES', f'risk_curve_{exp0}_single.png'), dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0d509f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(6,4))\n",
    "for key in ['obs', 'fulldep']:\n",
    "    risk0 = risk[key][f'{exp0}_base']/factor\n",
    "    lab = f'{labs[key]} ({risk0:.2f})'\n",
    "    ls = '--' if 'full' in key else '-'\n",
    "    lw = 3 if key == 'obs' else 1.5\n",
    "    df0 = dfs[key].copy()\n",
    "    if 'rp' not in df0:\n",
    "        df0 = df0.sort_values(f'{exp0}_base')\n",
    "        df0['rp'] = (1/(1-np.arange(df0.index.size)/df0.index.size))\n",
    "    (df0.set_index('rp')[f'{exp0}_base']/factor).plot(ls=ls, lw=lw, ax=ax, label=lab)\n",
    "ax.set_xlim([1, 500])\n",
    "if 'build' in exp0:\n",
    "    ax.set_ylim([0, 750])\n",
    "else:\n",
    "    ax.set_ylim([0, 220])\n",
    "ax.set_xscale('log')\n",
    "ax.set_xticks(rps)\n",
    "ax.set_xticklabels(rps)\n",
    "ax.set_ylabel(f'{ylabs[exp0]}')\n",
    "ax.set_xlabel('return period [year]')\n",
    "legend = ax.legend(title=f'compound scenario\\n {legend_titles[exp0]}', loc='upper left')#, fontsize='large')\n",
    "ax.set_title('Compound flood risk - base scenario')\n",
    "# plt.savefig(f'../FIGURES/risk_curve_{exp0}_compound.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774dc862",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(6, 4))\n",
    "key='magObs_timeObs_sum'\n",
    "for pf in ['base', 'dikes', 'dryproofing', 'zoning']:\n",
    "    exp = f'{exp0}_{pf}'\n",
    "    risk0 = risk[key][exp]/factor\n",
    "    pf = 'dry-proofing' if pf == 'dryproofing' else pf\n",
    "    lab = f'{pf} ({risk0:.2f})'\n",
    "#     ls = '-' if pf == 'base' else ':'\n",
    "    lw = 3 if pf == 'base' else 1.5\n",
    "    df0 = dfs[key].copy()\n",
    "    if 'rp' not in df0:\n",
    "        df0 = df0.sort_values(exp)\n",
    "        df0['rp'] = (1/(1-np.arange(df0.index.size)/df0.index.size))\n",
    "    (df0.set_index('rp')[exp]/factor).plot(ls='-', lw=lw, ax=ax, label=lab)\n",
    "ax.set_xlim([1, 500])\n",
    "if 'build' in exp0:\n",
    "    ax.set_ylim([0, 200])\n",
    "else:\n",
    "    ax.set_ylim([0, 140])\n",
    "ax.set_xscale('log')\n",
    "ax.set_xticks(rps)\n",
    "ax.set_xticklabels(rps)\n",
    "ax.set_ylabel(f'cumulative {ylabs[exp0]}')\n",
    "ax.set_xlabel('return period [year]')\n",
    "ax.legend(title=f'adaptation scenario\\n {legend_titles[exp0]}', loc='upper left')\n",
    "ax.set_title(f'Compound flood risk - {legend_titles[exp0][1:5]}')\n",
    "plt.savefig(f'../FIGURES/risk_curve_{exp0}_adaptation.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcf50bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "_df = samples['magObs_timeObs']\n",
    "np.sum(np.logical_and.reduce((_df['qb_rp']>10, _df['qp_rp']>10, _df['h_rp']>10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a35b0d",
   "metadata": {},
   "source": [
    "### RISK MAPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4990daf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_risk = (ds_impact * ds['cof']).sum('rps').compute()\n",
    "\n",
    "# bias correct to match totals\n",
    "df00 = df_adapt.T\n",
    "df00 = df00.rename({n:n.replace('buildings_value', 'cp_dam').replace('_base', '') for n in df00.index})\n",
    "df00 = df00.rename({n:n.replace('population_count', 'cp_pop').replace('_base', '') for n in df00.index})\n",
    "\n",
    "for dvar in df00.index:\n",
    "    ds_risk[dvar] = ds_risk[dvar]/ds_risk[dvar].sum().item()*df00.loc[dvar].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9527da60",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_risk = ds_risk.raster.reproject_like(mod0.staticmaps, 'nearest')\n",
    "for dvar in ds_risk.data_vars:\n",
    "    ds_risk[dvar] = ds_risk[dvar].where(mod0.staticmaps['msk']>0, -9999)\n",
    "    ds_risk[dvar].raster.set_nodata(-9999)\n",
    "ds_risk.raster.set_crs(mod0.crs)\n",
    "dir_out = os.path.join('../03_models', 'risk')\n",
    "ds_risk.raster.to_mapstack(dir_out, compress='lzw')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "38648ff479392915c1a5d77722aa6edad827edf2098e3798b9c4282ba45e9fb7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 ('compound_risk')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
