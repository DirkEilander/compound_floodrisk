{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045de6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hydromt_sfincs import SfincsModel, utils\n",
    "import numpy as np\n",
    "from os.path import join\n",
    "import hydromt\n",
    "import glob\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "import xarray as xr\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94124cce",
   "metadata": {},
   "source": [
    "## read flood and model data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5c9431",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdir = r\"../../3_models\"\n",
    "rdir = r\"../../4_results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2d7240",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_cols = ['h_rp','p_rp','qb_rp','qp_rp']\n",
    "ds = xr.open_zarr(join(rdir, 'hmax.zarr'))\n",
    "da1 = ds['hmax']#.isel(scen=slice(0,100)).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8bbbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_sample = pd.read_csv(join(rdir, r'sim_EVENTS_rp.csv'), index_col=0).rename(columns={'h_tsw_rp': 'h_rp', 'h_tsw': 'h'})\n",
    "\n",
    "\n",
    "df_flood = da1.reset_coords().drop_vars(['count', 'spatial_ref', 'x', 'y', 'hmax']).to_dataframe()\n",
    "# df_flood[index_cols] = np.maximum(1, df_flood[index_cols])\n",
    "ds_flood_rp = df_flood.reset_index().drop(columns=index_cols).set_index(\n",
    "    pd.MultiIndex.from_frame(df_flood[index_cols])\n",
    ").to_xarray()\n",
    "ds_flood_rp['index'] = ds_flood_rp['index'].fillna(-1).astype(int)\n",
    "ds_flood_rp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3cad74",
   "metadata": {},
   "source": [
    "## flood impact assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81ba072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# log-linear integration from FIAT\n",
    "def get_logcov(rp_lst):\n",
    "    f_lst = [1 / i for i in rp_lst]\n",
    "    lf = [np.log(1 / i) for i in rp_lst]\n",
    "    c = [(1 / (lf[i] - lf[i+1])) for i in range(len(rp_lst[:-1]))]\n",
    "    G = [(f_lst[i] * lf[i] - f_lst[i]) for i in range(len(rp_lst))]\n",
    "    a = [((1 + c[i] * lf[i+1]) * (f_lst[i] - f_lst[i+1]) + c[i] * (G[i+1] - G[i])) for i in range(len(rp_lst[:-1]))]\n",
    "    b = [(c[i] * (G[i] - G[i+1] + lf[i+1] * (f_lst[i+1] - f_lst[i]))) for i in range(len(rp_lst[:-1]))]\n",
    "    if len(rp_lst) == 1:\n",
    "        cov_lst = f_lst\n",
    "    else:\n",
    "        cov_lst = [b[0] if i == 0 else f_lst[i] + a[i-1] if i == len(rp_lst) - 1 else a[i-1] + b[i] for i in range(len(rp_lst))]\n",
    "\n",
    "    return cov_lst\n",
    "\n",
    "def loglin_trapz(exp, rp_lst=None):\n",
    "    if rp_lst is None:\n",
    "        rp_lst = (1/(1-np.arange(exp.size)/exp.size)).tolist()\n",
    "    exp_lst = np.sort(exp).tolist()\n",
    "    cov_lst = get_logcov(rp_lst)\n",
    "    risk = (np.asarray(cov_lst) * np.asarray(exp_lst)).sum()\n",
    "    return risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cd7d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "hmin = 0.15\n",
    "\n",
    "# small edit to set threshold for impact at 15cm\n",
    "df = pd.read_csv(join(mdir, 'fiat', 'susceptibility', 'AF000.csv'), index_col=0)\n",
    "df.columns = ['factor']\n",
    "df.index.name = 'depth'\n",
    "df[df.index<hmin] = 0\n",
    "ds_exp = hydromt.open_mfraster(join(mdir, 'fiat', 'exposure', '*.tif')).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f35ee45",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = {'obs': df_sample[df_sample['year']<=500]}\n",
    "# for postfix in ['_magInd_timeObs', '_magInd_timeInd', '_magObs_timeObs']:\n",
    "#     fn = os.path.join('../02_data', f'modelled_dataset_5000_years{postfix}_rp.csv')\n",
    "#     samples[postfix[1:]] = pd.read_csv(fn, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e38fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def flood_damage(da_flddph, da_exposure, df_susceptibility, **kwargs):\n",
    "    nodata = da_exposure.attrs['_FillValue']\n",
    "    da0 = df_susceptibility.to_xarray()['factor'].chunk({'depth':-1})\n",
    "    factor = np.minimum(1, da0.interp(depth=da_flddph, **kwargs))\n",
    "    damage = (factor * da_exposure).fillna(nodata).astype(np.float32)\n",
    "    damage.name = da_exposure.name\n",
    "    damage.attrs.update(**da_exposure.attrs)\n",
    "    return damage\n",
    "\n",
    "def flood_exposed(da_flddph, da_exposure, min_flddph=hmin):\n",
    "    exposed = xr.where(da_flddph>min_flddph,da_exposure,0.0).astype(np.float32)\n",
    "    exposed.attrs.update(**da_exposure.attrs)\n",
    "    exposed.name = da_exposure.name\n",
    "    return exposed\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76bc29a",
   "metadata": {},
   "source": [
    "## base scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbb2f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for single scen\n",
    "# scen='qb100_qp100_h000_p000'\n",
    "# da_dam = flood_damage(da1.sel(scen=scen), da_exp0, df).compute()\n",
    "# da_ppl = flood_exposed(da1.sel(scen=scen), da_exp1, 0.15).compute()\n",
    "# print(da_dam.sum().item()/1e6)\n",
    "# print(da_ppl.sum().item()/1e3)\n",
    "\n",
    "# root = glob.glob(join(mdir, f'*{scen}'))[0]\n",
    "# print(root)\n",
    "# da1.sel(scen=scen).raster.to_raster(join(root, 'gis', 'hmax_bias.tif'), compress='lzw')\n",
    "# fn_out = join(root, 'gis', f'{exposure}.tif') \n",
    "# da_dam.raster.to_raster(fn_out, compress='lzw')\n",
    "# fn_out = join(root, 'gis', f'population_count.tif') \n",
    "# da_ppl.raster.to_raster(fn_out, compress='lzw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb75a2ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# flood impact univariate and full dep compound scenario\n",
    "rps = np.array([2,5,10,25,50,100,250,500], dtype=int)\n",
    "\n",
    "da_qb = hydromt.open_mfraster(os.path.join(rdir, 'hmax', f'qb_rp*.tif'), concat=True, concat_dim='rps')\n",
    "da_qp = hydromt.open_mfraster(os.path.join(rdir, 'hmax', f'qp_rp*.tif'), concat=True, concat_dim='rps')\n",
    "da_h = hydromt.open_mfraster(os.path.join(rdir, 'hmax', f'h_rp*.tif'), concat=True, concat_dim='rps')\n",
    "da_p = hydromt.open_mfraster(os.path.join(rdir, 'hmax', f'p_rp*.tif'), concat=True, concat_dim='rps')\n",
    "da_c = hydromt.open_mfraster(os.path.join(rdir, 'hmax', f'compound_fulldep_rp*.tif'), concat=True, concat_dim='rps').rename({'compound': 'cp'})\n",
    "ds = xr.merge([da_qb, da_qp, da_h, da_p, da_c]).raster.flipud()\n",
    "rps[0] = 1\n",
    "ds['rps'] = xr.IndexVariable('rps', rps)\n",
    "ds['cof'] = xr.IndexVariable('rps', get_logcov(rps))\n",
    "ds = ds.set_coords('cof').load()\n",
    "ds_impact = xr.Dataset()\n",
    "for dvar in ds.data_vars:\n",
    "    ds_impact[f'{dvar}_dam'] = flood_damage(ds[dvar], ds_exp['buildings_value'], df).compute()\n",
    "    ds_impact[f'{dvar}_pop'] = flood_exposed(ds[dvar], ds_exp['population_count'], hmin).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9769a10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flood impact all simulation events\n",
    "da_dam = flood_damage(da1, ds_exp['buildings_value'], df)\n",
    "da_dam.attrs.update(unit='USD')\n",
    "da_ppl = flood_exposed(da1, ds_exp['population_count'], hmin)\n",
    "da_dam.attrs.update(unit='people')\n",
    "ds_impact = xr.merge([da_dam, da_ppl]).persist()\n",
    "# ds_impact.chunk({'x':-1, 'y':-1, 'scen':10}).to_zarr(join(rdir, 'flood_impact.zarr'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762c8d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_impact_agg = ds_impact.sum(('x', 'y')).round(2)\n",
    "df_out = ds_impact_agg.reset_coords().drop_vars(['count','spatial_ref','index']).to_dataframe()\n",
    "df_out.to_csv(join(rdir, 'flood_impact_base.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c142260",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_cols = ['h_rp','p_rp','qb_rp','qp_rp']\n",
    "drop_cols = ['scen']#, 'h','p','qb','qp']\n",
    "exp_cols = ['buildings_value', 'population_count']\n",
    "\n",
    "# read impacts and convert to 4D xarray\n",
    "df_flood = pd.read_csv(join(rdir, 'flood_impact_base.csv'), index_col=0)\n",
    "df_flood[index_cols] = df_flood[index_cols]#np.maximum(1, df_flood[index_cols])\n",
    "df_flood = df_flood.reset_index()\n",
    "ds_flood_rp = df_flood.reset_index().drop(columns=index_cols+drop_cols).set_index(\n",
    "    pd.MultiIndex.from_frame(df_flood[index_cols])\n",
    ").to_xarray()\n",
    "\n",
    "# interpolate damages for each sample\n",
    "samples = {'obs': df_sample}\n",
    "for key in samples:\n",
    "    ds0_rp = samples[key][index_cols + ['year']].to_xarray().set_coords('year')\n",
    "    ds0_flood = ds_flood_rp[exp_cols].interp(ds0_rp, method='linear')\n",
    "    for col in exp_cols:\n",
    "        samples[key][f'{col}_base'] = ds0_flood[col].to_series()#.fillna(0)\n",
    "\n",
    "samples[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ac7867",
   "metadata": {},
   "source": [
    "## dikes rp 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79f303f",
   "metadata": {},
   "outputs": [],
   "source": [
    "qrp, hrp = 10, 10\n",
    "\n",
    "# interpolate damages for each sample\n",
    "for key in samples:\n",
    "    df0 = samples[key][index_cols + ['year']].copy()\n",
    "    df0['h_rp'] = np.where(df0['h_rp']<hrp,1,df0['h_rp'])\n",
    "    df0['qb_rp'] = np.where(df0['qb_rp']<hrp,1,df0['qb_rp'])\n",
    "    df0['qp_rp'] = np.where(df0['qp_rp']<hrp,1,df0['qp_rp'])\n",
    "    ds0_rp = df0.to_xarray().set_coords('year')\n",
    "    ds0_flood = ds_flood_rp[cols].interp(ds0_rp, method='linear')\n",
    "    for col in cols:\n",
    "        samples[key][f'{col}_dikes'] = ds0_flood[col].to_series()#.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eef9a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kort door de bocht!! \n",
    "ds_impact[f'cp_dam_dikes'] = ds_impact[f'cp_dam'].sel(rps=[10, 50, 100, 500])\n",
    "ds_impact[f'cp_pop_dikes'] = ds_impact[f'cp_pop'].sel(rps=[10, 50, 100, 500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0333db0a",
   "metadata": {},
   "source": [
    "## zoning rp 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9007ff0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct exposure with 5 year flood zone\n",
    "qrp, hrp = 5, 5\n",
    "da0 = da1.sel(scen=[f'qb{qrp:03d}_qp000_h000_p000', f'qb000_qp{qrp:03d}_h000_p000', f'qb000_qp000_h{hrp:03d}_p000']).max('scen').squeeze(drop=True)\n",
    "fld_bin = da0 > hmin\n",
    "ds_exp_zone = ds_exp.where(~fld_bin, 0)\n",
    "\n",
    "(ds_exp - ds_exp_zone).sum().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6104c90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "da_dam = flood_damage(da1, ds_exp_zone['buildings_value'], df)\n",
    "da_dam_agg = da_dam.sum(('x', 'y')).round(2)\n",
    "da_ppl = flood_exposed(da1, ds_exp_zone['population_count'], hmin)\n",
    "da_ppl_agg = da_ppl.sum(('x', 'y')).round(2)\n",
    "\n",
    "df_out = da_dam_agg.reset_coords().drop_vars(['band','spatial_ref']).to_dataframe()\n",
    "df_out[da_ppl.name] = da_ppl_agg.reset_coords(drop=True).to_dataframe()\n",
    "df_out.to_csv(join(mdir, '98_fiat', 'flood_impact_zoning.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1746225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read impacts and convert to 4D xarray\n",
    "df_flood = pd.read_csv(join(mdir, '98_fiat', 'flood_impact_zoning.csv'), index_col=0)\n",
    "df_flood[index_cols] = np.maximum(1, df_flood[index_cols])\n",
    "df_flood = df_flood.reset_index()\n",
    "ds_flood_rp = df_flood.reset_index().drop(columns=index_cols+drop_cols).set_index(\n",
    "    pd.MultiIndex.from_frame(df_flood[index_cols])\n",
    ").to_xarray()\n",
    "\n",
    "# interpolate damages for each sample\n",
    "for key in samples:\n",
    "    ds0_rp = samples[key][index_cols + ['year']].to_xarray().set_coords('year')\n",
    "    ds0_flood = ds_flood_rp[cols].interp(ds0_rp, method='linear')\n",
    "    for col in cols:\n",
    "        samples[key][f'{col}_zoning'] = ds0_flood[col].to_series()#.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87a3fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_impact[f'cp_dam_zoning'] = flood_damage(ds['cp'], ds_exp_zone['buildings_value'], df)\n",
    "ds_impact[f'cp_pop_zoning'] = flood_exposed(ds['cp'], ds_exp_zone['population_count'], hmin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1656fec5",
   "metadata": {},
   "source": [
    "## dry proofing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017e4f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify depth-damage curve\n",
    "hmin_dry = 0.5\n",
    "df_dry = df.copy()\n",
    "df_dry[df_dry.index<hmin_dry] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fc1cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "da_dam = flood_damage(da1, ds_exp['buildings_value'], df_dry)\n",
    "da_dam_agg = da_dam.sum(('x', 'y')).round(2)\n",
    "da_ppl = flood_exposed(da1, ds_exp['population_count'], hmin_dry)\n",
    "da_ppl_agg = da_ppl.sum(('x', 'y')).round(2)\n",
    "\n",
    "df_out = da_dam_agg.reset_coords().drop_vars(['band','spatial_ref']).to_dataframe()\n",
    "df_out[da_ppl.name] = da_ppl_agg.reset_coords(drop=True).to_dataframe()\n",
    "df_out.to_csv(join(mdir, '98_fiat', 'flood_impact_dryproofing.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e4606f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read impacts and convert to 4D xarray\n",
    "df_flood = pd.read_csv(join(mdir, '98_fiat', 'flood_impact_dryproofing.csv'), index_col=0)\n",
    "df_flood[index_cols] = np.maximum(1, df_flood[index_cols])\n",
    "df_flood = df_flood.reset_index()\n",
    "ds_flood_rp = df_flood.reset_index().drop(columns=index_cols+drop_cols).set_index(\n",
    "    pd.MultiIndex.from_frame(df_flood[index_cols])\n",
    ").to_xarray()\n",
    "\n",
    "# interpolate damages for each sample\n",
    "for key in samples:\n",
    "    ds0_rp = samples[key][index_cols + ['year']].to_xarray().set_coords('year')\n",
    "    ds0_flood = ds_flood_rp[cols].interp(ds0_rp, method='linear')\n",
    "    for col in cols:\n",
    "        samples[key][f'{col}_dryproofing'] = ds0_flood[col].to_series()#.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf03be04",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in samples:\n",
    "    df_sample = samples[key]\n",
    "    fn = os.path.join('../02_data', f'modelled_dataset_5000_years_{key}_rp_impact.csv')\n",
    "    df_sample.to_csv(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91067a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_impact[f'cp_dam_dryproofing'] = flood_damage(ds['cp'], ds_exp['buildings_value'], df_dry)\n",
    "ds_impact[f'cp_pop_dryproofing'] = flood_exposed(ds['cp'], ds_exp['population_count'], hmin_dry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfc4d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # combine outputs\n",
    "# cols = ['buildings_value', 'population_count']\n",
    "# drop_cols = ['h_rp','p_rp','qb_rp','qp_rp']\n",
    "# df_lst = []\n",
    "# for postfix in ['base', 'dikes', 'dryproofing', 'zoning']:\n",
    "#     dfi = pd.read_csv(join(mdir, '98_fiat', f'flood_impact_{postfix}.csv'), index_col=0)\n",
    "#     dfi = dfi.rename(columns={c: f'{c}_{postfix}' for c in cols})\n",
    "#     if postfix != 'base':\n",
    "#         dfi = dfi.drop(columns=drop_cols)\n",
    "#     df_lst.append(dfi)\n",
    "# df_out = pd.concat(df_lst, axis=1)\n",
    "# df_out.to_csv(join(mdir, '98_fiat', 'flood_impact.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d522ecf",
   "metadata": {},
   "source": [
    "## risk calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0101af",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_cols = ['h_rp','p_rp','qb_rp','qp_rp']\n",
    "exp_cols = ['buildings_value','population_count']\n",
    "# samples = dict()\n",
    "# for postfix in ['_magInd_timeObs', '_magInd_timeInd', '_magObs_timeObs']:\n",
    "#     fn = os.path.join('../02_data', f'modelled_dataset_5000_years{postfix}_rp_impact.csv')\n",
    "#     samples[postfix[1:]] = pd.read_csv(fn, index_col=0)\n",
    "    \n",
    "df_flood = pd.read_csv(join(rdir, 'flood_impact_base.csv'), index_col=0)\n",
    "df_flood = df_flood.rename(columns={c: f'{c}_base' for c in exp_cols})\n",
    "# df_flood[index_cols] = np.maximum(1, df_flood[index_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52d551f",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_cols0 = df_flood.columns[-2:].values.tolist()\n",
    "risk = dict()\n",
    "dfs = dict()\n",
    "for col in index_cols:\n",
    "    zero_cols = [c for c in index_cols if c != col]\n",
    "    df0 = df_flood[np.all(df_flood[zero_cols]==1, axis=1)].sort_values(col)\n",
    "    risk[col.split('_')[0]] = pd.Series({c: loglin_trapz(df0[c], df0[col]) for c in exp_cols0})\n",
    "    dfs[col.split('_')[0]] = df0[[col] + exp_cols0].rename(columns={col: 'rp'})\n",
    "sum([df[exp_cols0[0]] for _, df in risk.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2ccfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = df_flood[np.all(np.diff(df_flood[index_cols], axis=1)==0, axis=1)].sort_values(col)\n",
    "dfs['fullDep'] = df0[['h_rp'] + exp_cols0].rename(columns={'h_rp': 'rp'})\n",
    "risk['fullDep'] = pd.Series({c: loglin_trapz(df0[c], df0['h_rp']) for c in exp_cols0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5a1841",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in samples:\n",
    "    df = samples[key]\n",
    "    \n",
    "    exp_cols = [c for c in df.columns if (c.startswith('buildings') or c.startswith('pop'))]\n",
    "    print(key, df.index.size)\n",
    "    \n",
    "    df0 = df[['year'] + exp_cols].fillna(0).groupby('year').max().reset_index()\n",
    "    dfs[f'{key}'] = df0\n",
    "    risk[f'{key}'] = pd.Series({c: loglin_trapz(df0[c]) for c in exp_cols})\n",
    "\n",
    "    # df0 = df[['year']+exp_cols].groupby('year').sum().sort_values(exp_cols[0]).reset_index()\n",
    "    # dfs[f'{key}_sum'] = df0\n",
    "    # risk[f'{key}_sum'] = pd.Series({c: loglin_trapz(df0[c]) for c in exp_cols})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c05516",
   "metadata": {},
   "outputs": [],
   "source": [
    "df0[df0.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5795db89",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_risk = pd.DataFrame(risk).T\n",
    "df_risk['buildings_value_base']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758e501c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_risk[['buildings_value_base', 'population_count_base']].rename(labs).to_csv('risk_base.csv')\n",
    "df_adapt = df_risk.loc[['magObs_timeObs_sum'], :].rename(labs)\n",
    "df_adapt.to_csv('risk_adapt.csv')\n",
    "df_adapt.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e0bdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "labs = {\n",
    "    'fullDep': 'full dependence',\n",
    "    'magObs_timeObs': 'obs. dependence',\n",
    "    'magInd_timeInd': 'full indep.',\n",
    "    'magInd_timeObs': 'independence',\n",
    "    'magObs_timeObs_sum': 'obs. dependence',\n",
    "    'magInd_timeInd_sum': 'full indep.',\n",
    "    'magInd_timeObs_sum': 'independence',\n",
    "    'p': 'pluvial', \n",
    "    'qp': 'fluvial Pungwe', \n",
    "    'qb': 'fluvial Buzi', \n",
    "    'h': 'coastal', \n",
    "}\n",
    "rps= [1,2,5,10,50,100,500]\n",
    "ylabs = {\n",
    "    'buildings_value': 'annual building damage [mil. USD]',\n",
    "    'population_count': 'annual people exposed [-]'\n",
    "}\n",
    "legend_titles = {\n",
    "    'buildings_value': '(EAD [mil. USD])',\n",
    "    'population_count': '(EAAP [x 1000])'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f2e82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "exp0, factor = 'population_count', 1e3\n",
    "exp0, factor = 'buildings_value', 1e6\n",
    "fig, ax = plt.subplots(1,1, figsize=(6,4))\n",
    "for key in ['qp', 'qb', 'h', 'p'][-4:]:\n",
    "    risk0 = risk[key][f'{exp0}_base']/factor\n",
    "    lab = f'{labs[key]} ({risk0:.2f})'\n",
    "#     lab = f'{labs[key]}'\n",
    "    ls = '-' if 'dep' in lab else '--'\n",
    "    zorder = 1 if 'dep' in lab else 2\n",
    "    df0 = dfs[key].copy()\n",
    "    if 'rp' not in df0:\n",
    "        df0 = df0.sort_values(f'{exp0}_base')\n",
    "        df0['rp'] = (1/(1-np.arange(df0.index.size)/df0.index.size))\n",
    "    (df0.set_index('rp')[f'{exp0}_base']/factor).plot(ls=ls, lw=1.5, ax=ax, label=lab, zorder=zorder)\n",
    "ax.set_xlim([1, 500])\n",
    "if 'build' in exp0:\n",
    "    ax.set_ylim([0, 150])\n",
    "else:\n",
    "    ax.set_ylim([0, 120])\n",
    "ax.set_xscale('log')\n",
    "ax.set_xticks(rps)\n",
    "ax.set_xticklabels(rps)\n",
    "ax.set_ylabel(f'{ylabs[exp0]}')\n",
    "ax.set_xlabel('return period [year]')\n",
    "# ax.legend(title='driver scenario', loc='upper left')\n",
    "ax.legend(title=f'driver scenario\\n {legend_titles[exp0]}', loc='upper left')\n",
    "ax.set_title('Univariate flood risk - base scenario')\n",
    "plt.savefig(os.path.join('../FIGURES', f'risk_curve_{exp0}_single.png'), dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0d509f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(6,4))\n",
    "for key in ['magObs_timeObs_sum', 'magInd_timeObs_sum', 'fullDep']:\n",
    "    risk0 = risk[key][f'{exp0}_base']/factor\n",
    "    lab = f'{labs[key]} ({risk0:.2f})'\n",
    "    ls = '--' if 'full' in key else '-'\n",
    "    lw = 3 if key == 'magObs_timeObs_sum' else 1.5\n",
    "    df0 = dfs[key].copy()\n",
    "    if 'rp' not in df0:\n",
    "        df0 = df0.sort_values(f'{exp0}_base')\n",
    "        df0['rp'] = (1/(1-np.arange(df0.index.size)/df0.index.size))\n",
    "    (df0.set_index('rp')[f'{exp0}_base']/factor).plot(ls=ls, lw=lw, ax=ax, label=lab)\n",
    "ax.set_xlim([1, 500])\n",
    "if 'build' in exp0:\n",
    "    ax.set_ylim([0, 300])\n",
    "else:\n",
    "    ax.set_ylim([0, 150])\n",
    "ax.set_xscale('log')\n",
    "ax.set_xticks(rps)\n",
    "ax.set_xticklabels(rps)\n",
    "ax.set_ylabel(f'{ylabs[exp0]}')\n",
    "ax.set_xlabel('return period [year]')\n",
    "legend = ax.legend(title=f'compound scenario\\n {legend_titles[exp0]}', loc='upper left')#, fontsize='large')\n",
    "ax.set_title('Compound flood risk - base scenario')\n",
    "plt.savefig(f'../FIGURES/risk_curve_{exp0}_compound.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774dc862",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(6, 4))\n",
    "key='magObs_timeObs_sum'\n",
    "for pf in ['base', 'dikes', 'dryproofing', 'zoning']:\n",
    "    exp = f'{exp0}_{pf}'\n",
    "    risk0 = risk[key][exp]/factor\n",
    "    pf = 'dry-proofing' if pf == 'dryproofing' else pf\n",
    "    lab = f'{pf} ({risk0:.2f})'\n",
    "#     ls = '-' if pf == 'base' else ':'\n",
    "    lw = 3 if pf == 'base' else 1.5\n",
    "    df0 = dfs[key].copy()\n",
    "    if 'rp' not in df0:\n",
    "        df0 = df0.sort_values(exp)\n",
    "        df0['rp'] = (1/(1-np.arange(df0.index.size)/df0.index.size))\n",
    "    (df0.set_index('rp')[exp]/factor).plot(ls='-', lw=lw, ax=ax, label=lab)\n",
    "ax.set_xlim([1, 500])\n",
    "if 'build' in exp0:\n",
    "    ax.set_ylim([0, 200])\n",
    "else:\n",
    "    ax.set_ylim([0, 140])\n",
    "ax.set_xscale('log')\n",
    "ax.set_xticks(rps)\n",
    "ax.set_xticklabels(rps)\n",
    "ax.set_ylabel(f'cumulative {ylabs[exp0]}')\n",
    "ax.set_xlabel('return period [year]')\n",
    "ax.legend(title=f'adaptation scenario\\n {legend_titles[exp0]}', loc='upper left')\n",
    "ax.set_title(f'Compound flood risk - {legend_titles[exp0][1:5]}')\n",
    "plt.savefig(f'../FIGURES/risk_curve_{exp0}_adaptation.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcf50bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "_df = samples['magObs_timeObs']\n",
    "np.sum(np.logical_and.reduce((_df['qb_rp']>10, _df['qp_rp']>10, _df['h_rp']>10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a35b0d",
   "metadata": {},
   "source": [
    "### RISK MAPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4990daf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_risk = (ds_impact * ds['cof']).sum('rps').compute()\n",
    "\n",
    "# bias correct to match totals\n",
    "df00 = df_adapt.T\n",
    "df00 = df00.rename({n:n.replace('buildings_value', 'cp_dam').replace('_base', '') for n in df00.index})\n",
    "df00 = df00.rename({n:n.replace('population_count', 'cp_pop').replace('_base', '') for n in df00.index})\n",
    "\n",
    "for dvar in df00.index:\n",
    "    ds_risk[dvar] = ds_risk[dvar]/ds_risk[dvar].sum().item()*df00.loc[dvar].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9527da60",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_risk = ds_risk.raster.reproject_like(mod0.staticmaps, 'nearest')\n",
    "for dvar in ds_risk.data_vars:\n",
    "    ds_risk[dvar] = ds_risk[dvar].where(mod0.staticmaps['msk']>0, -9999)\n",
    "    ds_risk[dvar].raster.set_nodata(-9999)\n",
    "ds_risk.raster.set_crs(mod0.crs)\n",
    "dir_out = os.path.join('../03_models', 'risk')\n",
    "ds_risk.raster.to_mapstack(dir_out, compress='lzw')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "38648ff479392915c1a5d77722aa6edad827edf2098e3798b9c4282ba45e9fb7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 ('compound_risk')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
