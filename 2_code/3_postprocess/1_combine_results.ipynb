{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from os.path import dirname, isfile, join\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read simulation scenarios\n",
    "mdir = r\"../../3_models/sfincs\"\n",
    "rdir = r\"../../4_results\"\n",
    "scens = pd.read_csv(join(rdir, 'sim_SCEN.csv'), index_col=0).rename(columns={'h_tsw_rp': 'h_rp'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hydromt_sfincs import SfincsModel\n",
    "mod0 = SfincsModel(join(mdir, '00_base_riv'), mode='r')\n",
    "rivmsk = mod0.staticmaps['rivmsk'].raster.flipud()==1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read results\n",
    "import xarray as xr\n",
    "\n",
    "da_lst, drop_lst = [], []\n",
    "for i, scen in scens.iterrows():\n",
    "    name = scen['scen']\n",
    "    root = join(mdir, name)\n",
    "    fn_hmax = join(root, 'gis', 'hmax.tif')\n",
    "    if isfile(fn_hmax):\n",
    "        da_lst.append(xr.open_rasterio(fn_hmax, chunks='auto').squeeze(drop=True))\n",
    "    else:\n",
    "        drop_lst.append(i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine results\n",
    "\n",
    "da_hmax = xr.concat(da_lst, dim='index').rename('hmax')\n",
    "da_hmax.raster.set_nodata(-999)\n",
    "da_hmax.raster.set_crs(mod0.crs)\n",
    "da_hmax['index'] = xr.IndexVariable('index', scens.index.values)\n",
    "da_meta = scens.drop(drop_lst).to_xarray()\n",
    "da_hmax = xr.merge([da_hmax, da_meta]).set_coords(da_meta.data_vars)['hmax']\n",
    "da_hmax = da_hmax.swap_dims({'index':'scen'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct for permanent water \n",
    "# & bias correct for Q but make sure to preserve P\n",
    "nodata = da_hmax.raster.nodata\n",
    "# get Q2 + MSHW map for bias correction\n",
    "bias, da0 = 'q2', da_hmax.sel(scen=['qb002_qp002_h000_p000']).squeeze(drop=True)\n",
    "da0.raster.flipud().raster.to_raster(join(rdir, 'hmax', f'bias_{bias}.tif'))\n",
    "mask = np.logical_and(da0!=nodata, ~rivmsk)\n",
    "# preserve P in all scenarios\n",
    "pscen = [f'qb000_qp000_h000_p{s[-3:]}' for s in da_hmax.scen.values]\n",
    "dap = da_hmax.sel(scen=pscen).reset_coords(drop=True).drop('scen')\n",
    "dap = np.maximum(0, dap - da_hmax.sel(scen=['qb000_qp000_h000_p000']).squeeze(drop=True))\n",
    "# take maximum of pscen and bias corrected scens\n",
    "da_hmax1 = np.maximum(dap, da_hmax-da0).where(mask, nodata)\n",
    "da_hmax1.raster.set_nodata(nodata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da_hmax1.chunk({'scen':50, 'x':250, 'y':250}).to_dataset().to_zarr(join(rdir, 'hmax.zarr'))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "688a12aaff51d3ef71993f7ebede17ccae79b4aba2f74e006d22f13d19a0bc19"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 ('compound_risk')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
